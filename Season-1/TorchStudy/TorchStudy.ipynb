{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Torch Study\n",
    "## Three main features of PyTorch\n",
    " - Tensor: similar to numpy.array but can run on GPUs\n",
    " - Autograd: automatic differentiation for all operations on Tensors\n",
    " - NN: nn.module, framework to build neural network easily\n",
    "\n",
    "## Materials\n",
    " - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    " - [PyTorch Examples](https://github.com/jcjohnson/pytorch-examples)\n",
    "\n",
    "## Examples\n",
    " - 3 Layers Neural Network\n",
    "   - Input Layer: 1000 neurons\n",
    "   - Hidden Layer: 100 neurons\n",
    "      - ReLU Activation Function\n",
    "   - Output Layer: 10 neurons\n",
    "- Training Data: 100 samples\n",
    "   - Learning Rate: 1e-6\n",
    "   - Training Iterations: 500"
   ],
   "id": "9bcfeb2e0b0d6c4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Numpy Version\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 1\n",
    "\n",
    "# generate the training data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# init the weights\n",
    "w1 = np.random.randn(D_in, D_h)\n",
    "w2 = np.random.randn(D_h, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.dot(w1)                                                           # N, D_h\n",
    "    h_relu = np.maximum(h, 0)                                   # N, D_h\n",
    "    y_pred = h_relu.dot(w2)                                         # N, D_out\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = np.square(y_pred - y).sum()                      # scalar\n",
    "    print(t, loss)\n",
    "\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)                                        # N, D_out\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)                                 # D_h, D_out\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)                                 # N, D_h\n",
    "    grad_h = grad_h_relu.copy()                                             # N, D_h\n",
    "    grad_h[h < 0] = 0                                                               # N, D_h\n",
    "    grad_w1 = x.T.dot(grad_h)                                               # D_in, D_h\n",
    "\n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ],
   "id": "1ad41e84323dfa80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tensor Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate the training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device)\n",
    "w2 = torch.randn(D_h, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # update the weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ],
   "id": "ab3483953fa258ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Autograd Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, requires_grad=True)\n",
    "\n",
    "# training\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # use autograd to do the back-propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights, prevent torch do autograd for this part\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # zero grads\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "id": "b4cba1b62ee3df49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MyReLU Version\n",
    "import torch\n",
    "from my_relu import MyReLU\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, requires_grad=True)\n",
    "\n",
    "# training\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference, with MyReLU\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # use autograd to do the back-propagation\n",
    "    # gonna call MyReLU.backward()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights, prevent torch do autograd for this part\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # zero grads\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "id": "14ba4cbac76cf782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NN Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# define the model\n",
    "model = torch.nn.Sequential(            # 创建一个顺序模型容器\n",
    "    torch.nn.Linear(D_in, D_h),             # 添加第一层全连接层，输入维度1000，输出维度100\n",
    "    torch.nn.ReLU(),                                # 添加 ReLU 激活函数层\n",
    "    torch.nn.Linear(D_h, D_out)             # 添加第二层全连接层，输入维度100，输出维度10\n",
    ").to(device)\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')             # 定义均方误差损失函数，使用求和方式计算\n",
    "\n",
    "# training, use bigger learning rate\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # back-propagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters(weights)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():                    #  遍历模型中的所有参数\n",
    "            param -= learning_rate * param.grad             # 使用梯度下降法更新参数：参数 = 参数 - 学习率 × 梯度"
   ],
   "id": "bcbe5b11000504a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:45:54.435755Z",
     "start_time": "2025-07-31T07:45:52.821367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom Module Version\n",
    "import torch\n",
    "from my_nn import TwoLayerNet\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# define the model\n",
    "model = TwoLayerNet(D_in, D_h, D_out).to(device)\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# training, use bigger learning rate\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # back-propagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters(weights)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ],
   "id": "98ce482211cc309e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1005.9903564453125\n",
      "1 929.0047607421875\n",
      "2 863.9293823242188\n",
      "3 807.3326416015625\n",
      "4 757.4134521484375\n",
      "5 712.4595947265625\n",
      "6 671.5608520507812\n",
      "7 633.9004516601562\n",
      "8 598.9429321289062\n",
      "9 566.4400634765625\n",
      "10 535.914794921875\n",
      "11 507.2608337402344\n",
      "12 479.9627685546875\n",
      "13 454.00067138671875\n",
      "14 429.159912109375\n",
      "15 405.43109130859375\n",
      "16 382.64862060546875\n",
      "17 360.8590087890625\n",
      "18 340.0908203125\n",
      "19 320.2925109863281\n",
      "20 301.37420654296875\n",
      "21 283.4112548828125\n",
      "22 266.4350891113281\n",
      "23 250.31593322753906\n",
      "24 235.05264282226562\n",
      "25 220.65667724609375\n",
      "26 207.07933044433594\n",
      "27 194.26718139648438\n",
      "28 182.20199584960938\n",
      "29 170.85586547851562\n",
      "30 160.17042541503906\n",
      "31 150.13290405273438\n",
      "32 140.72592163085938\n",
      "33 131.90960693359375\n",
      "34 123.63257598876953\n",
      "35 115.87843322753906\n",
      "36 108.61011505126953\n",
      "37 101.78626251220703\n",
      "38 95.39813995361328\n",
      "39 89.419189453125\n",
      "40 83.84007263183594\n",
      "41 78.62237548828125\n",
      "42 73.74696350097656\n",
      "43 69.18798828125\n",
      "44 64.92525482177734\n",
      "45 60.9373664855957\n",
      "46 57.20482635498047\n",
      "47 53.71139907836914\n",
      "48 50.45107650756836\n",
      "49 47.39810562133789\n",
      "50 44.543426513671875\n",
      "51 41.87417984008789\n",
      "52 39.381736755371094\n",
      "53 37.05028533935547\n",
      "54 34.868568420410156\n",
      "55 32.81855010986328\n",
      "56 30.901485443115234\n",
      "57 29.100229263305664\n",
      "58 27.41339874267578\n",
      "59 25.831623077392578\n",
      "60 24.34893798828125\n",
      "61 22.96383285522461\n",
      "62 21.66409683227539\n",
      "63 20.444181442260742\n",
      "64 19.297279357910156\n",
      "65 18.21908950805664\n",
      "66 17.205358505249023\n",
      "67 16.25342559814453\n",
      "68 15.359628677368164\n",
      "69 14.51863956451416\n",
      "70 13.72740364074707\n",
      "71 12.984075546264648\n",
      "72 12.283407211303711\n",
      "73 11.623831748962402\n",
      "74 11.002847671508789\n",
      "75 10.4180269241333\n",
      "76 9.866656303405762\n",
      "77 9.346711158752441\n",
      "78 8.856636047363281\n",
      "79 8.394280433654785\n",
      "80 7.95741605758667\n",
      "81 7.545013427734375\n",
      "82 7.155916690826416\n",
      "83 6.788766860961914\n",
      "84 6.441573619842529\n",
      "85 6.113441467285156\n",
      "86 5.803012847900391\n",
      "87 5.510538578033447\n",
      "88 5.233640193939209\n",
      "89 4.971925258636475\n",
      "90 4.725051403045654\n",
      "91 4.491137504577637\n",
      "92 4.269448280334473\n",
      "93 4.060006141662598\n",
      "94 3.861694812774658\n",
      "95 3.6737637519836426\n",
      "96 3.4953651428222656\n",
      "97 3.3262434005737305\n",
      "98 3.1659464836120605\n",
      "99 3.014054536819458\n",
      "100 2.8700547218322754\n",
      "101 2.7335479259490967\n",
      "102 2.6041178703308105\n",
      "103 2.481306791305542\n",
      "104 2.364577054977417\n",
      "105 2.253826856613159\n",
      "106 2.1486310958862305\n",
      "107 2.0488600730895996\n",
      "108 1.9539822340011597\n",
      "109 1.8638734817504883\n",
      "110 1.7782182693481445\n",
      "111 1.6967668533325195\n",
      "112 1.6192970275878906\n",
      "113 1.5456278324127197\n",
      "114 1.4755674600601196\n",
      "115 1.408989429473877\n",
      "116 1.3456766605377197\n",
      "117 1.285341501235962\n",
      "118 1.2279160022735596\n",
      "119 1.1732914447784424\n",
      "120 1.1212081909179688\n",
      "121 1.071601390838623\n",
      "122 1.0244052410125732\n",
      "123 0.979508638381958\n",
      "124 0.9367411136627197\n",
      "125 0.8959901332855225\n",
      "126 0.8571676015853882\n",
      "127 0.8201107978820801\n",
      "128 0.7847551107406616\n",
      "129 0.7510671019554138\n",
      "130 0.7188816070556641\n",
      "131 0.6881653070449829\n",
      "132 0.65884929895401\n",
      "133 0.6308441162109375\n",
      "134 0.6040226817131042\n",
      "135 0.5784530639648438\n",
      "136 0.5540539026260376\n",
      "137 0.5307132005691528\n",
      "138 0.5084159970283508\n",
      "139 0.4871271848678589\n",
      "140 0.4667973220348358\n",
      "141 0.44732779264450073\n",
      "142 0.4287286400794983\n",
      "143 0.4109802544116974\n",
      "144 0.3939933776855469\n",
      "145 0.37775731086730957\n",
      "146 0.36222395300865173\n",
      "147 0.3473604619503021\n",
      "148 0.3331484794616699\n",
      "149 0.31955286860466003\n",
      "150 0.30654212832450867\n",
      "151 0.29407429695129395\n",
      "152 0.2821430563926697\n",
      "153 0.2707255780696869\n",
      "154 0.2598057687282562\n",
      "155 0.24933740496635437\n",
      "156 0.23931556940078735\n",
      "157 0.2297150194644928\n",
      "158 0.22052863240242004\n",
      "159 0.21173658967018127\n",
      "160 0.2033168226480484\n",
      "161 0.19523712992668152\n",
      "162 0.18748746812343597\n",
      "163 0.1800645887851715\n",
      "164 0.172955721616745\n",
      "165 0.16613440215587616\n",
      "166 0.15959402918815613\n",
      "167 0.15333440899848938\n",
      "168 0.14733253419399261\n",
      "169 0.14157456159591675\n",
      "170 0.13605612516403198\n",
      "171 0.13076451420783997\n",
      "172 0.12568709254264832\n",
      "173 0.12081660330295563\n",
      "174 0.11614835262298584\n",
      "175 0.11166495084762573\n",
      "176 0.10736363381147385\n",
      "177 0.10323426127433777\n",
      "178 0.0992751196026802\n",
      "179 0.09547267854213715\n",
      "180 0.0918223187327385\n",
      "181 0.0883140042424202\n",
      "182 0.08494991064071655\n",
      "183 0.08172022551298141\n",
      "184 0.07861902564764023\n",
      "185 0.0756453350186348\n",
      "186 0.07278275489807129\n",
      "187 0.07003054767847061\n",
      "188 0.06738664209842682\n",
      "189 0.06484931707382202\n",
      "190 0.06241089850664139\n",
      "191 0.06006750464439392\n",
      "192 0.05781788378953934\n",
      "193 0.055657871067523956\n",
      "194 0.05358101800084114\n",
      "195 0.05158558115363121\n",
      "196 0.04966714233160019\n",
      "197 0.04782181233167648\n",
      "198 0.04605094715952873\n",
      "199 0.044346489012241364\n",
      "200 0.04270956665277481\n",
      "201 0.04113328829407692\n",
      "202 0.03961813822388649\n",
      "203 0.03816112503409386\n",
      "204 0.036760080605745316\n",
      "205 0.035413507372140884\n",
      "206 0.0341196209192276\n",
      "207 0.03287338465452194\n",
      "208 0.0316753126680851\n",
      "209 0.030523128807544708\n",
      "210 0.029414057731628418\n",
      "211 0.028346551582217216\n",
      "212 0.02731970138847828\n",
      "213 0.026331845670938492\n",
      "214 0.025381192564964294\n",
      "215 0.0244662556797266\n",
      "216 0.02358657866716385\n",
      "217 0.022739754989743233\n",
      "218 0.021923847496509552\n",
      "219 0.021138407289981842\n",
      "220 0.02038266882300377\n",
      "221 0.019655266776680946\n",
      "222 0.018955234438180923\n",
      "223 0.018280982971191406\n",
      "224 0.017631573602557182\n",
      "225 0.017006803303956985\n",
      "226 0.01640389673411846\n",
      "227 0.0158223956823349\n",
      "228 0.015262515284121037\n",
      "229 0.014723282307386398\n",
      "230 0.014204300008714199\n",
      "231 0.013704584911465645\n",
      "232 0.013222815468907356\n",
      "233 0.012758949771523476\n",
      "234 0.012312223203480244\n",
      "235 0.011881565675139427\n",
      "236 0.011466487310826778\n",
      "237 0.01106688380241394\n",
      "238 0.01068178005516529\n",
      "239 0.010310706682503223\n",
      "240 0.009953231550753117\n",
      "241 0.009608147665858269\n",
      "242 0.009275974705815315\n",
      "243 0.008955626748502254\n",
      "244 0.008646847680211067\n",
      "245 0.008348972536623478\n",
      "246 0.008062413893640041\n",
      "247 0.007785583380609751\n",
      "248 0.007518724538385868\n",
      "249 0.007261650636792183\n",
      "250 0.007013705559074879\n",
      "251 0.006774653680622578\n",
      "252 0.006544089876115322\n",
      "253 0.0063214912079274654\n",
      "254 0.006106946617364883\n",
      "255 0.005899806506931782\n",
      "256 0.0056999195367097855\n",
      "257 0.0055072433315217495\n",
      "258 0.005321424454450607\n",
      "259 0.005142129026353359\n",
      "260 0.004969026427716017\n",
      "261 0.0048020826652646065\n",
      "262 0.004641090985387564\n",
      "263 0.004485702142119408\n",
      "264 0.004335632547736168\n",
      "265 0.004190852399915457\n",
      "266 0.0040511745028197765\n",
      "267 0.003916277550160885\n",
      "268 0.003786054439842701\n",
      "269 0.003660593181848526\n",
      "270 0.0035392437130212784\n",
      "271 0.003422207199037075\n",
      "272 0.003309238236397505\n",
      "273 0.0032001049257814884\n",
      "274 0.003094705753028393\n",
      "275 0.0029930078890174627\n",
      "276 0.002894849982112646\n",
      "277 0.002800006652250886\n",
      "278 0.0027084387838840485\n",
      "279 0.002620006911456585\n",
      "280 0.00253450870513916\n",
      "281 0.0024520019069314003\n",
      "282 0.0023722820915281773\n",
      "283 0.002295274753123522\n",
      "284 0.0022208932787179947\n",
      "285 0.0021489718928933144\n",
      "286 0.0020795464515686035\n",
      "287 0.0020123710855841637\n",
      "288 0.0019475225126370788\n",
      "289 0.0018848450854420662\n",
      "290 0.0018242732621729374\n",
      "291 0.00176575081422925\n",
      "292 0.0017091752961277962\n",
      "293 0.001654525171034038\n",
      "294 0.0016016808804124594\n",
      "295 0.0015506013296544552\n",
      "296 0.0015012279618531466\n",
      "297 0.0014534750953316689\n",
      "298 0.0014073664788156748\n",
      "299 0.0013627896551042795\n",
      "300 0.0013196568470448256\n",
      "301 0.0012779065873473883\n",
      "302 0.0012376136146485806\n",
      "303 0.0011986271711066365\n",
      "304 0.001160913030616939\n",
      "305 0.0011244346387684345\n",
      "306 0.0010891638230532408\n",
      "307 0.0010550502920523286\n",
      "308 0.001022054348140955\n",
      "309 0.0009901321027427912\n",
      "310 0.00095926015637815\n",
      "311 0.000929461675696075\n",
      "312 0.0009005528409034014\n",
      "313 0.0008726563537493348\n",
      "314 0.0008457278599962592\n",
      "315 0.0008198246359825134\n",
      "316 0.0007947614649310708\n",
      "317 0.0007705073803663254\n",
      "318 0.0007470051059499383\n",
      "319 0.0007243008585646749\n",
      "320 0.0007023222278803587\n",
      "321 0.0006810289924032986\n",
      "322 0.0006604199297726154\n",
      "323 0.0006404851446859539\n",
      "324 0.0006211797008290887\n",
      "325 0.0006025233305990696\n",
      "326 0.0005844388506375253\n",
      "327 0.0005669283564202487\n",
      "328 0.0005499573890119791\n",
      "329 0.0005335206515155733\n",
      "330 0.0005176032427698374\n",
      "331 0.0005021902616135776\n",
      "332 0.00048727940884418786\n",
      "333 0.00047282769810408354\n",
      "334 0.00045883594430051744\n",
      "335 0.0004452844732441008\n",
      "336 0.00043214927427470684\n",
      "337 0.0004194403882138431\n",
      "338 0.0004071063012816012\n",
      "339 0.0003951725666411221\n",
      "340 0.0003836019313894212\n",
      "341 0.00037238397635519505\n",
      "342 0.0003615214372985065\n",
      "343 0.0003509903617668897\n",
      "344 0.00034078338649123907\n",
      "345 0.00033088508644141257\n",
      "346 0.0003212948504369706\n",
      "347 0.0003119978355243802\n",
      "348 0.0003029841464012861\n",
      "349 0.00029424409149214625\n",
      "350 0.00028576425393112004\n",
      "351 0.00027755630435422063\n",
      "352 0.0002695893927011639\n",
      "353 0.00026186471222899854\n",
      "354 0.0002543841546867043\n",
      "355 0.00024712696904316545\n",
      "356 0.0002400808152742684\n",
      "357 0.0002332363510504365\n",
      "358 0.0002266092342324555\n",
      "359 0.00022018401068635285\n",
      "360 0.00021394091891124845\n",
      "361 0.00020788572146557271\n",
      "362 0.0002020179235842079\n",
      "363 0.00019631630857475102\n",
      "364 0.0001907962723635137\n",
      "365 0.000185433731530793\n",
      "366 0.0001802323095034808\n",
      "367 0.00017517550440970808\n",
      "368 0.00017027388093993068\n",
      "369 0.00016551747103221714\n",
      "370 0.0001608984312042594\n",
      "371 0.00015641206118743867\n",
      "372 0.00015206437092274427\n",
      "373 0.00014783997903577983\n",
      "374 0.00014373959857039154\n",
      "375 0.0001397637533955276\n",
      "376 0.00013589946320280433\n",
      "377 0.00013214476348366588\n",
      "378 0.00012850096391048282\n",
      "379 0.00012496548879425973\n",
      "380 0.00012152743875049055\n",
      "381 0.00011819375504273921\n",
      "382 0.0001149533418356441\n",
      "383 0.00011180329602211714\n",
      "384 0.00010874874715227634\n",
      "385 0.00010577886132523417\n",
      "386 0.00010290157661074772\n",
      "387 0.00010009584366343915\n",
      "388 9.737478103488684e-05\n",
      "389 9.473324462305754e-05\n",
      "390 9.216595208272338e-05\n",
      "391 8.96704223123379e-05\n",
      "392 8.724650251679122e-05\n",
      "393 8.488720050081611e-05\n",
      "394 8.260076720034704e-05\n",
      "395 8.037837687879801e-05\n",
      "396 7.821420149412006e-05\n",
      "397 7.611045293742791e-05\n",
      "398 7.407084194710478e-05\n",
      "399 7.208803435787559e-05\n",
      "400 7.015983283054084e-05\n",
      "401 6.828508776379749e-05\n",
      "402 6.646222755080089e-05\n",
      "403 6.469172512879595e-05\n",
      "404 6.297031359281391e-05\n",
      "405 6.129638495622203e-05\n",
      "406 5.966961907688528e-05\n",
      "407 5.80889500270132e-05\n",
      "408 5.654885899275541e-05\n",
      "409 5.5053067626431584e-05\n",
      "410 5.3594867495121434e-05\n",
      "411 5.2184739615768194e-05\n",
      "412 5.080714254290797e-05\n",
      "413 4.9472597311250865e-05\n",
      "414 4.81696697534062e-05\n",
      "415 4.690346395364031e-05\n",
      "416 4.567514042719267e-05\n",
      "417 4.44790166511666e-05\n",
      "418 4.3312509660609066e-05\n",
      "419 4.217982859699987e-05\n",
      "420 4.1076964407693595e-05\n",
      "421 4.000787521363236e-05\n",
      "422 3.896617272403091e-05\n",
      "423 3.795158409047872e-05\n",
      "424 3.69644294551108e-05\n",
      "425 3.600552736315876e-05\n",
      "426 3.50695445376914e-05\n",
      "427 3.416015169932507e-05\n",
      "428 3.3277960028499365e-05\n",
      "429 3.241487866034731e-05\n",
      "430 3.157741230097599e-05\n",
      "431 3.076071516261436e-05\n",
      "432 2.9969727620482445e-05\n",
      "433 2.919807593571022e-05\n",
      "434 2.844671325874515e-05\n",
      "435 2.771543040580582e-05\n",
      "436 2.7002719434676692e-05\n",
      "437 2.6312034606235102e-05\n",
      "438 2.5637542421463877e-05\n",
      "439 2.4977758585009724e-05\n",
      "440 2.4339424271602184e-05\n",
      "441 2.3719707314739935e-05\n",
      "442 2.3112341295927763e-05\n",
      "443 2.2522997824125923e-05\n",
      "444 2.1950403606751934e-05\n",
      "445 2.1391475456766784e-05\n",
      "446 2.0847817722824402e-05\n",
      "447 2.031668191193603e-05\n",
      "448 1.9800345398834907e-05\n",
      "449 1.929877544171177e-05\n",
      "450 1.8809971152222715e-05\n",
      "451 1.8332193576497957e-05\n",
      "452 1.7868987924885005e-05\n",
      "453 1.7417791241314262e-05\n",
      "454 1.6977848645183258e-05\n",
      "455 1.6549629435758106e-05\n",
      "456 1.6131556549225934e-05\n",
      "457 1.5726296624052338e-05\n",
      "458 1.532942224002909e-05\n",
      "459 1.49435054481728e-05\n",
      "460 1.4567881407856476e-05\n",
      "461 1.4202360034687445e-05\n",
      "462 1.3846407455275767e-05\n",
      "463 1.349956619378645e-05\n",
      "464 1.316066482104361e-05\n",
      "465 1.2830942978325766e-05\n",
      "466 1.251004414370982e-05\n",
      "467 1.2197495379950851e-05\n",
      "468 1.1893164810317103e-05\n",
      "469 1.1596292097237892e-05\n",
      "470 1.1307665772619657e-05\n",
      "471 1.1024786545021925e-05\n",
      "472 1.074989813787397e-05\n",
      "473 1.04830050986493e-05\n",
      "474 1.0222974196949508e-05\n",
      "475 9.969372513296548e-06\n",
      "476 9.721266906126402e-06\n",
      "477 9.480778317083605e-06\n",
      "478 9.244925422535744e-06\n",
      "479 9.017328920890577e-06\n",
      "480 8.792073458607774e-06\n",
      "481 8.576264008297585e-06\n",
      "482 8.36421713756863e-06\n",
      "483 8.156413059623446e-06\n",
      "484 7.955761248013005e-06\n",
      "485 7.758790161460638e-06\n",
      "486 7.567655302409548e-06\n",
      "487 7.381889645330375e-06\n",
      "488 7.199957508419175e-06\n",
      "489 7.022514182608575e-06\n",
      "490 6.850298632343765e-06\n",
      "491 6.681515515083447e-06\n",
      "492 6.517562269436894e-06\n",
      "493 6.357343863783171e-06\n",
      "494 6.2016406445764005e-06\n",
      "495 6.049476724001579e-06\n",
      "496 5.9008693824580405e-06\n",
      "497 5.756324298999971e-06\n",
      "498 5.6154976846301e-06\n",
      "499 5.47897070646286e-06\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a73b19dbc48c2583",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
