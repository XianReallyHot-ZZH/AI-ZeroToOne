{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Torch Study\n",
    "## Three main features of PyTorch\n",
    " - Tensor: similar to numpy.array but can run on GPUs\n",
    " - Autograd: automatic differentiation for all operations on Tensors\n",
    " - NN: nn.module, framework to build neural network easily\n",
    "\n",
    "## Materials\n",
    " - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    " - [PyTorch Examples](https://github.com/jcjohnson/pytorch-examples)\n",
    "\n",
    "## Examples\n",
    " - 3 Layers Neural Network\n",
    "   - Input Layer: 1000 neurons\n",
    "   - Hidden Layer: 100 neurons\n",
    "      - ReLU Activation Function\n",
    "   - Output Layer: 10 neurons\n",
    "- Training Data: 100 samples\n",
    "   - Learning Rate: 1e-6\n",
    "   - Training Iterations: 500"
   ],
   "id": "9bcfeb2e0b0d6c4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T13:36:28.774300Z",
     "start_time": "2025-07-27T13:36:28.413097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Numpy Version\n",
    "import numpy as np\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 1\n",
    "\n",
    "# generate the training data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# init the weights\n",
    "w1 = np.random.randn(D_in, D_h)\n",
    "w2 = np.random.randn(D_h, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.dot(w1)                                                           # N, D_h\n",
    "    h_relu = np.maximum(h, 0)                                   # N, D_h\n",
    "    y_pred = h_relu.dot(w2)                                         # N, D_out\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = np.square(y_pred - y).sum()                      # scalar\n",
    "    print(t, loss)\n",
    "\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)                                        # N, D_out\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)                                 # D_h, D_out\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)                                 # N, D_h\n",
    "    grad_h = grad_h_relu.copy()                                             # N, D_h\n",
    "    grad_h[h < 0] = 0                                                               # N, D_h\n",
    "    grad_w1 = x.T.dot(grad_h)                                               # D_in, D_h\n",
    "\n",
    "    # update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ],
   "id": "1ad41e84323dfa80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7786461.329196663\n",
      "1 31653739.27209103\n",
      "2 174139653.20773974\n",
      "3 748611884.9003179\n",
      "4 660017659.67774\n",
      "5 1409401.1021944047\n",
      "6 1094314.3589107117\n",
      "7 861348.2275402917\n",
      "8 686124.1930547106\n",
      "9 552430.196416511\n",
      "10 449261.62310414284\n",
      "11 368991.71385069535\n",
      "12 306016.8931984958\n",
      "13 256270.6603296833\n",
      "14 216707.94649342413\n",
      "15 185036.1500692812\n",
      "16 159532.07268017298\n",
      "17 138888.4788328411\n",
      "18 122055.51839409303\n",
      "19 108245.90365259226\n",
      "20 96853.06345443486\n",
      "21 87369.27764761692\n",
      "22 79414.02051803091\n",
      "23 72689.07021809494\n",
      "24 66959.97876360305\n",
      "25 62040.055094904135\n",
      "26 57779.91969219708\n",
      "27 54061.90806452672\n",
      "28 50790.89877960367\n",
      "29 47892.04253141788\n",
      "30 45305.86945023097\n",
      "31 42981.5948964014\n",
      "32 40878.544284637544\n",
      "33 38964.76887245237\n",
      "34 37214.02403589538\n",
      "35 35604.03251395711\n",
      "36 34117.11497562927\n",
      "37 32738.61810156912\n",
      "38 31455.84771363692\n",
      "39 30258.287984807408\n",
      "40 29136.62211627128\n",
      "41 28083.65250147671\n",
      "42 27093.315229974425\n",
      "43 26159.067981556687\n",
      "44 25276.00210460947\n",
      "45 24440.0222138235\n",
      "46 23647.367466968175\n",
      "47 22894.680387514512\n",
      "48 22179.089814305502\n",
      "49 21497.91118331912\n",
      "50 20848.888622956034\n",
      "51 20229.759961012966\n",
      "52 19638.494512355493\n",
      "53 19073.393398586944\n",
      "54 18532.768341519877\n",
      "55 18015.56997806911\n",
      "56 17520.1233668555\n",
      "57 17045.21961913426\n",
      "58 16589.806583906277\n",
      "59 16152.559920364862\n",
      "60 15732.533986790304\n",
      "61 15328.887071183923\n",
      "62 14940.868567457748\n",
      "63 14567.415369891121\n",
      "64 14207.947563368996\n",
      "65 13861.696103593327\n",
      "66 13527.749287989716\n",
      "67 13205.815954122678\n",
      "68 12895.269770783743\n",
      "69 12595.790225205377\n",
      "70 12306.766151687976\n",
      "71 12027.542668244732\n",
      "72 11757.755041706423\n",
      "73 11497.086610995384\n",
      "74 11244.974417271846\n",
      "75 11000.990559182905\n",
      "76 10764.883333437952\n",
      "77 10536.265391257879\n",
      "78 10314.772320751772\n",
      "79 10100.187840465087\n",
      "80 9892.243723256737\n",
      "81 9690.597118622407\n",
      "82 9495.080879655652\n",
      "83 9305.328255808457\n",
      "84 9120.95086386442\n",
      "85 8941.925234720573\n",
      "86 8768.022138434691\n",
      "87 8599.091330927411\n",
      "88 8434.97777331358\n",
      "89 8275.47626806245\n",
      "90 8120.417581638165\n",
      "91 7969.686002344042\n",
      "92 7823.032833879583\n",
      "93 7680.345898874438\n",
      "94 7541.45330927862\n",
      "95 7406.234418287009\n",
      "96 7274.541373488399\n",
      "97 7146.3061772626415\n",
      "98 7021.3696433877285\n",
      "99 6899.645412038714\n",
      "100 6780.997936754911\n",
      "101 6665.39870354662\n",
      "102 6552.634894140833\n",
      "103 6442.639968436468\n",
      "104 6335.342627939301\n",
      "105 6230.661698662404\n",
      "106 6128.519853266947\n",
      "107 6028.872897742034\n",
      "108 5931.560699288548\n",
      "109 5836.531866301624\n",
      "110 5743.759631202892\n",
      "111 5653.1671954991625\n",
      "112 5564.653112315071\n",
      "113 5478.167859815668\n",
      "114 5393.705780247693\n",
      "115 5311.201449520838\n",
      "116 5230.543566767413\n",
      "117 5151.689765761122\n",
      "118 5074.552258251479\n",
      "119 4999.113576490672\n",
      "120 4925.331336001884\n",
      "121 4853.162967287587\n",
      "122 4782.547114587859\n",
      "123 4713.425147398125\n",
      "124 4645.7626540865585\n",
      "125 4579.522507978377\n",
      "126 4514.674559970524\n",
      "127 4451.190766365016\n",
      "128 4389.02553594556\n",
      "129 4328.125835708916\n",
      "130 4268.476908763904\n",
      "131 4210.024137260584\n",
      "132 4152.754250501882\n",
      "133 4096.6311635913125\n",
      "134 4041.6290443076514\n",
      "135 3987.719888336534\n",
      "136 3934.853316446444\n",
      "137 3883.027147244621\n",
      "138 3832.2177620644416\n",
      "139 3782.3726459943377\n",
      "140 3733.501987087472\n",
      "141 3685.593970445565\n",
      "142 3638.561802861543\n",
      "143 3592.442730097911\n",
      "144 3547.1643753530134\n",
      "145 3502.731611224761\n",
      "146 3459.1233263427757\n",
      "147 3416.3358358482615\n",
      "148 3374.3288515555114\n",
      "149 3333.0787840613207\n",
      "150 3292.5796232753355\n",
      "151 3252.80670348058\n",
      "152 3213.7448143160054\n",
      "153 3175.389477744595\n",
      "154 3137.7080321388294\n",
      "155 3100.6817836745663\n",
      "156 3064.3009188268657\n",
      "157 3028.55687050304\n",
      "158 2993.4419638132977\n",
      "159 2958.9228547338116\n",
      "160 2925.005090904392\n",
      "161 2891.6595607450795\n",
      "162 2858.8797828858555\n",
      "163 2826.648481484504\n",
      "164 2794.952990458997\n",
      "165 2763.7776354342677\n",
      "166 2733.1293774299183\n",
      "167 2702.986566535705\n",
      "168 2673.333538725008\n",
      "169 2644.1701908663504\n",
      "170 2615.478758503942\n",
      "171 2587.252211264205\n",
      "172 2559.471656109384\n",
      "173 2532.1483488347567\n",
      "174 2505.2516298095466\n",
      "175 2478.7785828619108\n",
      "176 2452.718988960109\n",
      "177 2427.0662082223707\n",
      "178 2401.8171372508014\n",
      "179 2376.9987812895015\n",
      "180 2352.5846586239095\n",
      "181 2328.5479045415123\n",
      "182 2304.8687594487283\n",
      "183 2281.5412521933986\n",
      "184 2258.5701137507654\n",
      "185 2235.9464486214247\n",
      "186 2213.665239540945\n",
      "187 2191.7084241470648\n",
      "188 2170.079103740489\n",
      "189 2148.76799666185\n",
      "190 2127.7742718376553\n",
      "191 2107.0864099140936\n",
      "192 2086.7013574687967\n",
      "193 2066.607788884286\n",
      "194 2046.8025756371787\n",
      "195 2027.2789872067497\n",
      "196 2008.0338063013496\n",
      "197 1989.0636276231974\n",
      "198 1970.3596952733378\n",
      "199 1951.919826283222\n",
      "200 1933.7516042728992\n",
      "201 1915.8264316384007\n",
      "202 1898.1494094694033\n",
      "203 1880.7148854589148\n",
      "204 1863.522168939313\n",
      "205 1846.5624902641403\n",
      "206 1829.8358511585186\n",
      "207 1813.3316337837562\n",
      "208 1797.0477249012858\n",
      "209 1780.9854135052935\n",
      "210 1765.1414870925225\n",
      "211 1749.5011423316998\n",
      "212 1734.0655155422708\n",
      "213 1718.8341883931355\n",
      "214 1703.8023705331161\n",
      "215 1688.96258051767\n",
      "216 1674.3174521859466\n",
      "217 1659.8682442107518\n",
      "218 1645.5984007783782\n",
      "219 1631.512192971003\n",
      "220 1617.6031684324373\n",
      "221 1603.8711128482337\n",
      "222 1590.3088247616015\n",
      "223 1576.9203558656418\n",
      "224 1563.693333783802\n",
      "225 1550.633665896382\n",
      "226 1537.733711551657\n",
      "227 1525.0015822489606\n",
      "228 1512.4597214652467\n",
      "229 1500.074584148761\n",
      "230 1487.836979282133\n",
      "231 1475.7508100985094\n",
      "232 1463.8075076884938\n",
      "233 1452.0083637945766\n",
      "234 1440.358370412945\n",
      "235 1428.8383828919593\n",
      "236 1417.4587072175786\n",
      "237 1406.210004136356\n",
      "238 1395.0951705730422\n",
      "239 1384.1065607194223\n",
      "240 1373.2458280043863\n",
      "241 1362.5114961309682\n",
      "242 1351.907735556562\n",
      "243 1341.4247430848538\n",
      "244 1331.0615863555693\n",
      "245 1320.8143645829227\n",
      "246 1310.6825717262961\n",
      "247 1300.6697911312233\n",
      "248 1290.765068363632\n",
      "249 1280.9715293558386\n",
      "250 1271.2929064117354\n",
      "251 1261.7162667018345\n",
      "252 1252.2462064141357\n",
      "253 1242.879418021757\n",
      "254 1233.6130676524556\n",
      "255 1224.452962963431\n",
      "256 1215.4026820665156\n",
      "257 1206.4460889092386\n",
      "258 1197.5844087928558\n",
      "259 1188.8172862645417\n",
      "260 1180.1445429993325\n",
      "261 1171.5665782966491\n",
      "262 1163.0780025263566\n",
      "263 1154.6775661106735\n",
      "264 1146.3673163472647\n",
      "265 1138.1442084316366\n",
      "266 1130.0096004377929\n",
      "267 1121.9543455452638\n",
      "268 1113.9820429852814\n",
      "269 1106.0921058355045\n",
      "270 1098.284222834143\n",
      "271 1090.5562250771932\n",
      "272 1082.9058795166873\n",
      "273 1075.3337957724516\n",
      "274 1067.8384146398612\n",
      "275 1060.4163947604948\n",
      "276 1053.0686273904755\n",
      "277 1045.7951576758203\n",
      "278 1038.5944680413243\n",
      "279 1031.465076292258\n",
      "280 1024.4050631135385\n",
      "281 1017.4164085286116\n",
      "282 1010.5032246773234\n",
      "283 1003.651738749008\n",
      "284 996.8658046238992\n",
      "285 990.1473002646687\n",
      "286 983.4919604489769\n",
      "287 976.9002879514536\n",
      "288 970.3734447180614\n",
      "289 963.9073872573081\n",
      "290 957.5031305046076\n",
      "291 951.1604215664703\n",
      "292 944.8770840517582\n",
      "293 938.6532327403828\n",
      "294 932.4864155506411\n",
      "295 926.3773249100043\n",
      "296 920.3247569674681\n",
      "297 914.3318260335187\n",
      "298 908.3937768137748\n",
      "299 902.5095997299084\n",
      "300 896.6781659634637\n",
      "301 890.8991967279903\n",
      "302 885.1723417865992\n",
      "303 879.4972845646366\n",
      "304 873.8751596620394\n",
      "305 868.3043932497064\n",
      "306 862.7827365992817\n",
      "307 857.3146778036885\n",
      "308 851.8926170794891\n",
      "309 846.516973657871\n",
      "310 841.1959553841905\n",
      "311 835.9233394305519\n",
      "312 830.6963667392491\n",
      "313 825.5183697220118\n",
      "314 820.3836248344796\n",
      "315 815.2956384248836\n",
      "316 810.2512891661661\n",
      "317 805.2486653957307\n",
      "318 800.2890594139359\n",
      "319 795.3710818228336\n",
      "320 790.4953468634749\n",
      "321 785.661827315909\n",
      "322 780.871916701498\n",
      "323 776.1249192652822\n",
      "324 771.415942419384\n",
      "325 766.7458296368657\n",
      "326 762.1147711559225\n",
      "327 757.5231505172056\n",
      "328 752.9713071416018\n",
      "329 748.458635069471\n",
      "330 743.9814212062315\n",
      "331 739.5432365556645\n",
      "332 735.138678395993\n",
      "333 730.7712435991266\n",
      "334 726.4390571910608\n",
      "335 722.1421902968312\n",
      "336 717.8796910931227\n",
      "337 713.6517843817763\n",
      "338 709.4594538271971\n",
      "339 705.3042884154672\n",
      "340 701.1790395719484\n",
      "341 697.0859172348537\n",
      "342 693.0257353870677\n",
      "343 688.9983179307691\n",
      "344 685.0027876698587\n",
      "345 681.0427984243578\n",
      "346 677.1139454878945\n",
      "347 673.2138403900367\n",
      "348 669.342974112678\n",
      "349 665.5025293797214\n",
      "350 661.6929314719963\n",
      "351 657.9134608302311\n",
      "352 654.1626092633759\n",
      "353 650.4417531191922\n",
      "354 646.7511359714667\n",
      "355 643.087087641934\n",
      "356 639.4514406947554\n",
      "357 635.8430703033938\n",
      "358 632.2623307057929\n",
      "359 628.709071380026\n",
      "360 625.183982586017\n",
      "361 621.6866411341359\n",
      "362 618.2179054936148\n",
      "363 614.7724759447942\n",
      "364 611.3523185675866\n",
      "365 607.9572729668582\n",
      "366 604.587903361605\n",
      "367 601.245119931809\n",
      "368 597.9316549785041\n",
      "369 594.6503418243552\n",
      "370 591.3923573421097\n",
      "371 588.1584946252185\n",
      "372 584.9487805575679\n",
      "373 581.762539691902\n",
      "374 578.6004587530101\n",
      "375 575.4608967060035\n",
      "376 572.3446468564391\n",
      "377 569.250941492907\n",
      "378 566.1818605607248\n",
      "379 563.1332244548777\n",
      "380 560.1065048928901\n",
      "381 557.1011512239422\n",
      "382 554.117882882824\n",
      "383 551.1572528312223\n",
      "384 548.2172507334715\n",
      "385 545.2976852525967\n",
      "386 542.3989680029377\n",
      "387 539.522670062765\n",
      "388 536.6651916882953\n",
      "389 533.8278140996521\n",
      "390 531.0103705664527\n",
      "391 528.2138937881339\n",
      "392 525.4359762848634\n",
      "393 522.6782752775629\n",
      "394 519.9395091923869\n",
      "395 517.2224909039844\n",
      "396 514.5214050321181\n",
      "397 511.8392139391475\n",
      "398 509.17679103776356\n",
      "399 506.53171182800224\n",
      "400 503.9051917177244\n",
      "401 501.29622312453745\n",
      "402 498.7052903409833\n",
      "403 496.1323890722572\n",
      "404 493.57737006562974\n",
      "405 491.03978734403637\n",
      "406 488.51959096585693\n",
      "407 486.015797119978\n",
      "408 483.52876017207103\n",
      "409 481.0588580017801\n",
      "410 478.60541268634086\n",
      "411 476.16842283677346\n",
      "412 473.7517411453179\n",
      "413 471.34829495008455\n",
      "414 468.96069684991454\n",
      "415 466.58826964485354\n",
      "416 464.2314459200026\n",
      "417 461.8904774554699\n",
      "418 459.5654022560928\n",
      "419 457.25640614910157\n",
      "420 454.96272757254457\n",
      "421 452.68332826308284\n",
      "422 450.4186180164843\n",
      "423 448.1690984869249\n",
      "424 445.9345873823977\n",
      "425 443.7142676073388\n",
      "426 441.50860358116466\n",
      "427 439.31772358794285\n",
      "428 437.14056866300797\n",
      "429 434.979645883557\n",
      "430 432.83156180561656\n",
      "431 430.696565267875\n",
      "432 428.5747965068672\n",
      "433 426.4671241427795\n",
      "434 424.3734505324581\n",
      "435 422.2923597864753\n",
      "436 420.22492142292805\n",
      "437 418.17085538478244\n",
      "438 416.1300425126973\n",
      "439 414.1018204672356\n",
      "440 412.0865601142988\n",
      "441 410.0842489975105\n",
      "442 408.09413439212005\n",
      "443 406.1168505080283\n",
      "444 404.1520436503522\n",
      "445 402.19903995499385\n",
      "446 400.25907314257194\n",
      "447 398.33256787405423\n",
      "448 396.41754768897755\n",
      "449 394.51329703588635\n",
      "450 392.62047902610436\n",
      "451 390.7395274303397\n",
      "452 388.87036484052715\n",
      "453 387.01299091246665\n",
      "454 385.1680174977026\n",
      "455 383.3340492052211\n",
      "456 381.5105849875801\n",
      "457 379.6982448497129\n",
      "458 377.89711746095963\n",
      "459 376.1072143618651\n",
      "460 374.32837564035947\n",
      "461 372.56021572913227\n",
      "462 370.8031060690166\n",
      "463 369.0574901337013\n",
      "464 367.3219627651882\n",
      "465 365.5990346439621\n",
      "466 363.8846576544838\n",
      "467 362.18047557682956\n",
      "468 360.4869371353655\n",
      "469 358.8033497930105\n",
      "470 357.12946697038717\n",
      "471 355.46592226098386\n",
      "472 353.812694777614\n",
      "473 352.1687127231299\n",
      "474 350.5666317513355\n",
      "475 348.9870285630536\n",
      "476 347.4175037228049\n",
      "477 345.8574247447283\n",
      "478 344.30645753423084\n",
      "479 342.7644777667899\n",
      "480 341.2318833440199\n",
      "481 339.70871274092696\n",
      "482 338.19472029422445\n",
      "483 336.6899589034283\n",
      "484 335.1938239614672\n",
      "485 333.70660094605347\n",
      "486 332.2281242492901\n",
      "487 330.7587123454391\n",
      "488 329.29793107491474\n",
      "489 327.84565141977197\n",
      "490 326.40164641889316\n",
      "491 324.9660933725093\n",
      "492 323.5386455563838\n",
      "493 322.1196857798933\n",
      "494 320.73067301621563\n",
      "495 319.35064287900207\n",
      "496 317.9788967846045\n",
      "497 316.61482565553166\n",
      "498 315.25909163714783\n",
      "499 313.9112755985869\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T06:44:53.778140Z",
     "start_time": "2025-07-31T06:44:50.708374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tensor Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate the training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device)\n",
    "w2 = torch.randn(D_h, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # back-propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # update the weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ],
   "id": "ab3483953fa258ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 49954264.0\n",
      "1 125549504.0\n",
      "2 495385408.0\n",
      "3 767501824.0\n",
      "4 15511727.0\n",
      "5 9328604.0\n",
      "6 6337955.0\n",
      "7 4604965.0\n",
      "8 3498550.0\n",
      "9 2746234.0\n",
      "10 2212082.0\n",
      "11 1820547.25\n",
      "12 1525997.625\n",
      "13 1299669.5\n",
      "14 1122867.5\n",
      "15 982837.0625\n",
      "16 870294.9375\n",
      "17 778595.75\n",
      "18 702857.9375\n",
      "19 639709.5625\n",
      "20 586447.0625\n",
      "21 541004.0625\n",
      "22 501734.375\n",
      "23 467522.0\n",
      "24 437451.0625\n",
      "25 410769.6875\n",
      "26 386917.875\n",
      "27 365490.1875\n",
      "28 346132.1875\n",
      "29 328551.21875\n",
      "30 312489.9375\n",
      "31 297740.15625\n",
      "32 284136.90625\n",
      "33 271539.75\n",
      "34 259829.8125\n",
      "35 248911.09375\n",
      "36 238714.609375\n",
      "37 229162.71875\n",
      "38 220210.796875\n",
      "39 211790.296875\n",
      "40 203855.09375\n",
      "41 196367.96875\n",
      "42 189292.078125\n",
      "43 182593.71875\n",
      "44 176248.203125\n",
      "45 170229.734375\n",
      "46 164520.1875\n",
      "47 159102.53125\n",
      "48 153959.5\n",
      "49 149060.171875\n",
      "50 144387.203125\n",
      "51 139928.796875\n",
      "52 135662.375\n",
      "53 131582.46875\n",
      "54 127685.3359375\n",
      "55 123967.015625\n",
      "56 120404.5859375\n",
      "57 116995.609375\n",
      "58 113724.9453125\n",
      "59 110587.296875\n",
      "60 107573.4375\n",
      "61 104681.515625\n",
      "62 101906.1328125\n",
      "63 99235.0234375\n",
      "64 96667.15625\n",
      "65 94195.390625\n",
      "66 91813.234375\n",
      "67 89526.0390625\n",
      "68 87319.234375\n",
      "69 85192.4921875\n",
      "70 83144.046875\n",
      "71 81166.21875\n",
      "72 79256.390625\n",
      "73 77412.0546875\n",
      "74 75628.5703125\n",
      "75 73904.4375\n",
      "76 72238.3125\n",
      "77 70624.7890625\n",
      "78 69064.265625\n",
      "79 67552.609375\n",
      "80 66090.375\n",
      "81 64673.75\n",
      "82 63300.08984375\n",
      "83 61969.8046875\n",
      "84 60679.65625\n",
      "85 59426.92578125\n",
      "86 58211.26953125\n",
      "87 57030.3671875\n",
      "88 55883.25\n",
      "89 54770.28125\n",
      "90 53690.92578125\n",
      "91 52644.45703125\n",
      "92 51628.4296875\n",
      "93 50639.28125\n",
      "94 49677.33984375\n",
      "95 48741.51171875\n",
      "96 47831.09375\n",
      "97 46944.7890625\n",
      "98 46081.53125\n",
      "99 45241.125\n",
      "100 44422.0078125\n",
      "101 43623.21875\n",
      "102 42845.046875\n",
      "103 42085.921875\n",
      "104 41346.75390625\n",
      "105 40626.0703125\n",
      "106 39922.71484375\n",
      "107 39236.4921875\n",
      "108 38567.4609375\n",
      "109 37914.3046875\n",
      "110 37276.69921875\n",
      "111 36654.2421875\n",
      "112 36046.53125\n",
      "113 35453.07421875\n",
      "114 34873.390625\n",
      "115 34306.98046875\n",
      "116 33752.828125\n",
      "117 33211.34375\n",
      "118 32681.310546875\n",
      "119 32163.126953125\n",
      "120 31656.7109375\n",
      "121 31161.21875\n",
      "122 30676.416015625\n",
      "123 30202.73828125\n",
      "124 29739.08984375\n",
      "125 29285.890625\n",
      "126 28843.40625\n",
      "127 28409.908203125\n",
      "128 27985.41015625\n",
      "129 27569.80859375\n",
      "130 27161.806640625\n",
      "131 26762.11328125\n",
      "132 26371.0078125\n",
      "133 25987.912109375\n",
      "134 25612.296875\n",
      "135 25244.337890625\n",
      "136 24884.1015625\n",
      "137 24530.55078125\n",
      "138 24183.884765625\n",
      "139 23843.9765625\n",
      "140 23511.9375\n",
      "141 23186.26953125\n",
      "142 22866.8203125\n",
      "143 22553.96875\n",
      "144 22247.078125\n",
      "145 21945.98828125\n",
      "146 21650.5625\n",
      "147 21361.1796875\n",
      "148 21077.271484375\n",
      "149 20798.583984375\n",
      "150 20524.90625\n",
      "151 20256.19140625\n",
      "152 19992.6953125\n",
      "153 19734.1640625\n",
      "154 19480.267578125\n",
      "155 19230.689453125\n",
      "156 18985.4375\n",
      "157 18744.578125\n",
      "158 18507.939453125\n",
      "159 18275.5390625\n",
      "160 18047.095703125\n",
      "161 17822.58203125\n",
      "162 17601.9375\n",
      "163 17385.046875\n",
      "164 17171.740234375\n",
      "165 16962.0546875\n",
      "166 16755.962890625\n",
      "167 16553.5078125\n",
      "168 16354.408203125\n",
      "169 16158.6142578125\n",
      "170 15966.12890625\n",
      "171 15776.751953125\n",
      "172 15590.4658203125\n",
      "173 15407.2421875\n",
      "174 15227.0283203125\n",
      "175 15049.7890625\n",
      "176 14875.400390625\n",
      "177 14703.689453125\n",
      "178 14534.7841796875\n",
      "179 14368.716796875\n",
      "180 14205.0537109375\n",
      "181 14043.923828125\n",
      "182 13885.337890625\n",
      "183 13729.2314453125\n",
      "184 13575.4716796875\n",
      "185 13424.126953125\n",
      "186 13275.1416015625\n",
      "187 13128.4091796875\n",
      "188 12983.908203125\n",
      "189 12841.6171875\n",
      "190 12701.453125\n",
      "191 12560.134765625\n",
      "192 12421.0625\n",
      "193 12284.2060546875\n",
      "194 12149.5234375\n",
      "195 12016.919921875\n",
      "196 11886.42578125\n",
      "197 11757.943359375\n",
      "198 11631.3798828125\n",
      "199 11506.7529296875\n",
      "200 11383.9931640625\n",
      "201 11263.076171875\n",
      "202 11143.986328125\n",
      "203 11026.662109375\n",
      "204 10911.283203125\n",
      "205 10797.681640625\n",
      "206 10685.5341796875\n",
      "207 10574.96875\n",
      "208 10466.0458984375\n",
      "209 10358.654296875\n",
      "210 10252.9580078125\n",
      "211 10148.74609375\n",
      "212 10045.9873046875\n",
      "213 9944.7119140625\n",
      "214 9844.8681640625\n",
      "215 9746.3779296875\n",
      "216 9649.2880859375\n",
      "217 9553.5\n",
      "218 9459.0419921875\n",
      "219 9365.9091796875\n",
      "220 9274.01171875\n",
      "221 9183.3720703125\n",
      "222 9093.98046875\n",
      "223 9005.763671875\n",
      "224 8918.8037109375\n",
      "225 8832.896484375\n",
      "226 8748.119140625\n",
      "227 8664.4814453125\n",
      "228 8581.9462890625\n",
      "229 8500.5244140625\n",
      "230 8420.173828125\n",
      "231 8340.85546875\n",
      "232 8262.697265625\n",
      "233 8185.529296875\n",
      "234 8109.3642578125\n",
      "235 8034.1923828125\n",
      "236 7959.92822265625\n",
      "237 7886.65234375\n",
      "238 7814.298828125\n",
      "239 7742.873046875\n",
      "240 7672.328125\n",
      "241 7602.67333984375\n",
      "242 7533.8857421875\n",
      "243 7465.9716796875\n",
      "244 7398.90576171875\n",
      "245 7332.7080078125\n",
      "246 7267.27587890625\n",
      "247 7202.619140625\n",
      "248 7138.8583984375\n",
      "249 7075.890625\n",
      "250 7013.7138671875\n",
      "251 6952.2802734375\n",
      "252 6891.5654296875\n",
      "253 6831.5859375\n",
      "254 6772.31103515625\n",
      "255 6713.7548828125\n",
      "256 6655.89990234375\n",
      "257 6598.74462890625\n",
      "258 6542.259765625\n",
      "259 6486.4267578125\n",
      "260 6431.2734375\n",
      "261 6376.7685546875\n",
      "262 6322.8681640625\n",
      "263 6269.59765625\n",
      "264 6216.97265625\n",
      "265 6164.94921875\n",
      "266 6113.55078125\n",
      "267 6062.7255859375\n",
      "268 6012.49609375\n",
      "269 5962.857421875\n",
      "270 5913.79150390625\n",
      "271 5865.2861328125\n",
      "272 5817.296875\n",
      "273 5769.8603515625\n",
      "274 5722.9482421875\n",
      "275 5676.5869140625\n",
      "276 5630.7490234375\n",
      "277 5585.337890625\n",
      "278 5540.4140625\n",
      "279 5496.13330078125\n",
      "280 5452.52880859375\n",
      "281 5409.41552734375\n",
      "282 5366.79736328125\n",
      "283 5324.68994140625\n",
      "284 5283.04833984375\n",
      "285 5241.85546875\n",
      "286 5201.10302734375\n",
      "287 5160.7978515625\n",
      "288 5120.95263671875\n",
      "289 5081.51708984375\n",
      "290 5042.5234375\n",
      "291 5003.9423828125\n",
      "292 4965.716796875\n",
      "293 4927.88916015625\n",
      "294 4890.44677734375\n",
      "295 4853.421875\n",
      "296 4816.78662109375\n",
      "297 4780.52783203125\n",
      "298 4744.64404296875\n",
      "299 4709.1474609375\n",
      "300 4674.02587890625\n",
      "301 4639.26513671875\n",
      "302 4604.9169921875\n",
      "303 4570.9306640625\n",
      "304 4537.314453125\n",
      "305 4504.0283203125\n",
      "306 4471.0849609375\n",
      "307 4438.478515625\n",
      "308 4406.20947265625\n",
      "309 4374.2822265625\n",
      "310 4342.6513671875\n",
      "311 4311.35546875\n",
      "312 4280.35498046875\n",
      "313 4249.66162109375\n",
      "314 4219.28857421875\n",
      "315 4189.2265625\n",
      "316 4159.443359375\n",
      "317 4129.9580078125\n",
      "318 4100.7626953125\n",
      "319 4071.8720703125\n",
      "320 4043.27294921875\n",
      "321 4014.939208984375\n",
      "322 3986.89111328125\n",
      "323 3959.1328125\n",
      "324 3931.658203125\n",
      "325 3904.451416015625\n",
      "326 3877.525390625\n",
      "327 3850.834228515625\n",
      "328 3824.395751953125\n",
      "329 3798.227294921875\n",
      "330 3772.315185546875\n",
      "331 3746.6396484375\n",
      "332 3721.208740234375\n",
      "333 3696.016357421875\n",
      "334 3671.05859375\n",
      "335 3646.331298828125\n",
      "336 3621.833251953125\n",
      "337 3597.564453125\n",
      "338 3573.52880859375\n",
      "339 3549.720703125\n",
      "340 3526.1318359375\n",
      "341 3502.749267578125\n",
      "342 3479.630126953125\n",
      "343 3456.75146484375\n",
      "344 3434.08349609375\n",
      "345 3411.622314453125\n",
      "346 3389.3564453125\n",
      "347 3367.298828125\n",
      "348 3345.439453125\n",
      "349 3323.785888671875\n",
      "350 3302.328857421875\n",
      "351 3281.04736328125\n",
      "352 3259.9619140625\n",
      "353 3239.068359375\n",
      "354 3218.35205078125\n",
      "355 3197.8193359375\n",
      "356 3177.47802734375\n",
      "357 3157.322509765625\n",
      "358 3137.33154296875\n",
      "359 3117.524658203125\n",
      "360 3097.87841796875\n",
      "361 3078.40185546875\n",
      "362 3059.101318359375\n",
      "363 3039.974609375\n",
      "364 3021.009033203125\n",
      "365 3002.20947265625\n",
      "366 2983.563232421875\n",
      "367 2965.088623046875\n",
      "368 2946.768798828125\n",
      "369 2928.60302734375\n",
      "370 2910.59375\n",
      "371 2892.740234375\n",
      "372 2875.04833984375\n",
      "373 2857.494140625\n",
      "374 2840.0869140625\n",
      "375 2822.829345703125\n",
      "376 2805.725830078125\n",
      "377 2788.7490234375\n",
      "378 2771.917236328125\n",
      "379 2755.20703125\n",
      "380 2738.639892578125\n",
      "381 2722.213134765625\n",
      "382 2705.92138671875\n",
      "383 2689.765625\n",
      "384 2673.73974609375\n",
      "385 2657.849609375\n",
      "386 2642.09326171875\n",
      "387 2626.46142578125\n",
      "388 2610.95751953125\n",
      "389 2595.586669921875\n",
      "390 2580.332275390625\n",
      "391 2565.2099609375\n",
      "392 2550.204833984375\n",
      "393 2535.315673828125\n",
      "394 2520.54736328125\n",
      "395 2505.9111328125\n",
      "396 2491.38037109375\n",
      "397 2476.963623046875\n",
      "398 2462.66552734375\n",
      "399 2448.48291015625\n",
      "400 2434.723388671875\n",
      "401 2421.165283203125\n",
      "402 2407.70068359375\n",
      "403 2394.341552734375\n",
      "404 2381.09814453125\n",
      "405 2367.959716796875\n",
      "406 2354.924560546875\n",
      "407 2341.9931640625\n",
      "408 2329.16259765625\n",
      "409 2316.437744140625\n",
      "410 2303.80908203125\n",
      "411 2291.274658203125\n",
      "412 2278.841552734375\n",
      "413 2266.5087890625\n",
      "414 2254.26318359375\n",
      "415 2242.1201171875\n",
      "416 2230.0673828125\n",
      "417 2218.114990234375\n",
      "418 2206.25244140625\n",
      "419 2194.480224609375\n",
      "420 2182.79638671875\n",
      "421 2171.203125\n",
      "422 2159.69482421875\n",
      "423 2148.275390625\n",
      "424 2136.94384765625\n",
      "425 2125.70458984375\n",
      "426 2114.544677734375\n",
      "427 2103.4658203125\n",
      "428 2092.46923828125\n",
      "429 2081.55712890625\n",
      "430 2070.724853515625\n",
      "431 2059.97412109375\n",
      "432 2049.299560546875\n",
      "433 2038.71435546875\n",
      "434 2028.2008056640625\n",
      "435 2017.763427734375\n",
      "436 2007.4066162109375\n",
      "437 1997.1224365234375\n",
      "438 1986.912353515625\n",
      "439 1976.77734375\n",
      "440 1966.7138671875\n",
      "441 1956.7269287109375\n",
      "442 1946.81689453125\n",
      "443 1936.9744873046875\n",
      "444 1927.206787109375\n",
      "445 1917.50732421875\n",
      "446 1907.8736572265625\n",
      "447 1898.30908203125\n",
      "448 1888.830322265625\n",
      "449 1879.416748046875\n",
      "450 1870.0709228515625\n",
      "451 1860.7958984375\n",
      "452 1851.5911865234375\n",
      "453 1842.4462890625\n",
      "454 1833.362060546875\n",
      "455 1824.34765625\n",
      "456 1815.39599609375\n",
      "457 1806.503173828125\n",
      "458 1797.672119140625\n",
      "459 1788.9085693359375\n",
      "460 1780.2047119140625\n",
      "461 1771.5609130859375\n",
      "462 1762.979248046875\n",
      "463 1754.462158203125\n",
      "464 1745.998779296875\n",
      "465 1737.59619140625\n",
      "466 1729.24462890625\n",
      "467 1720.9560546875\n",
      "468 1712.7279052734375\n",
      "469 1704.55419921875\n",
      "470 1696.4378662109375\n",
      "471 1688.381103515625\n",
      "472 1680.3785400390625\n",
      "473 1672.431640625\n",
      "474 1664.5369873046875\n",
      "475 1656.697265625\n",
      "476 1648.9085693359375\n",
      "477 1641.17626953125\n",
      "478 1633.494140625\n",
      "479 1625.8602294921875\n",
      "480 1618.283447265625\n",
      "481 1610.7618408203125\n",
      "482 1603.289306640625\n",
      "483 1595.859375\n",
      "484 1588.48291015625\n",
      "485 1581.1552734375\n",
      "486 1573.8812255859375\n",
      "487 1566.6707763671875\n",
      "488 1559.5235595703125\n",
      "489 1552.4268798828125\n",
      "490 1545.37841796875\n",
      "491 1538.377685546875\n",
      "492 1531.420654296875\n",
      "493 1524.5089111328125\n",
      "494 1517.6466064453125\n",
      "495 1510.8272705078125\n",
      "496 1504.0504150390625\n",
      "497 1497.3231201171875\n",
      "498 1490.6365966796875\n",
      "499 1483.9967041015625\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T06:50:39.026201Z",
     "start_time": "2025-07-31T06:50:38.627429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Autograd Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, requires_grad=True)\n",
    "\n",
    "# training\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # use autograd to do the back-propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights, prevent torch do autograd for this part\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # zero grads\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "id": "b4cba1b62ee3df49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(40031508., grad_fn=<SumBackward0>)\n",
      "1 tensor(71834512., grad_fn=<SumBackward0>)\n",
      "2 tensor(3.2308e+08, grad_fn=<SumBackward0>)\n",
      "3 tensor(1.0019e+09, grad_fn=<SumBackward0>)\n",
      "4 tensor(1.6766e+08, grad_fn=<SumBackward0>)\n",
      "5 tensor(26556596., grad_fn=<SumBackward0>)\n",
      "6 tensor(15460080., grad_fn=<SumBackward0>)\n",
      "7 tensor(10649197., grad_fn=<SumBackward0>)\n",
      "8 tensor(7943660., grad_fn=<SumBackward0>)\n",
      "9 tensor(6229142., grad_fn=<SumBackward0>)\n",
      "10 tensor(5051521.5000, grad_fn=<SumBackward0>)\n",
      "11 tensor(4200188., grad_fn=<SumBackward0>)\n",
      "12 tensor(3566603.5000, grad_fn=<SumBackward0>)\n",
      "13 tensor(3075790., grad_fn=<SumBackward0>)\n",
      "14 tensor(2683527.5000, grad_fn=<SumBackward0>)\n",
      "15 tensor(2365857.2500, grad_fn=<SumBackward0>)\n",
      "16 tensor(2105503.7500, grad_fn=<SumBackward0>)\n",
      "17 tensor(1887601.5000, grad_fn=<SumBackward0>)\n",
      "18 tensor(1703584.8750, grad_fn=<SumBackward0>)\n",
      "19 tensor(1547180.7500, grad_fn=<SumBackward0>)\n",
      "20 tensor(1411691.7500, grad_fn=<SumBackward0>)\n",
      "21 tensor(1293940.6250, grad_fn=<SumBackward0>)\n",
      "22 tensor(1190767.1250, grad_fn=<SumBackward0>)\n",
      "23 tensor(1099419.3750, grad_fn=<SumBackward0>)\n",
      "24 tensor(1018300.2500, grad_fn=<SumBackward0>)\n",
      "25 tensor(946156., grad_fn=<SumBackward0>)\n",
      "26 tensor(881529.8750, grad_fn=<SumBackward0>)\n",
      "27 tensor(823269.1250, grad_fn=<SumBackward0>)\n",
      "28 tensor(770423.2500, grad_fn=<SumBackward0>)\n",
      "29 tensor(722636.4375, grad_fn=<SumBackward0>)\n",
      "30 tensor(679270.4375, grad_fn=<SumBackward0>)\n",
      "31 tensor(639565.1250, grad_fn=<SumBackward0>)\n",
      "32 tensor(603216.2500, grad_fn=<SumBackward0>)\n",
      "33 tensor(569815.5000, grad_fn=<SumBackward0>)\n",
      "34 tensor(539139.5000, grad_fn=<SumBackward0>)\n",
      "35 tensor(510875.8125, grad_fn=<SumBackward0>)\n",
      "36 tensor(484784.2812, grad_fn=<SumBackward0>)\n",
      "37 tensor(460586., grad_fn=<SumBackward0>)\n",
      "38 tensor(438138.3750, grad_fn=<SumBackward0>)\n",
      "39 tensor(417323.5312, grad_fn=<SumBackward0>)\n",
      "40 tensor(397956.1562, grad_fn=<SumBackward0>)\n",
      "41 tensor(379869.6562, grad_fn=<SumBackward0>)\n",
      "42 tensor(362994.7500, grad_fn=<SumBackward0>)\n",
      "43 tensor(347191.2500, grad_fn=<SumBackward0>)\n",
      "44 tensor(332422.3438, grad_fn=<SumBackward0>)\n",
      "45 tensor(318533.2188, grad_fn=<SumBackward0>)\n",
      "46 tensor(305471.0625, grad_fn=<SumBackward0>)\n",
      "47 tensor(293167.5625, grad_fn=<SumBackward0>)\n",
      "48 tensor(281589.6562, grad_fn=<SumBackward0>)\n",
      "49 tensor(270697.7500, grad_fn=<SumBackward0>)\n",
      "50 tensor(260409.2812, grad_fn=<SumBackward0>)\n",
      "51 tensor(250655.5625, grad_fn=<SumBackward0>)\n",
      "52 tensor(241428.8594, grad_fn=<SumBackward0>)\n",
      "53 tensor(232694.0156, grad_fn=<SumBackward0>)\n",
      "54 tensor(224413.1719, grad_fn=<SumBackward0>)\n",
      "55 tensor(216595.4688, grad_fn=<SumBackward0>)\n",
      "56 tensor(209178.2812, grad_fn=<SumBackward0>)\n",
      "57 tensor(202129.4375, grad_fn=<SumBackward0>)\n",
      "58 tensor(195429.0156, grad_fn=<SumBackward0>)\n",
      "59 tensor(189054.8125, grad_fn=<SumBackward0>)\n",
      "60 tensor(182988.0312, grad_fn=<SumBackward0>)\n",
      "61 tensor(177211.7188, grad_fn=<SumBackward0>)\n",
      "62 tensor(171710.7969, grad_fn=<SumBackward0>)\n",
      "63 tensor(166459.0938, grad_fn=<SumBackward0>)\n",
      "64 tensor(161442.7500, grad_fn=<SumBackward0>)\n",
      "65 tensor(156647.0469, grad_fn=<SumBackward0>)\n",
      "66 tensor(152062.0469, grad_fn=<SumBackward0>)\n",
      "67 tensor(147674.2031, grad_fn=<SumBackward0>)\n",
      "68 tensor(143476.4688, grad_fn=<SumBackward0>)\n",
      "69 tensor(139457.2500, grad_fn=<SumBackward0>)\n",
      "70 tensor(135608.7656, grad_fn=<SumBackward0>)\n",
      "71 tensor(131921.7188, grad_fn=<SumBackward0>)\n",
      "72 tensor(128381.5625, grad_fn=<SumBackward0>)\n",
      "73 tensor(124983.2891, grad_fn=<SumBackward0>)\n",
      "74 tensor(121727.2188, grad_fn=<SumBackward0>)\n",
      "75 tensor(118594.5703, grad_fn=<SumBackward0>)\n",
      "76 tensor(115587.9375, grad_fn=<SumBackward0>)\n",
      "77 tensor(112698.4531, grad_fn=<SumBackward0>)\n",
      "78 tensor(109920.7031, grad_fn=<SumBackward0>)\n",
      "79 tensor(107244.6406, grad_fn=<SumBackward0>)\n",
      "80 tensor(104677.1562, grad_fn=<SumBackward0>)\n",
      "81 tensor(102206.5781, grad_fn=<SumBackward0>)\n",
      "82 tensor(99831.7266, grad_fn=<SumBackward0>)\n",
      "83 tensor(97542.0312, grad_fn=<SumBackward0>)\n",
      "84 tensor(95336.8438, grad_fn=<SumBackward0>)\n",
      "85 tensor(93211.5312, grad_fn=<SumBackward0>)\n",
      "86 tensor(91160.4375, grad_fn=<SumBackward0>)\n",
      "87 tensor(89178.8906, grad_fn=<SumBackward0>)\n",
      "88 tensor(87262.8516, grad_fn=<SumBackward0>)\n",
      "89 tensor(85411.6562, grad_fn=<SumBackward0>)\n",
      "90 tensor(83623.5781, grad_fn=<SumBackward0>)\n",
      "91 tensor(81893.1016, grad_fn=<SumBackward0>)\n",
      "92 tensor(80216.7812, grad_fn=<SumBackward0>)\n",
      "93 tensor(78595.2578, grad_fn=<SumBackward0>)\n",
      "94 tensor(77027.4219, grad_fn=<SumBackward0>)\n",
      "95 tensor(75508.0859, grad_fn=<SumBackward0>)\n",
      "96 tensor(74037.6406, grad_fn=<SumBackward0>)\n",
      "97 tensor(72613., grad_fn=<SumBackward0>)\n",
      "98 tensor(71234.3750, grad_fn=<SumBackward0>)\n",
      "99 tensor(69899., grad_fn=<SumBackward0>)\n",
      "100 tensor(68604.7891, grad_fn=<SumBackward0>)\n",
      "101 tensor(67348.0391, grad_fn=<SumBackward0>)\n",
      "102 tensor(66129.8203, grad_fn=<SumBackward0>)\n",
      "103 tensor(64946.6562, grad_fn=<SumBackward0>)\n",
      "104 tensor(63796.5234, grad_fn=<SumBackward0>)\n",
      "105 tensor(62679.5391, grad_fn=<SumBackward0>)\n",
      "106 tensor(61594.9062, grad_fn=<SumBackward0>)\n",
      "107 tensor(60540.0117, grad_fn=<SumBackward0>)\n",
      "108 tensor(59513.4297, grad_fn=<SumBackward0>)\n",
      "109 tensor(58512.6250, grad_fn=<SumBackward0>)\n",
      "110 tensor(57538.3047, grad_fn=<SumBackward0>)\n",
      "111 tensor(56590.5664, grad_fn=<SumBackward0>)\n",
      "112 tensor(55668.6992, grad_fn=<SumBackward0>)\n",
      "113 tensor(54771.3398, grad_fn=<SumBackward0>)\n",
      "114 tensor(53897.1680, grad_fn=<SumBackward0>)\n",
      "115 tensor(53047.0898, grad_fn=<SumBackward0>)\n",
      "116 tensor(52218.5234, grad_fn=<SumBackward0>)\n",
      "117 tensor(51410.2188, grad_fn=<SumBackward0>)\n",
      "118 tensor(50622.7539, grad_fn=<SumBackward0>)\n",
      "119 tensor(49855.2812, grad_fn=<SumBackward0>)\n",
      "120 tensor(49107.2734, grad_fn=<SumBackward0>)\n",
      "121 tensor(48378.5078, grad_fn=<SumBackward0>)\n",
      "122 tensor(47667.1875, grad_fn=<SumBackward0>)\n",
      "123 tensor(46973.0781, grad_fn=<SumBackward0>)\n",
      "124 tensor(46295.5039, grad_fn=<SumBackward0>)\n",
      "125 tensor(45635.1797, grad_fn=<SumBackward0>)\n",
      "126 tensor(44989.4922, grad_fn=<SumBackward0>)\n",
      "127 tensor(44358.9688, grad_fn=<SumBackward0>)\n",
      "128 tensor(43742.8828, grad_fn=<SumBackward0>)\n",
      "129 tensor(43141.1133, grad_fn=<SumBackward0>)\n",
      "130 tensor(42554.4648, grad_fn=<SumBackward0>)\n",
      "131 tensor(41980.8594, grad_fn=<SumBackward0>)\n",
      "132 tensor(41419.4805, grad_fn=<SumBackward0>)\n",
      "133 tensor(40870.9062, grad_fn=<SumBackward0>)\n",
      "134 tensor(40334.5117, grad_fn=<SumBackward0>)\n",
      "135 tensor(39809.8984, grad_fn=<SumBackward0>)\n",
      "136 tensor(39296.6367, grad_fn=<SumBackward0>)\n",
      "137 tensor(38794.6133, grad_fn=<SumBackward0>)\n",
      "138 tensor(38303.3086, grad_fn=<SumBackward0>)\n",
      "139 tensor(37822.4922, grad_fn=<SumBackward0>)\n",
      "140 tensor(37351.5586, grad_fn=<SumBackward0>)\n",
      "141 tensor(36890.8281, grad_fn=<SumBackward0>)\n",
      "142 tensor(36441.0117, grad_fn=<SumBackward0>)\n",
      "143 tensor(36000.0469, grad_fn=<SumBackward0>)\n",
      "144 tensor(35567.7344, grad_fn=<SumBackward0>)\n",
      "145 tensor(35144.5742, grad_fn=<SumBackward0>)\n",
      "146 tensor(34729.6289, grad_fn=<SumBackward0>)\n",
      "147 tensor(34322.7852, grad_fn=<SumBackward0>)\n",
      "148 tensor(33923.7070, grad_fn=<SumBackward0>)\n",
      "149 tensor(33532.4141, grad_fn=<SumBackward0>)\n",
      "150 tensor(33148.3945, grad_fn=<SumBackward0>)\n",
      "151 tensor(32772.4141, grad_fn=<SumBackward0>)\n",
      "152 tensor(32403.5762, grad_fn=<SumBackward0>)\n",
      "153 tensor(32042.0098, grad_fn=<SumBackward0>)\n",
      "154 tensor(31687.3027, grad_fn=<SumBackward0>)\n",
      "155 tensor(31339.0781, grad_fn=<SumBackward0>)\n",
      "156 tensor(30997.3594, grad_fn=<SumBackward0>)\n",
      "157 tensor(30661.8086, grad_fn=<SumBackward0>)\n",
      "158 tensor(30332.4727, grad_fn=<SumBackward0>)\n",
      "159 tensor(30009.4180, grad_fn=<SumBackward0>)\n",
      "160 tensor(29691.7676, grad_fn=<SumBackward0>)\n",
      "161 tensor(29380.2344, grad_fn=<SumBackward0>)\n",
      "162 tensor(29074.3320, grad_fn=<SumBackward0>)\n",
      "163 tensor(28773.3828, grad_fn=<SumBackward0>)\n",
      "164 tensor(28477.5781, grad_fn=<SumBackward0>)\n",
      "165 tensor(28187.0293, grad_fn=<SumBackward0>)\n",
      "166 tensor(27901.3711, grad_fn=<SumBackward0>)\n",
      "167 tensor(27620.4922, grad_fn=<SumBackward0>)\n",
      "168 tensor(27344.4238, grad_fn=<SumBackward0>)\n",
      "169 tensor(27073.1074, grad_fn=<SumBackward0>)\n",
      "170 tensor(26806.1504, grad_fn=<SumBackward0>)\n",
      "171 tensor(26543.6172, grad_fn=<SumBackward0>)\n",
      "172 tensor(26285.6387, grad_fn=<SumBackward0>)\n",
      "173 tensor(26032.0703, grad_fn=<SumBackward0>)\n",
      "174 tensor(25782.4883, grad_fn=<SumBackward0>)\n",
      "175 tensor(25536.6875, grad_fn=<SumBackward0>)\n",
      "176 tensor(25294.9297, grad_fn=<SumBackward0>)\n",
      "177 tensor(25057.0645, grad_fn=<SumBackward0>)\n",
      "178 tensor(24822.6758, grad_fn=<SumBackward0>)\n",
      "179 tensor(24591.8496, grad_fn=<SumBackward0>)\n",
      "180 tensor(24364.4512, grad_fn=<SumBackward0>)\n",
      "181 tensor(24140.9766, grad_fn=<SumBackward0>)\n",
      "182 tensor(23921.0742, grad_fn=<SumBackward0>)\n",
      "183 tensor(23704.5488, grad_fn=<SumBackward0>)\n",
      "184 tensor(23491.2793, grad_fn=<SumBackward0>)\n",
      "185 tensor(23281.2559, grad_fn=<SumBackward0>)\n",
      "186 tensor(23074.2656, grad_fn=<SumBackward0>)\n",
      "187 tensor(22870.4648, grad_fn=<SumBackward0>)\n",
      "188 tensor(22669.5879, grad_fn=<SumBackward0>)\n",
      "189 tensor(22471.8984, grad_fn=<SumBackward0>)\n",
      "190 tensor(22276.8965, grad_fn=<SumBackward0>)\n",
      "191 tensor(22084.7422, grad_fn=<SumBackward0>)\n",
      "192 tensor(21894.4336, grad_fn=<SumBackward0>)\n",
      "193 tensor(21707.0371, grad_fn=<SumBackward0>)\n",
      "194 tensor(21522.3184, grad_fn=<SumBackward0>)\n",
      "195 tensor(21340.4805, grad_fn=<SumBackward0>)\n",
      "196 tensor(21161.2051, grad_fn=<SumBackward0>)\n",
      "197 tensor(20984.2344, grad_fn=<SumBackward0>)\n",
      "198 tensor(20809.7129, grad_fn=<SumBackward0>)\n",
      "199 tensor(20637.7578, grad_fn=<SumBackward0>)\n",
      "200 tensor(20467.3867, grad_fn=<SumBackward0>)\n",
      "201 tensor(20299.2695, grad_fn=<SumBackward0>)\n",
      "202 tensor(20133.5664, grad_fn=<SumBackward0>)\n",
      "203 tensor(19970.0586, grad_fn=<SumBackward0>)\n",
      "204 tensor(19809.1484, grad_fn=<SumBackward0>)\n",
      "205 tensor(19650.5547, grad_fn=<SumBackward0>)\n",
      "206 tensor(19494.0332, grad_fn=<SumBackward0>)\n",
      "207 tensor(19339.7266, grad_fn=<SumBackward0>)\n",
      "208 tensor(19187.4316, grad_fn=<SumBackward0>)\n",
      "209 tensor(19037.2480, grad_fn=<SumBackward0>)\n",
      "210 tensor(18888.5938, grad_fn=<SumBackward0>)\n",
      "211 tensor(18741.8242, grad_fn=<SumBackward0>)\n",
      "212 tensor(18596.9355, grad_fn=<SumBackward0>)\n",
      "213 tensor(18454.1445, grad_fn=<SumBackward0>)\n",
      "214 tensor(18313.1348, grad_fn=<SumBackward0>)\n",
      "215 tensor(18173.9004, grad_fn=<SumBackward0>)\n",
      "216 tensor(18036.3652, grad_fn=<SumBackward0>)\n",
      "217 tensor(17900.7305, grad_fn=<SumBackward0>)\n",
      "218 tensor(17766.8027, grad_fn=<SumBackward0>)\n",
      "219 tensor(17634.6777, grad_fn=<SumBackward0>)\n",
      "220 tensor(17504.0918, grad_fn=<SumBackward0>)\n",
      "221 tensor(17375.1758, grad_fn=<SumBackward0>)\n",
      "222 tensor(17247.4492, grad_fn=<SumBackward0>)\n",
      "223 tensor(17121.3789, grad_fn=<SumBackward0>)\n",
      "224 tensor(16996.9141, grad_fn=<SumBackward0>)\n",
      "225 tensor(16873.9375, grad_fn=<SumBackward0>)\n",
      "226 tensor(16752.4609, grad_fn=<SumBackward0>)\n",
      "227 tensor(16632.3906, grad_fn=<SumBackward0>)\n",
      "228 tensor(16513.7949, grad_fn=<SumBackward0>)\n",
      "229 tensor(16393.0469, grad_fn=<SumBackward0>)\n",
      "230 tensor(16273.8213, grad_fn=<SumBackward0>)\n",
      "231 tensor(16156.0703, grad_fn=<SumBackward0>)\n",
      "232 tensor(16039.9697, grad_fn=<SumBackward0>)\n",
      "233 tensor(15925.2207, grad_fn=<SumBackward0>)\n",
      "234 tensor(15811.9873, grad_fn=<SumBackward0>)\n",
      "235 tensor(15700.1719, grad_fn=<SumBackward0>)\n",
      "236 tensor(15589.8008, grad_fn=<SumBackward0>)\n",
      "237 tensor(15480.8789, grad_fn=<SumBackward0>)\n",
      "238 tensor(15373.2568, grad_fn=<SumBackward0>)\n",
      "239 tensor(15267.0332, grad_fn=<SumBackward0>)\n",
      "240 tensor(15162.1270, grad_fn=<SumBackward0>)\n",
      "241 tensor(15058.5029, grad_fn=<SumBackward0>)\n",
      "242 tensor(14956.2480, grad_fn=<SumBackward0>)\n",
      "243 tensor(14855.1396, grad_fn=<SumBackward0>)\n",
      "244 tensor(14755.2217, grad_fn=<SumBackward0>)\n",
      "245 tensor(14656.4766, grad_fn=<SumBackward0>)\n",
      "246 tensor(14558.9531, grad_fn=<SumBackward0>)\n",
      "247 tensor(14462.5283, grad_fn=<SumBackward0>)\n",
      "248 tensor(14367.2432, grad_fn=<SumBackward0>)\n",
      "249 tensor(14272.9580, grad_fn=<SumBackward0>)\n",
      "250 tensor(14179.7598, grad_fn=<SumBackward0>)\n",
      "251 tensor(14087.6738, grad_fn=<SumBackward0>)\n",
      "252 tensor(13996.6270, grad_fn=<SumBackward0>)\n",
      "253 tensor(13906.5898, grad_fn=<SumBackward0>)\n",
      "254 tensor(13817.5674, grad_fn=<SumBackward0>)\n",
      "255 tensor(13729.3135, grad_fn=<SumBackward0>)\n",
      "256 tensor(13642.0811, grad_fn=<SumBackward0>)\n",
      "257 tensor(13555.7578, grad_fn=<SumBackward0>)\n",
      "258 tensor(13470.3643, grad_fn=<SumBackward0>)\n",
      "259 tensor(13385.9316, grad_fn=<SumBackward0>)\n",
      "260 tensor(13302.4600, grad_fn=<SumBackward0>)\n",
      "261 tensor(13219.8604, grad_fn=<SumBackward0>)\n",
      "262 tensor(13138.1748, grad_fn=<SumBackward0>)\n",
      "263 tensor(13057.3340, grad_fn=<SumBackward0>)\n",
      "264 tensor(12977.4678, grad_fn=<SumBackward0>)\n",
      "265 tensor(12898.3096, grad_fn=<SumBackward0>)\n",
      "266 tensor(12819.9766, grad_fn=<SumBackward0>)\n",
      "267 tensor(12742.4697, grad_fn=<SumBackward0>)\n",
      "268 tensor(12665.7080, grad_fn=<SumBackward0>)\n",
      "269 tensor(12589.8564, grad_fn=<SumBackward0>)\n",
      "270 tensor(12514.7598, grad_fn=<SumBackward0>)\n",
      "271 tensor(12440.4238, grad_fn=<SumBackward0>)\n",
      "272 tensor(12366.9404, grad_fn=<SumBackward0>)\n",
      "273 tensor(12294.1836, grad_fn=<SumBackward0>)\n",
      "274 tensor(12222.1699, grad_fn=<SumBackward0>)\n",
      "275 tensor(12150.8701, grad_fn=<SumBackward0>)\n",
      "276 tensor(12080.2861, grad_fn=<SumBackward0>)\n",
      "277 tensor(12010.4482, grad_fn=<SumBackward0>)\n",
      "278 tensor(11941.2617, grad_fn=<SumBackward0>)\n",
      "279 tensor(11872.7539, grad_fn=<SumBackward0>)\n",
      "280 tensor(11804.9170, grad_fn=<SumBackward0>)\n",
      "281 tensor(11737.8066, grad_fn=<SumBackward0>)\n",
      "282 tensor(11671.2754, grad_fn=<SumBackward0>)\n",
      "283 tensor(11605.4385, grad_fn=<SumBackward0>)\n",
      "284 tensor(11540.2090, grad_fn=<SumBackward0>)\n",
      "285 tensor(11475.6074, grad_fn=<SumBackward0>)\n",
      "286 tensor(11411.6318, grad_fn=<SumBackward0>)\n",
      "287 tensor(11348.2705, grad_fn=<SumBackward0>)\n",
      "288 tensor(11285.5664, grad_fn=<SumBackward0>)\n",
      "289 tensor(11223.3799, grad_fn=<SumBackward0>)\n",
      "290 tensor(11161.7920, grad_fn=<SumBackward0>)\n",
      "291 tensor(11100.7559, grad_fn=<SumBackward0>)\n",
      "292 tensor(11040.3105, grad_fn=<SumBackward0>)\n",
      "293 tensor(10980.4746, grad_fn=<SumBackward0>)\n",
      "294 tensor(10921.2285, grad_fn=<SumBackward0>)\n",
      "295 tensor(10862.5254, grad_fn=<SumBackward0>)\n",
      "296 tensor(10804.3262, grad_fn=<SumBackward0>)\n",
      "297 tensor(10746.6602, grad_fn=<SumBackward0>)\n",
      "298 tensor(10689.5498, grad_fn=<SumBackward0>)\n",
      "299 tensor(10632.9707, grad_fn=<SumBackward0>)\n",
      "300 tensor(10576.8682, grad_fn=<SumBackward0>)\n",
      "301 tensor(10521.2490, grad_fn=<SumBackward0>)\n",
      "302 tensor(10466.1064, grad_fn=<SumBackward0>)\n",
      "303 tensor(10411.4355, grad_fn=<SumBackward0>)\n",
      "304 tensor(10357.3096, grad_fn=<SumBackward0>)\n",
      "305 tensor(10303.6777, grad_fn=<SumBackward0>)\n",
      "306 tensor(10250.3721, grad_fn=<SumBackward0>)\n",
      "307 tensor(10197.5293, grad_fn=<SumBackward0>)\n",
      "308 tensor(10145.1221, grad_fn=<SumBackward0>)\n",
      "309 tensor(10093.1719, grad_fn=<SumBackward0>)\n",
      "310 tensor(10041.6982, grad_fn=<SumBackward0>)\n",
      "311 tensor(9990.6748, grad_fn=<SumBackward0>)\n",
      "312 tensor(9940.0771, grad_fn=<SumBackward0>)\n",
      "313 tensor(9889.9434, grad_fn=<SumBackward0>)\n",
      "314 tensor(9840.1924, grad_fn=<SumBackward0>)\n",
      "315 tensor(9790.8672, grad_fn=<SumBackward0>)\n",
      "316 tensor(9742.0244, grad_fn=<SumBackward0>)\n",
      "317 tensor(9693.5625, grad_fn=<SumBackward0>)\n",
      "318 tensor(9645.5342, grad_fn=<SumBackward0>)\n",
      "319 tensor(9597.9131, grad_fn=<SumBackward0>)\n",
      "320 tensor(9550.6885, grad_fn=<SumBackward0>)\n",
      "321 tensor(9503.8701, grad_fn=<SumBackward0>)\n",
      "322 tensor(9457.4160, grad_fn=<SumBackward0>)\n",
      "323 tensor(9411.4238, grad_fn=<SumBackward0>)\n",
      "324 tensor(9365.7676, grad_fn=<SumBackward0>)\n",
      "325 tensor(9320.4824, grad_fn=<SumBackward0>)\n",
      "326 tensor(9275.5586, grad_fn=<SumBackward0>)\n",
      "327 tensor(9231.0264, grad_fn=<SumBackward0>)\n",
      "328 tensor(9186.8525, grad_fn=<SumBackward0>)\n",
      "329 tensor(9143.0303, grad_fn=<SumBackward0>)\n",
      "330 tensor(9099.5527, grad_fn=<SumBackward0>)\n",
      "331 tensor(9056.4033, grad_fn=<SumBackward0>)\n",
      "332 tensor(9013.6455, grad_fn=<SumBackward0>)\n",
      "333 tensor(8971.2100, grad_fn=<SumBackward0>)\n",
      "334 tensor(8929.0908, grad_fn=<SumBackward0>)\n",
      "335 tensor(8887.3184, grad_fn=<SumBackward0>)\n",
      "336 tensor(8845.9004, grad_fn=<SumBackward0>)\n",
      "337 tensor(8804.8242, grad_fn=<SumBackward0>)\n",
      "338 tensor(8764.0205, grad_fn=<SumBackward0>)\n",
      "339 tensor(8723.5703, grad_fn=<SumBackward0>)\n",
      "340 tensor(8683.4014, grad_fn=<SumBackward0>)\n",
      "341 tensor(8643.5420, grad_fn=<SumBackward0>)\n",
      "342 tensor(8604.0430, grad_fn=<SumBackward0>)\n",
      "343 tensor(8564.9209, grad_fn=<SumBackward0>)\n",
      "344 tensor(8526.1045, grad_fn=<SumBackward0>)\n",
      "345 tensor(8487.5654, grad_fn=<SumBackward0>)\n",
      "346 tensor(8449.3232, grad_fn=<SumBackward0>)\n",
      "347 tensor(8411.3691, grad_fn=<SumBackward0>)\n",
      "348 tensor(8373.7090, grad_fn=<SumBackward0>)\n",
      "349 tensor(8336.3379, grad_fn=<SumBackward0>)\n",
      "350 tensor(8299.2520, grad_fn=<SumBackward0>)\n",
      "351 tensor(8262.4365, grad_fn=<SumBackward0>)\n",
      "352 tensor(8225.9131, grad_fn=<SumBackward0>)\n",
      "353 tensor(8189.7139, grad_fn=<SumBackward0>)\n",
      "354 tensor(8153.8003, grad_fn=<SumBackward0>)\n",
      "355 tensor(8118.1699, grad_fn=<SumBackward0>)\n",
      "356 tensor(8082.7065, grad_fn=<SumBackward0>)\n",
      "357 tensor(8047.5508, grad_fn=<SumBackward0>)\n",
      "358 tensor(8012.6431, grad_fn=<SumBackward0>)\n",
      "359 tensor(7978.0088, grad_fn=<SumBackward0>)\n",
      "360 tensor(7943.6191, grad_fn=<SumBackward0>)\n",
      "361 tensor(7909.4937, grad_fn=<SumBackward0>)\n",
      "362 tensor(7875.5913, grad_fn=<SumBackward0>)\n",
      "363 tensor(7841.9331, grad_fn=<SumBackward0>)\n",
      "364 tensor(7808.5249, grad_fn=<SumBackward0>)\n",
      "365 tensor(7775.3711, grad_fn=<SumBackward0>)\n",
      "366 tensor(7742.4414, grad_fn=<SumBackward0>)\n",
      "367 tensor(7709.7549, grad_fn=<SumBackward0>)\n",
      "368 tensor(7677.2969, grad_fn=<SumBackward0>)\n",
      "369 tensor(7645.0571, grad_fn=<SumBackward0>)\n",
      "370 tensor(7613.0522, grad_fn=<SumBackward0>)\n",
      "371 tensor(7581.3008, grad_fn=<SumBackward0>)\n",
      "372 tensor(7549.7417, grad_fn=<SumBackward0>)\n",
      "373 tensor(7518.4092, grad_fn=<SumBackward0>)\n",
      "374 tensor(7487.3101, grad_fn=<SumBackward0>)\n",
      "375 tensor(7456.4316, grad_fn=<SumBackward0>)\n",
      "376 tensor(7425.7563, grad_fn=<SumBackward0>)\n",
      "377 tensor(7395.3477, grad_fn=<SumBackward0>)\n",
      "378 tensor(7365.1230, grad_fn=<SumBackward0>)\n",
      "379 tensor(7335.1123, grad_fn=<SumBackward0>)\n",
      "380 tensor(7305.3267, grad_fn=<SumBackward0>)\n",
      "381 tensor(7275.7217, grad_fn=<SumBackward0>)\n",
      "382 tensor(7246.3301, grad_fn=<SumBackward0>)\n",
      "383 tensor(7217.1313, grad_fn=<SumBackward0>)\n",
      "384 tensor(7188.1299, grad_fn=<SumBackward0>)\n",
      "385 tensor(7159.3623, grad_fn=<SumBackward0>)\n",
      "386 tensor(7130.7256, grad_fn=<SumBackward0>)\n",
      "387 tensor(7102.2930, grad_fn=<SumBackward0>)\n",
      "388 tensor(7074.0562, grad_fn=<SumBackward0>)\n",
      "389 tensor(7045.9873, grad_fn=<SumBackward0>)\n",
      "390 tensor(7018.1348, grad_fn=<SumBackward0>)\n",
      "391 tensor(6990.4980, grad_fn=<SumBackward0>)\n",
      "392 tensor(6963.0596, grad_fn=<SumBackward0>)\n",
      "393 tensor(6935.7842, grad_fn=<SumBackward0>)\n",
      "394 tensor(6908.6953, grad_fn=<SumBackward0>)\n",
      "395 tensor(6881.7661, grad_fn=<SumBackward0>)\n",
      "396 tensor(6855.0142, grad_fn=<SumBackward0>)\n",
      "397 tensor(6828.4185, grad_fn=<SumBackward0>)\n",
      "398 tensor(6802.0273, grad_fn=<SumBackward0>)\n",
      "399 tensor(6775.8643, grad_fn=<SumBackward0>)\n",
      "400 tensor(6749.9917, grad_fn=<SumBackward0>)\n",
      "401 tensor(6724.2891, grad_fn=<SumBackward0>)\n",
      "402 tensor(6698.7563, grad_fn=<SumBackward0>)\n",
      "403 tensor(6673.3740, grad_fn=<SumBackward0>)\n",
      "404 tensor(6648.2139, grad_fn=<SumBackward0>)\n",
      "405 tensor(6623.1934, grad_fn=<SumBackward0>)\n",
      "406 tensor(6598.3262, grad_fn=<SumBackward0>)\n",
      "407 tensor(6573.6143, grad_fn=<SumBackward0>)\n",
      "408 tensor(6549.0420, grad_fn=<SumBackward0>)\n",
      "409 tensor(6524.6240, grad_fn=<SumBackward0>)\n",
      "410 tensor(6500.3931, grad_fn=<SumBackward0>)\n",
      "411 tensor(6476.3247, grad_fn=<SumBackward0>)\n",
      "412 tensor(6452.3965, grad_fn=<SumBackward0>)\n",
      "413 tensor(6428.5903, grad_fn=<SumBackward0>)\n",
      "414 tensor(6404.9531, grad_fn=<SumBackward0>)\n",
      "415 tensor(6381.4600, grad_fn=<SumBackward0>)\n",
      "416 tensor(6358.1074, grad_fn=<SumBackward0>)\n",
      "417 tensor(6334.9077, grad_fn=<SumBackward0>)\n",
      "418 tensor(6311.8623, grad_fn=<SumBackward0>)\n",
      "419 tensor(6288.9321, grad_fn=<SumBackward0>)\n",
      "420 tensor(6266.1504, grad_fn=<SumBackward0>)\n",
      "421 tensor(6243.5269, grad_fn=<SumBackward0>)\n",
      "422 tensor(6221.0552, grad_fn=<SumBackward0>)\n",
      "423 tensor(6198.7085, grad_fn=<SumBackward0>)\n",
      "424 tensor(6176.5093, grad_fn=<SumBackward0>)\n",
      "425 tensor(6154.4570, grad_fn=<SumBackward0>)\n",
      "426 tensor(6132.5381, grad_fn=<SumBackward0>)\n",
      "427 tensor(6110.7295, grad_fn=<SumBackward0>)\n",
      "428 tensor(6089.0586, grad_fn=<SumBackward0>)\n",
      "429 tensor(6067.5298, grad_fn=<SumBackward0>)\n",
      "430 tensor(6046.1196, grad_fn=<SumBackward0>)\n",
      "431 tensor(6024.8496, grad_fn=<SumBackward0>)\n",
      "432 tensor(6003.6943, grad_fn=<SumBackward0>)\n",
      "433 tensor(5982.6626, grad_fn=<SumBackward0>)\n",
      "434 tensor(5961.7441, grad_fn=<SumBackward0>)\n",
      "435 tensor(5940.9526, grad_fn=<SumBackward0>)\n",
      "436 tensor(5920.3047, grad_fn=<SumBackward0>)\n",
      "437 tensor(5899.7642, grad_fn=<SumBackward0>)\n",
      "438 tensor(5879.3311, grad_fn=<SumBackward0>)\n",
      "439 tensor(5859.0566, grad_fn=<SumBackward0>)\n",
      "440 tensor(5838.9033, grad_fn=<SumBackward0>)\n",
      "441 tensor(5818.8623, grad_fn=<SumBackward0>)\n",
      "442 tensor(5798.9424, grad_fn=<SumBackward0>)\n",
      "443 tensor(5779.1445, grad_fn=<SumBackward0>)\n",
      "444 tensor(5759.4790, grad_fn=<SumBackward0>)\n",
      "445 tensor(5739.9106, grad_fn=<SumBackward0>)\n",
      "446 tensor(5720.4414, grad_fn=<SumBackward0>)\n",
      "447 tensor(5701.0977, grad_fn=<SumBackward0>)\n",
      "448 tensor(5681.8750, grad_fn=<SumBackward0>)\n",
      "449 tensor(5662.7480, grad_fn=<SumBackward0>)\n",
      "450 tensor(5643.7261, grad_fn=<SumBackward0>)\n",
      "451 tensor(5624.8145, grad_fn=<SumBackward0>)\n",
      "452 tensor(5606.0107, grad_fn=<SumBackward0>)\n",
      "453 tensor(5587.3101, grad_fn=<SumBackward0>)\n",
      "454 tensor(5568.7119, grad_fn=<SumBackward0>)\n",
      "455 tensor(5550.2246, grad_fn=<SumBackward0>)\n",
      "456 tensor(5531.8184, grad_fn=<SumBackward0>)\n",
      "457 tensor(5513.5225, grad_fn=<SumBackward0>)\n",
      "458 tensor(5495.3301, grad_fn=<SumBackward0>)\n",
      "459 tensor(5477.2524, grad_fn=<SumBackward0>)\n",
      "460 tensor(5459.2520, grad_fn=<SumBackward0>)\n",
      "461 tensor(5441.3643, grad_fn=<SumBackward0>)\n",
      "462 tensor(5423.5728, grad_fn=<SumBackward0>)\n",
      "463 tensor(5405.8750, grad_fn=<SumBackward0>)\n",
      "464 tensor(5388.2856, grad_fn=<SumBackward0>)\n",
      "465 tensor(5370.7803, grad_fn=<SumBackward0>)\n",
      "466 tensor(5353.3750, grad_fn=<SumBackward0>)\n",
      "467 tensor(5336.0752, grad_fn=<SumBackward0>)\n",
      "468 tensor(5318.8491, grad_fn=<SumBackward0>)\n",
      "469 tensor(5301.7314, grad_fn=<SumBackward0>)\n",
      "470 tensor(5284.7124, grad_fn=<SumBackward0>)\n",
      "471 tensor(5267.7695, grad_fn=<SumBackward0>)\n",
      "472 tensor(5250.9263, grad_fn=<SumBackward0>)\n",
      "473 tensor(5234.1724, grad_fn=<SumBackward0>)\n",
      "474 tensor(5217.4995, grad_fn=<SumBackward0>)\n",
      "475 tensor(5200.9253, grad_fn=<SumBackward0>)\n",
      "476 tensor(5184.4297, grad_fn=<SumBackward0>)\n",
      "477 tensor(5168.0142, grad_fn=<SumBackward0>)\n",
      "478 tensor(5151.6846, grad_fn=<SumBackward0>)\n",
      "479 tensor(5135.4424, grad_fn=<SumBackward0>)\n",
      "480 tensor(5119.1621, grad_fn=<SumBackward0>)\n",
      "481 tensor(5102.9619, grad_fn=<SumBackward0>)\n",
      "482 tensor(5086.8452, grad_fn=<SumBackward0>)\n",
      "483 tensor(5070.8237, grad_fn=<SumBackward0>)\n",
      "484 tensor(5054.8809, grad_fn=<SumBackward0>)\n",
      "485 tensor(5039.0254, grad_fn=<SumBackward0>)\n",
      "486 tensor(5023.2476, grad_fn=<SumBackward0>)\n",
      "487 tensor(5007.5464, grad_fn=<SumBackward0>)\n",
      "488 tensor(4991.9277, grad_fn=<SumBackward0>)\n",
      "489 tensor(4976.3979, grad_fn=<SumBackward0>)\n",
      "490 tensor(4960.9541, grad_fn=<SumBackward0>)\n",
      "491 tensor(4945.5835, grad_fn=<SumBackward0>)\n",
      "492 tensor(4930.3047, grad_fn=<SumBackward0>)\n",
      "493 tensor(4915.0830, grad_fn=<SumBackward0>)\n",
      "494 tensor(4899.9502, grad_fn=<SumBackward0>)\n",
      "495 tensor(4884.8896, grad_fn=<SumBackward0>)\n",
      "496 tensor(4869.9087, grad_fn=<SumBackward0>)\n",
      "497 tensor(4854.9893, grad_fn=<SumBackward0>)\n",
      "498 tensor(4839.9209, grad_fn=<SumBackward0>)\n",
      "499 tensor(4824.9375, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:20:20.743139Z",
     "start_time": "2025-07-31T07:20:20.299639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MyReLU Version\n",
    "import torch\n",
    "from my_relu import MyReLU\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# init the weights\n",
    "w1 = torch.randn(D_in, D_h, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D_h, D_out, device=device, requires_grad=True)\n",
    "\n",
    "# training\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward inference, with MyReLU\n",
    "    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # use autograd to do the back-propagation\n",
    "    # gonna call MyReLU.backward()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights, prevent torch do autograd for this part\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # zero grads\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ],
   "id": "14ba4cbac76cf782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(51349004., grad_fn=<SumBackward0>)\n",
      "1 tensor(1.3182e+08, grad_fn=<SumBackward0>)\n",
      "2 tensor(4.9419e+08, grad_fn=<SumBackward0>)\n",
      "3 tensor(6.9187e+08, grad_fn=<SumBackward0>)\n",
      "4 tensor(12659630., grad_fn=<SumBackward0>)\n",
      "5 tensor(7746988.5000, grad_fn=<SumBackward0>)\n",
      "6 tensor(5248556., grad_fn=<SumBackward0>)\n",
      "7 tensor(3786125., grad_fn=<SumBackward0>)\n",
      "8 tensor(2857854.2500, grad_fn=<SumBackward0>)\n",
      "9 tensor(2236181.2500, grad_fn=<SumBackward0>)\n",
      "10 tensor(1802806.7500, grad_fn=<SumBackward0>)\n",
      "11 tensor(1491155.3750, grad_fn=<SumBackward0>)\n",
      "12 tensor(1261058.5000, grad_fn=<SumBackward0>)\n",
      "13 tensor(1086990.3750, grad_fn=<SumBackward0>)\n",
      "14 tensor(952239.3750, grad_fn=<SumBackward0>)\n",
      "15 tensor(845501.5000, grad_fn=<SumBackward0>)\n",
      "16 tensor(759418.2500, grad_fn=<SumBackward0>)\n",
      "17 tensor(688506.1250, grad_fn=<SumBackward0>)\n",
      "18 tensor(628955.1250, grad_fn=<SumBackward0>)\n",
      "19 tensor(578168.5000, grad_fn=<SumBackward0>)\n",
      "20 tensor(534349.1250, grad_fn=<SumBackward0>)\n",
      "21 tensor(496058.5312, grad_fn=<SumBackward0>)\n",
      "22 tensor(462321.7500, grad_fn=<SumBackward0>)\n",
      "23 tensor(432250.7500, grad_fn=<SumBackward0>)\n",
      "24 tensor(405183.2188, grad_fn=<SumBackward0>)\n",
      "25 tensor(380698.3438, grad_fn=<SumBackward0>)\n",
      "26 tensor(358414.8750, grad_fn=<SumBackward0>)\n",
      "27 tensor(338066.4062, grad_fn=<SumBackward0>)\n",
      "28 tensor(319406.0938, grad_fn=<SumBackward0>)\n",
      "29 tensor(302259.8438, grad_fn=<SumBackward0>)\n",
      "30 tensor(286434.3750, grad_fn=<SumBackward0>)\n",
      "31 tensor(271777.1875, grad_fn=<SumBackward0>)\n",
      "32 tensor(258179.8750, grad_fn=<SumBackward0>)\n",
      "33 tensor(245526.2031, grad_fn=<SumBackward0>)\n",
      "34 tensor(233723.8125, grad_fn=<SumBackward0>)\n",
      "35 tensor(222704.6250, grad_fn=<SumBackward0>)\n",
      "36 tensor(212395.6875, grad_fn=<SumBackward0>)\n",
      "37 tensor(202744.9688, grad_fn=<SumBackward0>)\n",
      "38 tensor(193687.0625, grad_fn=<SumBackward0>)\n",
      "39 tensor(185186.5781, grad_fn=<SumBackward0>)\n",
      "40 tensor(177209.7188, grad_fn=<SumBackward0>)\n",
      "41 tensor(169721.8125, grad_fn=<SumBackward0>)\n",
      "42 tensor(162664.7188, grad_fn=<SumBackward0>)\n",
      "43 tensor(156025.4844, grad_fn=<SumBackward0>)\n",
      "44 tensor(149767.5938, grad_fn=<SumBackward0>)\n",
      "45 tensor(143848.8438, grad_fn=<SumBackward0>)\n",
      "46 tensor(138250.7656, grad_fn=<SumBackward0>)\n",
      "47 tensor(132951.6562, grad_fn=<SumBackward0>)\n",
      "48 tensor(127929.1328, grad_fn=<SumBackward0>)\n",
      "49 tensor(123168.6719, grad_fn=<SumBackward0>)\n",
      "50 tensor(118656.0234, grad_fn=<SumBackward0>)\n",
      "51 tensor(114363.4141, grad_fn=<SumBackward0>)\n",
      "52 tensor(110278.8125, grad_fn=<SumBackward0>)\n",
      "53 tensor(106388.1016, grad_fn=<SumBackward0>)\n",
      "54 tensor(102689.8438, grad_fn=<SumBackward0>)\n",
      "55 tensor(99166.3281, grad_fn=<SumBackward0>)\n",
      "56 tensor(95804.7109, grad_fn=<SumBackward0>)\n",
      "57 tensor(92601.1016, grad_fn=<SumBackward0>)\n",
      "58 tensor(89542.2188, grad_fn=<SumBackward0>)\n",
      "59 tensor(86624.0391, grad_fn=<SumBackward0>)\n",
      "60 tensor(83834.1250, grad_fn=<SumBackward0>)\n",
      "61 tensor(81164.7344, grad_fn=<SumBackward0>)\n",
      "62 tensor(78609.1484, grad_fn=<SumBackward0>)\n",
      "63 tensor(76160.7109, grad_fn=<SumBackward0>)\n",
      "64 tensor(73815.0234, grad_fn=<SumBackward0>)\n",
      "65 tensor(71565.9531, grad_fn=<SumBackward0>)\n",
      "66 tensor(69407.0625, grad_fn=<SumBackward0>)\n",
      "67 tensor(67334.9531, grad_fn=<SumBackward0>)\n",
      "68 tensor(65347.1836, grad_fn=<SumBackward0>)\n",
      "69 tensor(63436.6094, grad_fn=<SumBackward0>)\n",
      "70 tensor(61602.9336, grad_fn=<SumBackward0>)\n",
      "71 tensor(59839.3984, grad_fn=<SumBackward0>)\n",
      "72 tensor(58142.7148, grad_fn=<SumBackward0>)\n",
      "73 tensor(56509.7109, grad_fn=<SumBackward0>)\n",
      "74 tensor(54938.4258, grad_fn=<SumBackward0>)\n",
      "75 tensor(53424.3750, grad_fn=<SumBackward0>)\n",
      "76 tensor(51965.8281, grad_fn=<SumBackward0>)\n",
      "77 tensor(50564.3711, grad_fn=<SumBackward0>)\n",
      "78 tensor(49214.9805, grad_fn=<SumBackward0>)\n",
      "79 tensor(47913.0469, grad_fn=<SumBackward0>)\n",
      "80 tensor(46658.0469, grad_fn=<SumBackward0>)\n",
      "81 tensor(45448.9180, grad_fn=<SumBackward0>)\n",
      "82 tensor(44282.8047, grad_fn=<SumBackward0>)\n",
      "83 tensor(43157.2578, grad_fn=<SumBackward0>)\n",
      "84 tensor(42069.5117, grad_fn=<SumBackward0>)\n",
      "85 tensor(41018.4531, grad_fn=<SumBackward0>)\n",
      "86 tensor(40002.2812, grad_fn=<SumBackward0>)\n",
      "87 tensor(39018.5820, grad_fn=<SumBackward0>)\n",
      "88 tensor(38067., grad_fn=<SumBackward0>)\n",
      "89 tensor(37146.1562, grad_fn=<SumBackward0>)\n",
      "90 tensor(36254.7617, grad_fn=<SumBackward0>)\n",
      "91 tensor(35391.5586, grad_fn=<SumBackward0>)\n",
      "92 tensor(34555.5859, grad_fn=<SumBackward0>)\n",
      "93 tensor(33746.1016, grad_fn=<SumBackward0>)\n",
      "94 tensor(32961.6445, grad_fn=<SumBackward0>)\n",
      "95 tensor(32201.4492, grad_fn=<SumBackward0>)\n",
      "96 tensor(31464.7422, grad_fn=<SumBackward0>)\n",
      "97 tensor(30750.1055, grad_fn=<SumBackward0>)\n",
      "98 tensor(30056.9141, grad_fn=<SumBackward0>)\n",
      "99 tensor(29385.5996, grad_fn=<SumBackward0>)\n",
      "100 tensor(28736.2832, grad_fn=<SumBackward0>)\n",
      "101 tensor(28106.3184, grad_fn=<SumBackward0>)\n",
      "102 tensor(27494.5723, grad_fn=<SumBackward0>)\n",
      "103 tensor(26900.1719, grad_fn=<SumBackward0>)\n",
      "104 tensor(26322.7812, grad_fn=<SumBackward0>)\n",
      "105 tensor(25762.5605, grad_fn=<SumBackward0>)\n",
      "106 tensor(25218.4551, grad_fn=<SumBackward0>)\n",
      "107 tensor(24689.6445, grad_fn=<SumBackward0>)\n",
      "108 tensor(24175.6992, grad_fn=<SumBackward0>)\n",
      "109 tensor(23675.5879, grad_fn=<SumBackward0>)\n",
      "110 tensor(23188.1172, grad_fn=<SumBackward0>)\n",
      "111 tensor(22713.8438, grad_fn=<SumBackward0>)\n",
      "112 tensor(22252.5195, grad_fn=<SumBackward0>)\n",
      "113 tensor(21803.4785, grad_fn=<SumBackward0>)\n",
      "114 tensor(21366.2500, grad_fn=<SumBackward0>)\n",
      "115 tensor(20940.9219, grad_fn=<SumBackward0>)\n",
      "116 tensor(20526.2383, grad_fn=<SumBackward0>)\n",
      "117 tensor(20122.2734, grad_fn=<SumBackward0>)\n",
      "118 tensor(19728.9180, grad_fn=<SumBackward0>)\n",
      "119 tensor(19345.8359, grad_fn=<SumBackward0>)\n",
      "120 tensor(18972.8594, grad_fn=<SumBackward0>)\n",
      "121 tensor(18609.5977, grad_fn=<SumBackward0>)\n",
      "122 tensor(18255.2891, grad_fn=<SumBackward0>)\n",
      "123 tensor(17909.5469, grad_fn=<SumBackward0>)\n",
      "124 tensor(17572.4492, grad_fn=<SumBackward0>)\n",
      "125 tensor(17243.6055, grad_fn=<SumBackward0>)\n",
      "126 tensor(16922.5938, grad_fn=<SumBackward0>)\n",
      "127 tensor(16609.4688, grad_fn=<SumBackward0>)\n",
      "128 tensor(16303.7539, grad_fn=<SumBackward0>)\n",
      "129 tensor(16005.2949, grad_fn=<SumBackward0>)\n",
      "130 tensor(15713.9375, grad_fn=<SumBackward0>)\n",
      "131 tensor(15429.5664, grad_fn=<SumBackward0>)\n",
      "132 tensor(15151.7031, grad_fn=<SumBackward0>)\n",
      "133 tensor(14880.2754, grad_fn=<SumBackward0>)\n",
      "134 tensor(14615.4033, grad_fn=<SumBackward0>)\n",
      "135 tensor(14356.4102, grad_fn=<SumBackward0>)\n",
      "136 tensor(14103.5264, grad_fn=<SumBackward0>)\n",
      "137 tensor(13856.0850, grad_fn=<SumBackward0>)\n",
      "138 tensor(13614.4141, grad_fn=<SumBackward0>)\n",
      "139 tensor(13377.7188, grad_fn=<SumBackward0>)\n",
      "140 tensor(13146.3623, grad_fn=<SumBackward0>)\n",
      "141 tensor(12920.0986, grad_fn=<SumBackward0>)\n",
      "142 tensor(12698.8223, grad_fn=<SumBackward0>)\n",
      "143 tensor(12482.3945, grad_fn=<SumBackward0>)\n",
      "144 tensor(12270.7070, grad_fn=<SumBackward0>)\n",
      "145 tensor(12063.6162, grad_fn=<SumBackward0>)\n",
      "146 tensor(11860.9668, grad_fn=<SumBackward0>)\n",
      "147 tensor(11662.7949, grad_fn=<SumBackward0>)\n",
      "148 tensor(11468.9424, grad_fn=<SumBackward0>)\n",
      "149 tensor(11279.0986, grad_fn=<SumBackward0>)\n",
      "150 tensor(11093.1631, grad_fn=<SumBackward0>)\n",
      "151 tensor(10911.1172, grad_fn=<SumBackward0>)\n",
      "152 tensor(10732.8896, grad_fn=<SumBackward0>)\n",
      "153 tensor(10558.4033, grad_fn=<SumBackward0>)\n",
      "154 tensor(10387.5771, grad_fn=<SumBackward0>)\n",
      "155 tensor(10220.2744, grad_fn=<SumBackward0>)\n",
      "156 tensor(10056.3057, grad_fn=<SumBackward0>)\n",
      "157 tensor(9895.6514, grad_fn=<SumBackward0>)\n",
      "158 tensor(9738.2881, grad_fn=<SumBackward0>)\n",
      "159 tensor(9584.0732, grad_fn=<SumBackward0>)\n",
      "160 tensor(9433.0059, grad_fn=<SumBackward0>)\n",
      "161 tensor(9285.0332, grad_fn=<SumBackward0>)\n",
      "162 tensor(9139.7861, grad_fn=<SumBackward0>)\n",
      "163 tensor(8997.4570, grad_fn=<SumBackward0>)\n",
      "164 tensor(8858.0176, grad_fn=<SumBackward0>)\n",
      "165 tensor(8721.2480, grad_fn=<SumBackward0>)\n",
      "166 tensor(8587.1055, grad_fn=<SumBackward0>)\n",
      "167 tensor(8455.5850, grad_fn=<SumBackward0>)\n",
      "168 tensor(8326.3516, grad_fn=<SumBackward0>)\n",
      "169 tensor(8199.5791, grad_fn=<SumBackward0>)\n",
      "170 tensor(8075.1904, grad_fn=<SumBackward0>)\n",
      "171 tensor(7953.1958, grad_fn=<SumBackward0>)\n",
      "172 tensor(7833.5054, grad_fn=<SumBackward0>)\n",
      "173 tensor(7716.0552, grad_fn=<SumBackward0>)\n",
      "174 tensor(7600.9561, grad_fn=<SumBackward0>)\n",
      "175 tensor(7488.0752, grad_fn=<SumBackward0>)\n",
      "176 tensor(7377.3018, grad_fn=<SumBackward0>)\n",
      "177 tensor(7268.6899, grad_fn=<SumBackward0>)\n",
      "178 tensor(7162.1030, grad_fn=<SumBackward0>)\n",
      "179 tensor(7057.5864, grad_fn=<SumBackward0>)\n",
      "180 tensor(6955.0493, grad_fn=<SumBackward0>)\n",
      "181 tensor(6854.3862, grad_fn=<SumBackward0>)\n",
      "182 tensor(6755.5352, grad_fn=<SumBackward0>)\n",
      "183 tensor(6658.4551, grad_fn=<SumBackward0>)\n",
      "184 tensor(6563.1304, grad_fn=<SumBackward0>)\n",
      "185 tensor(6469.5049, grad_fn=<SumBackward0>)\n",
      "186 tensor(6377.5537, grad_fn=<SumBackward0>)\n",
      "187 tensor(6287.2939, grad_fn=<SumBackward0>)\n",
      "188 tensor(6198.5723, grad_fn=<SumBackward0>)\n",
      "189 tensor(6111.4683, grad_fn=<SumBackward0>)\n",
      "190 tensor(6025.8052, grad_fn=<SumBackward0>)\n",
      "191 tensor(5941.8076, grad_fn=<SumBackward0>)\n",
      "192 tensor(5859.6279, grad_fn=<SumBackward0>)\n",
      "193 tensor(5778.8555, grad_fn=<SumBackward0>)\n",
      "194 tensor(5699.4429, grad_fn=<SumBackward0>)\n",
      "195 tensor(5621.4258, grad_fn=<SumBackward0>)\n",
      "196 tensor(5544.7271, grad_fn=<SumBackward0>)\n",
      "197 tensor(5469.3325, grad_fn=<SumBackward0>)\n",
      "198 tensor(5395.1851, grad_fn=<SumBackward0>)\n",
      "199 tensor(5322.3833, grad_fn=<SumBackward0>)\n",
      "200 tensor(5250.9067, grad_fn=<SumBackward0>)\n",
      "201 tensor(5180.6094, grad_fn=<SumBackward0>)\n",
      "202 tensor(5111.5015, grad_fn=<SumBackward0>)\n",
      "203 tensor(5043.5225, grad_fn=<SumBackward0>)\n",
      "204 tensor(4976.5688, grad_fn=<SumBackward0>)\n",
      "205 tensor(4910.7314, grad_fn=<SumBackward0>)\n",
      "206 tensor(4845.9775, grad_fn=<SumBackward0>)\n",
      "207 tensor(4782.2905, grad_fn=<SumBackward0>)\n",
      "208 tensor(4719.6348, grad_fn=<SumBackward0>)\n",
      "209 tensor(4657.9941, grad_fn=<SumBackward0>)\n",
      "210 tensor(4597.3545, grad_fn=<SumBackward0>)\n",
      "211 tensor(4537.7358, grad_fn=<SumBackward0>)\n",
      "212 tensor(4479.2114, grad_fn=<SumBackward0>)\n",
      "213 tensor(4421.6333, grad_fn=<SumBackward0>)\n",
      "214 tensor(4364.9971, grad_fn=<SumBackward0>)\n",
      "215 tensor(4309.3184, grad_fn=<SumBackward0>)\n",
      "216 tensor(4254.5063, grad_fn=<SumBackward0>)\n",
      "217 tensor(4200.5430, grad_fn=<SumBackward0>)\n",
      "218 tensor(4147.4409, grad_fn=<SumBackward0>)\n",
      "219 tensor(4095.1782, grad_fn=<SumBackward0>)\n",
      "220 tensor(4043.7085, grad_fn=<SumBackward0>)\n",
      "221 tensor(3993.0125, grad_fn=<SumBackward0>)\n",
      "222 tensor(3943.1064, grad_fn=<SumBackward0>)\n",
      "223 tensor(3893.9937, grad_fn=<SumBackward0>)\n",
      "224 tensor(3845.6116, grad_fn=<SumBackward0>)\n",
      "225 tensor(3797.9746, grad_fn=<SumBackward0>)\n",
      "226 tensor(3751.0571, grad_fn=<SumBackward0>)\n",
      "227 tensor(3704.8413, grad_fn=<SumBackward0>)\n",
      "228 tensor(3659.3491, grad_fn=<SumBackward0>)\n",
      "229 tensor(3614.5698, grad_fn=<SumBackward0>)\n",
      "230 tensor(3570.4868, grad_fn=<SumBackward0>)\n",
      "231 tensor(3527.0713, grad_fn=<SumBackward0>)\n",
      "232 tensor(3484.3174, grad_fn=<SumBackward0>)\n",
      "233 tensor(3442.1929, grad_fn=<SumBackward0>)\n",
      "234 tensor(3400.7256, grad_fn=<SumBackward0>)\n",
      "235 tensor(3359.8469, grad_fn=<SumBackward0>)\n",
      "236 tensor(3319.5764, grad_fn=<SumBackward0>)\n",
      "237 tensor(3279.8811, grad_fn=<SumBackward0>)\n",
      "238 tensor(3240.7734, grad_fn=<SumBackward0>)\n",
      "239 tensor(3202.2461, grad_fn=<SumBackward0>)\n",
      "240 tensor(3164.2935, grad_fn=<SumBackward0>)\n",
      "241 tensor(3126.8950, grad_fn=<SumBackward0>)\n",
      "242 tensor(3090.0327, grad_fn=<SumBackward0>)\n",
      "243 tensor(3053.7090, grad_fn=<SumBackward0>)\n",
      "244 tensor(3017.9014, grad_fn=<SumBackward0>)\n",
      "245 tensor(2982.5962, grad_fn=<SumBackward0>)\n",
      "246 tensor(2947.7974, grad_fn=<SumBackward0>)\n",
      "247 tensor(2913.5042, grad_fn=<SumBackward0>)\n",
      "248 tensor(2879.7224, grad_fn=<SumBackward0>)\n",
      "249 tensor(2846.3989, grad_fn=<SumBackward0>)\n",
      "250 tensor(2813.5525, grad_fn=<SumBackward0>)\n",
      "251 tensor(2781.1965, grad_fn=<SumBackward0>)\n",
      "252 tensor(2749.4138, grad_fn=<SumBackward0>)\n",
      "253 tensor(2718.0786, grad_fn=<SumBackward0>)\n",
      "254 tensor(2687.1985, grad_fn=<SumBackward0>)\n",
      "255 tensor(2656.7549, grad_fn=<SumBackward0>)\n",
      "256 tensor(2626.7783, grad_fn=<SumBackward0>)\n",
      "257 tensor(2597.1992, grad_fn=<SumBackward0>)\n",
      "258 tensor(2568.0681, grad_fn=<SumBackward0>)\n",
      "259 tensor(2539.3521, grad_fn=<SumBackward0>)\n",
      "260 tensor(2511.0332, grad_fn=<SumBackward0>)\n",
      "261 tensor(2483.1042, grad_fn=<SumBackward0>)\n",
      "262 tensor(2455.5637, grad_fn=<SumBackward0>)\n",
      "263 tensor(2428.4014, grad_fn=<SumBackward0>)\n",
      "264 tensor(2401.6211, grad_fn=<SumBackward0>)\n",
      "265 tensor(2375.1958, grad_fn=<SumBackward0>)\n",
      "266 tensor(2349.1270, grad_fn=<SumBackward0>)\n",
      "267 tensor(2323.4185, grad_fn=<SumBackward0>)\n",
      "268 tensor(2298.0603, grad_fn=<SumBackward0>)\n",
      "269 tensor(2273.0427, grad_fn=<SumBackward0>)\n",
      "270 tensor(2248.3635, grad_fn=<SumBackward0>)\n",
      "271 tensor(2224.0176, grad_fn=<SumBackward0>)\n",
      "272 tensor(2199.9871, grad_fn=<SumBackward0>)\n",
      "273 tensor(2176.2886, grad_fn=<SumBackward0>)\n",
      "274 tensor(2152.9026, grad_fn=<SumBackward0>)\n",
      "275 tensor(2129.8262, grad_fn=<SumBackward0>)\n",
      "276 tensor(2107.0508, grad_fn=<SumBackward0>)\n",
      "277 tensor(2084.5735, grad_fn=<SumBackward0>)\n",
      "278 tensor(2062.3911, grad_fn=<SumBackward0>)\n",
      "279 tensor(2040.4938, grad_fn=<SumBackward0>)\n",
      "280 tensor(2018.8838, grad_fn=<SumBackward0>)\n",
      "281 tensor(1997.5605, grad_fn=<SumBackward0>)\n",
      "282 tensor(1976.5192, grad_fn=<SumBackward0>)\n",
      "283 tensor(1955.7455, grad_fn=<SumBackward0>)\n",
      "284 tensor(1935.2386, grad_fn=<SumBackward0>)\n",
      "285 tensor(1914.9960, grad_fn=<SumBackward0>)\n",
      "286 tensor(1895.0157, grad_fn=<SumBackward0>)\n",
      "287 tensor(1875.2941, grad_fn=<SumBackward0>)\n",
      "288 tensor(1855.8171, grad_fn=<SumBackward0>)\n",
      "289 tensor(1836.5964, grad_fn=<SumBackward0>)\n",
      "290 tensor(1817.6226, grad_fn=<SumBackward0>)\n",
      "291 tensor(1798.8955, grad_fn=<SumBackward0>)\n",
      "292 tensor(1780.4021, grad_fn=<SumBackward0>)\n",
      "293 tensor(1762.1409, grad_fn=<SumBackward0>)\n",
      "294 tensor(1744.1165, grad_fn=<SumBackward0>)\n",
      "295 tensor(1726.3190, grad_fn=<SumBackward0>)\n",
      "296 tensor(1708.7463, grad_fn=<SumBackward0>)\n",
      "297 tensor(1691.3997, grad_fn=<SumBackward0>)\n",
      "298 tensor(1674.2700, grad_fn=<SumBackward0>)\n",
      "299 tensor(1657.3662, grad_fn=<SumBackward0>)\n",
      "300 tensor(1640.6750, grad_fn=<SumBackward0>)\n",
      "301 tensor(1624.1909, grad_fn=<SumBackward0>)\n",
      "302 tensor(1607.9152, grad_fn=<SumBackward0>)\n",
      "303 tensor(1591.8387, grad_fn=<SumBackward0>)\n",
      "304 tensor(1575.9598, grad_fn=<SumBackward0>)\n",
      "305 tensor(1560.2837, grad_fn=<SumBackward0>)\n",
      "306 tensor(1544.8027, grad_fn=<SumBackward0>)\n",
      "307 tensor(1529.5017, grad_fn=<SumBackward0>)\n",
      "308 tensor(1514.3840, grad_fn=<SumBackward0>)\n",
      "309 tensor(1499.4592, grad_fn=<SumBackward0>)\n",
      "310 tensor(1484.7130, grad_fn=<SumBackward0>)\n",
      "311 tensor(1470.1479, grad_fn=<SumBackward0>)\n",
      "312 tensor(1455.7588, grad_fn=<SumBackward0>)\n",
      "313 tensor(1441.5457, grad_fn=<SumBackward0>)\n",
      "314 tensor(1427.5037, grad_fn=<SumBackward0>)\n",
      "315 tensor(1413.6274, grad_fn=<SumBackward0>)\n",
      "316 tensor(1399.9208, grad_fn=<SumBackward0>)\n",
      "317 tensor(1386.3788, grad_fn=<SumBackward0>)\n",
      "318 tensor(1373.0029, grad_fn=<SumBackward0>)\n",
      "319 tensor(1359.7820, grad_fn=<SumBackward0>)\n",
      "320 tensor(1346.7178, grad_fn=<SumBackward0>)\n",
      "321 tensor(1333.8093, grad_fn=<SumBackward0>)\n",
      "322 tensor(1321.0604, grad_fn=<SumBackward0>)\n",
      "323 tensor(1308.4583, grad_fn=<SumBackward0>)\n",
      "324 tensor(1296.0042, grad_fn=<SumBackward0>)\n",
      "325 tensor(1283.6959, grad_fn=<SumBackward0>)\n",
      "326 tensor(1271.5309, grad_fn=<SumBackward0>)\n",
      "327 tensor(1259.5078, grad_fn=<SumBackward0>)\n",
      "328 tensor(1247.6273, grad_fn=<SumBackward0>)\n",
      "329 tensor(1235.8895, grad_fn=<SumBackward0>)\n",
      "330 tensor(1224.2893, grad_fn=<SumBackward0>)\n",
      "331 tensor(1212.8596, grad_fn=<SumBackward0>)\n",
      "332 tensor(1201.5896, grad_fn=<SumBackward0>)\n",
      "333 tensor(1190.4478, grad_fn=<SumBackward0>)\n",
      "334 tensor(1179.4377, grad_fn=<SumBackward0>)\n",
      "335 tensor(1168.5497, grad_fn=<SumBackward0>)\n",
      "336 tensor(1157.7905, grad_fn=<SumBackward0>)\n",
      "337 tensor(1147.1528, grad_fn=<SumBackward0>)\n",
      "338 tensor(1136.6372, grad_fn=<SumBackward0>)\n",
      "339 tensor(1126.2395, grad_fn=<SumBackward0>)\n",
      "340 tensor(1115.9628, grad_fn=<SumBackward0>)\n",
      "341 tensor(1105.8021, grad_fn=<SumBackward0>)\n",
      "342 tensor(1095.7615, grad_fn=<SumBackward0>)\n",
      "343 tensor(1085.8337, grad_fn=<SumBackward0>)\n",
      "344 tensor(1076.0146, grad_fn=<SumBackward0>)\n",
      "345 tensor(1066.3043, grad_fn=<SumBackward0>)\n",
      "346 tensor(1056.7021, grad_fn=<SumBackward0>)\n",
      "347 tensor(1047.2098, grad_fn=<SumBackward0>)\n",
      "348 tensor(1037.8257, grad_fn=<SumBackward0>)\n",
      "349 tensor(1028.5763, grad_fn=<SumBackward0>)\n",
      "350 tensor(1019.4366, grad_fn=<SumBackward0>)\n",
      "351 tensor(1010.4002, grad_fn=<SumBackward0>)\n",
      "352 tensor(1001.4648, grad_fn=<SumBackward0>)\n",
      "353 tensor(992.6340, grad_fn=<SumBackward0>)\n",
      "354 tensor(983.8972, grad_fn=<SumBackward0>)\n",
      "355 tensor(975.2583, grad_fn=<SumBackward0>)\n",
      "356 tensor(966.7128, grad_fn=<SumBackward0>)\n",
      "357 tensor(958.2598, grad_fn=<SumBackward0>)\n",
      "358 tensor(949.9109, grad_fn=<SumBackward0>)\n",
      "359 tensor(941.6794, grad_fn=<SumBackward0>)\n",
      "360 tensor(933.5384, grad_fn=<SumBackward0>)\n",
      "361 tensor(925.4846, grad_fn=<SumBackward0>)\n",
      "362 tensor(917.5208, grad_fn=<SumBackward0>)\n",
      "363 tensor(909.6425, grad_fn=<SumBackward0>)\n",
      "364 tensor(901.8494, grad_fn=<SumBackward0>)\n",
      "365 tensor(894.1442, grad_fn=<SumBackward0>)\n",
      "366 tensor(886.5208, grad_fn=<SumBackward0>)\n",
      "367 tensor(878.9785, grad_fn=<SumBackward0>)\n",
      "368 tensor(871.5213, grad_fn=<SumBackward0>)\n",
      "369 tensor(864.1410, grad_fn=<SumBackward0>)\n",
      "370 tensor(856.8415, grad_fn=<SumBackward0>)\n",
      "371 tensor(849.6211, grad_fn=<SumBackward0>)\n",
      "372 tensor(842.4766, grad_fn=<SumBackward0>)\n",
      "373 tensor(835.4101, grad_fn=<SumBackward0>)\n",
      "374 tensor(828.4192, grad_fn=<SumBackward0>)\n",
      "375 tensor(821.5024, grad_fn=<SumBackward0>)\n",
      "376 tensor(814.6601, grad_fn=<SumBackward0>)\n",
      "377 tensor(807.8885, grad_fn=<SumBackward0>)\n",
      "378 tensor(801.1880, grad_fn=<SumBackward0>)\n",
      "379 tensor(794.5566, grad_fn=<SumBackward0>)\n",
      "380 tensor(787.9998, grad_fn=<SumBackward0>)\n",
      "381 tensor(781.5125, grad_fn=<SumBackward0>)\n",
      "382 tensor(775.0870, grad_fn=<SumBackward0>)\n",
      "383 tensor(768.7302, grad_fn=<SumBackward0>)\n",
      "384 tensor(762.4402, grad_fn=<SumBackward0>)\n",
      "385 tensor(756.2208, grad_fn=<SumBackward0>)\n",
      "386 tensor(750.0597, grad_fn=<SumBackward0>)\n",
      "387 tensor(743.9666, grad_fn=<SumBackward0>)\n",
      "388 tensor(737.9358, grad_fn=<SumBackward0>)\n",
      "389 tensor(731.9683, grad_fn=<SumBackward0>)\n",
      "390 tensor(726.0626, grad_fn=<SumBackward0>)\n",
      "391 tensor(720.2149, grad_fn=<SumBackward0>)\n",
      "392 tensor(714.4289, grad_fn=<SumBackward0>)\n",
      "393 tensor(708.7027, grad_fn=<SumBackward0>)\n",
      "394 tensor(703.0358, grad_fn=<SumBackward0>)\n",
      "395 tensor(697.4236, grad_fn=<SumBackward0>)\n",
      "396 tensor(691.8701, grad_fn=<SumBackward0>)\n",
      "397 tensor(686.3765, grad_fn=<SumBackward0>)\n",
      "398 tensor(680.9354, grad_fn=<SumBackward0>)\n",
      "399 tensor(675.5547, grad_fn=<SumBackward0>)\n",
      "400 tensor(670.2267, grad_fn=<SumBackward0>)\n",
      "401 tensor(664.9503, grad_fn=<SumBackward0>)\n",
      "402 tensor(659.7258, grad_fn=<SumBackward0>)\n",
      "403 tensor(654.5576, grad_fn=<SumBackward0>)\n",
      "404 tensor(649.4399, grad_fn=<SumBackward0>)\n",
      "405 tensor(644.3729, grad_fn=<SumBackward0>)\n",
      "406 tensor(639.3572, grad_fn=<SumBackward0>)\n",
      "407 tensor(634.3947, grad_fn=<SumBackward0>)\n",
      "408 tensor(629.4783, grad_fn=<SumBackward0>)\n",
      "409 tensor(624.6102, grad_fn=<SumBackward0>)\n",
      "410 tensor(619.7935, grad_fn=<SumBackward0>)\n",
      "411 tensor(615.0235, grad_fn=<SumBackward0>)\n",
      "412 tensor(610.3025, grad_fn=<SumBackward0>)\n",
      "413 tensor(605.6266, grad_fn=<SumBackward0>)\n",
      "414 tensor(600.9962, grad_fn=<SumBackward0>)\n",
      "415 tensor(596.4109, grad_fn=<SumBackward0>)\n",
      "416 tensor(591.8723, grad_fn=<SumBackward0>)\n",
      "417 tensor(587.3764, grad_fn=<SumBackward0>)\n",
      "418 tensor(582.9257, grad_fn=<SumBackward0>)\n",
      "419 tensor(578.5193, grad_fn=<SumBackward0>)\n",
      "420 tensor(574.1565, grad_fn=<SumBackward0>)\n",
      "421 tensor(569.8363, grad_fn=<SumBackward0>)\n",
      "422 tensor(565.5590, grad_fn=<SumBackward0>)\n",
      "423 tensor(561.3226, grad_fn=<SumBackward0>)\n",
      "424 tensor(557.1259, grad_fn=<SumBackward0>)\n",
      "425 tensor(552.9719, grad_fn=<SumBackward0>)\n",
      "426 tensor(548.8561, grad_fn=<SumBackward0>)\n",
      "427 tensor(544.7814, grad_fn=<SumBackward0>)\n",
      "428 tensor(540.7452, grad_fn=<SumBackward0>)\n",
      "429 tensor(536.7485, grad_fn=<SumBackward0>)\n",
      "430 tensor(532.7898, grad_fn=<SumBackward0>)\n",
      "431 tensor(528.8704, grad_fn=<SumBackward0>)\n",
      "432 tensor(524.9885, grad_fn=<SumBackward0>)\n",
      "433 tensor(521.1426, grad_fn=<SumBackward0>)\n",
      "434 tensor(517.3324, grad_fn=<SumBackward0>)\n",
      "435 tensor(513.5596, grad_fn=<SumBackward0>)\n",
      "436 tensor(509.8224, grad_fn=<SumBackward0>)\n",
      "437 tensor(506.1208, grad_fn=<SumBackward0>)\n",
      "438 tensor(502.4541, grad_fn=<SumBackward0>)\n",
      "439 tensor(498.8237, grad_fn=<SumBackward0>)\n",
      "440 tensor(495.2262, grad_fn=<SumBackward0>)\n",
      "441 tensor(491.6625, grad_fn=<SumBackward0>)\n",
      "442 tensor(488.1326, grad_fn=<SumBackward0>)\n",
      "443 tensor(484.6360, grad_fn=<SumBackward0>)\n",
      "444 tensor(481.1745, grad_fn=<SumBackward0>)\n",
      "445 tensor(477.7466, grad_fn=<SumBackward0>)\n",
      "446 tensor(474.3503, grad_fn=<SumBackward0>)\n",
      "447 tensor(470.9853, grad_fn=<SumBackward0>)\n",
      "448 tensor(467.6511, grad_fn=<SumBackward0>)\n",
      "449 tensor(464.3491, grad_fn=<SumBackward0>)\n",
      "450 tensor(461.0775, grad_fn=<SumBackward0>)\n",
      "451 tensor(457.8362, grad_fn=<SumBackward0>)\n",
      "452 tensor(454.6248, grad_fn=<SumBackward0>)\n",
      "453 tensor(451.4432, grad_fn=<SumBackward0>)\n",
      "454 tensor(448.2911, grad_fn=<SumBackward0>)\n",
      "455 tensor(445.1691, grad_fn=<SumBackward0>)\n",
      "456 tensor(442.0754, grad_fn=<SumBackward0>)\n",
      "457 tensor(439.0106, grad_fn=<SumBackward0>)\n",
      "458 tensor(435.9724, grad_fn=<SumBackward0>)\n",
      "459 tensor(432.9630, grad_fn=<SumBackward0>)\n",
      "460 tensor(429.9807, grad_fn=<SumBackward0>)\n",
      "461 tensor(427.0249, grad_fn=<SumBackward0>)\n",
      "462 tensor(424.0965, grad_fn=<SumBackward0>)\n",
      "463 tensor(421.1955, grad_fn=<SumBackward0>)\n",
      "464 tensor(418.3203, grad_fn=<SumBackward0>)\n",
      "465 tensor(415.4644, grad_fn=<SumBackward0>)\n",
      "466 tensor(412.6354, grad_fn=<SumBackward0>)\n",
      "467 tensor(409.8309, grad_fn=<SumBackward0>)\n",
      "468 tensor(407.0523, grad_fn=<SumBackward0>)\n",
      "469 tensor(404.2998, grad_fn=<SumBackward0>)\n",
      "470 tensor(401.5710, grad_fn=<SumBackward0>)\n",
      "471 tensor(398.8668, grad_fn=<SumBackward0>)\n",
      "472 tensor(396.1867, grad_fn=<SumBackward0>)\n",
      "473 tensor(393.5332, grad_fn=<SumBackward0>)\n",
      "474 tensor(390.9014, grad_fn=<SumBackward0>)\n",
      "475 tensor(388.2931, grad_fn=<SumBackward0>)\n",
      "476 tensor(385.7086, grad_fn=<SumBackward0>)\n",
      "477 tensor(383.1469, grad_fn=<SumBackward0>)\n",
      "478 tensor(380.6086, grad_fn=<SumBackward0>)\n",
      "479 tensor(378.0925, grad_fn=<SumBackward0>)\n",
      "480 tensor(375.5988, grad_fn=<SumBackward0>)\n",
      "481 tensor(373.1273, grad_fn=<SumBackward0>)\n",
      "482 tensor(370.6769, grad_fn=<SumBackward0>)\n",
      "483 tensor(368.2495, grad_fn=<SumBackward0>)\n",
      "484 tensor(365.8430, grad_fn=<SumBackward0>)\n",
      "485 tensor(363.4578, grad_fn=<SumBackward0>)\n",
      "486 tensor(361.0931, grad_fn=<SumBackward0>)\n",
      "487 tensor(358.7495, grad_fn=<SumBackward0>)\n",
      "488 tensor(356.4286, grad_fn=<SumBackward0>)\n",
      "489 tensor(354.1257, grad_fn=<SumBackward0>)\n",
      "490 tensor(351.8434, grad_fn=<SumBackward0>)\n",
      "491 tensor(349.5818, grad_fn=<SumBackward0>)\n",
      "492 tensor(347.3383, grad_fn=<SumBackward0>)\n",
      "493 tensor(345.1157, grad_fn=<SumBackward0>)\n",
      "494 tensor(342.9109, grad_fn=<SumBackward0>)\n",
      "495 tensor(340.7263, grad_fn=<SumBackward0>)\n",
      "496 tensor(338.5609, grad_fn=<SumBackward0>)\n",
      "497 tensor(336.4138, grad_fn=<SumBackward0>)\n",
      "498 tensor(334.2853, grad_fn=<SumBackward0>)\n",
      "499 tensor(332.1757, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:25:10.906073Z",
     "start_time": "2025-07-31T07:25:10.614441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NN Version\n",
    "import torch\n",
    "\n",
    "# cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, D_h, D_out = 100, 1000, 100, 10\n",
    "\n",
    "# generate training data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# define the model\n",
    "model = torch.nn.Sequential(            # 创建一个顺序模型容器\n",
    "    torch.nn.Linear(D_in, D_h),             # 添加第一层全连接层，输入维度1000，输出维度100\n",
    "    torch.nn.ReLU(),                                # 添加 ReLU 激活函数层\n",
    "    torch.nn.Linear(D_h, D_out)             # 添加第二层全连接层，输入维度100，输出维度10\n",
    ").to(device)\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')             # 定义均方误差损失函数，使用求和方式计算\n",
    "\n",
    "# training, use bigger learning rate\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # forward inference\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # back-propagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters(weights)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():                    #  遍历模型中的所有参数\n",
    "            param -= learning_rate * param.grad             # 使用梯度下降法更新参数：参数 = 参数 - 学习率 × 梯度"
   ],
   "id": "bcbe5b11000504a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1069.2894287109375\n",
      "1 991.26416015625\n",
      "2 924.0081787109375\n",
      "3 864.6050415039062\n",
      "4 811.5043334960938\n",
      "5 763.15380859375\n",
      "6 718.899658203125\n",
      "7 677.6898193359375\n",
      "8 639.1441650390625\n",
      "9 603.2233276367188\n",
      "10 569.4577026367188\n",
      "11 537.5819702148438\n",
      "12 507.3804626464844\n",
      "13 478.5048522949219\n",
      "14 451.02618408203125\n",
      "15 424.80841064453125\n",
      "16 399.8072509765625\n",
      "17 376.02191162109375\n",
      "18 353.44427490234375\n",
      "19 331.9925537109375\n",
      "20 311.6029968261719\n",
      "21 292.3212890625\n",
      "22 274.0980529785156\n",
      "23 256.8631591796875\n",
      "24 240.57989501953125\n",
      "25 225.20498657226562\n",
      "26 210.68820190429688\n",
      "27 197.04376220703125\n",
      "28 184.21603393554688\n",
      "29 172.1650848388672\n",
      "30 160.86558532714844\n",
      "31 150.26759338378906\n",
      "32 140.35894775390625\n",
      "33 131.09510803222656\n",
      "34 122.4367446899414\n",
      "35 114.34649658203125\n",
      "36 106.79692077636719\n",
      "37 99.75287628173828\n",
      "38 93.17315673828125\n",
      "39 87.02490234375\n",
      "40 81.30264282226562\n",
      "41 75.9598159790039\n",
      "42 70.98039245605469\n",
      "43 66.34660339355469\n",
      "44 62.03581237792969\n",
      "45 58.02110290527344\n",
      "46 54.28080749511719\n",
      "47 50.79076385498047\n",
      "48 47.53903579711914\n",
      "49 44.50959777832031\n",
      "50 41.6927490234375\n",
      "51 39.066802978515625\n",
      "52 36.619197845458984\n",
      "53 34.334503173828125\n",
      "54 32.20582962036133\n",
      "55 30.22406005859375\n",
      "56 28.372169494628906\n",
      "57 26.64295768737793\n",
      "58 25.03208351135254\n",
      "59 23.526206970214844\n",
      "60 22.12033462524414\n",
      "61 20.805526733398438\n",
      "62 19.577194213867188\n",
      "63 18.42902183532715\n",
      "64 17.35674476623535\n",
      "65 16.355501174926758\n",
      "66 15.41671371459961\n",
      "67 14.538286209106445\n",
      "68 13.714200019836426\n",
      "69 12.944717407226562\n",
      "70 12.22362995147705\n",
      "71 11.547082901000977\n",
      "72 10.910872459411621\n",
      "73 10.313329696655273\n",
      "74 9.753069877624512\n",
      "75 9.22658920288086\n",
      "76 8.731521606445312\n",
      "77 8.26618480682373\n",
      "78 7.828367710113525\n",
      "79 7.417280673980713\n",
      "80 7.030241966247559\n",
      "81 6.664866924285889\n",
      "82 6.319884300231934\n",
      "83 5.994815826416016\n",
      "84 5.687902927398682\n",
      "85 5.3986382484436035\n",
      "86 5.126219272613525\n",
      "87 4.868438720703125\n",
      "88 4.624826908111572\n",
      "89 4.394553184509277\n",
      "90 4.1768012046813965\n",
      "91 3.970902442932129\n",
      "92 3.7761764526367188\n",
      "93 3.5917625427246094\n",
      "94 3.41575288772583\n",
      "95 3.2492475509643555\n",
      "96 3.091521739959717\n",
      "97 2.942089796066284\n",
      "98 2.800586462020874\n",
      "99 2.666581630706787\n",
      "100 2.5394954681396484\n",
      "101 2.418764352798462\n",
      "102 2.3044369220733643\n",
      "103 2.1957955360412598\n",
      "104 2.0929319858551025\n",
      "105 1.9951573610305786\n",
      "106 1.9021488428115845\n",
      "107 1.8138576745986938\n",
      "108 1.73000168800354\n",
      "109 1.6505812406539917\n",
      "110 1.5750091075897217\n",
      "111 1.5031321048736572\n",
      "112 1.4348212480545044\n",
      "113 1.3699027299880981\n",
      "114 1.3081891536712646\n",
      "115 1.2495923042297363\n",
      "116 1.1937663555145264\n",
      "117 1.1405439376831055\n",
      "118 1.0899661779403687\n",
      "119 1.041792869567871\n",
      "120 0.995935320854187\n",
      "121 0.9522386193275452\n",
      "122 0.9105405211448669\n",
      "123 0.8707911968231201\n",
      "124 0.8329448103904724\n",
      "125 0.7967326641082764\n",
      "126 0.7622310519218445\n",
      "127 0.7293577790260315\n",
      "128 0.6979812979698181\n",
      "129 0.6679943799972534\n",
      "130 0.6394248008728027\n",
      "131 0.612139105796814\n",
      "132 0.5860974788665771\n",
      "133 0.561201810836792\n",
      "134 0.5374220609664917\n",
      "135 0.5147172808647156\n",
      "136 0.4930228590965271\n",
      "137 0.4723106324672699\n",
      "138 0.452536404132843\n",
      "139 0.43359577655792236\n",
      "140 0.41551682353019714\n",
      "141 0.39821797609329224\n",
      "142 0.38167864084243774\n",
      "143 0.36586686968803406\n",
      "144 0.35078126192092896\n",
      "145 0.3363508880138397\n",
      "146 0.3225657343864441\n",
      "147 0.30937230587005615\n",
      "148 0.29673343896865845\n",
      "149 0.2846478819847107\n",
      "150 0.27308768033981323\n",
      "151 0.2620229125022888\n",
      "152 0.25143229961395264\n",
      "153 0.24129632115364075\n",
      "154 0.23158226907253265\n",
      "155 0.22228720784187317\n",
      "156 0.21338172256946564\n",
      "157 0.20485247671604156\n",
      "158 0.19668349623680115\n",
      "159 0.1888635754585266\n",
      "160 0.18136414885520935\n",
      "161 0.17417731881141663\n",
      "162 0.16729383170604706\n",
      "163 0.1607065349817276\n",
      "164 0.15438678860664368\n",
      "165 0.1483316272497177\n",
      "166 0.14252762496471405\n",
      "167 0.13695886731147766\n",
      "168 0.13162118196487427\n",
      "169 0.12650619447231293\n",
      "170 0.12159737944602966\n",
      "171 0.11688654869794846\n",
      "172 0.11236746609210968\n",
      "173 0.10803316533565521\n",
      "174 0.10387712717056274\n",
      "175 0.09988636523485184\n",
      "176 0.0960601344704628\n",
      "177 0.09238491207361221\n",
      "178 0.08885844051837921\n",
      "179 0.08547545224428177\n",
      "180 0.08222320675849915\n",
      "181 0.07909763604402542\n",
      "182 0.07609869539737701\n",
      "183 0.07322198897600174\n",
      "184 0.07045261561870575\n",
      "185 0.06779681891202927\n",
      "186 0.06524615734815598\n",
      "187 0.06279563903808594\n",
      "188 0.060444075614213943\n",
      "189 0.058182720094919205\n",
      "190 0.05601046606898308\n",
      "191 0.053925417363643646\n",
      "192 0.05192011594772339\n",
      "193 0.04999217391014099\n",
      "194 0.04813876748085022\n",
      "195 0.04635980352759361\n",
      "196 0.04465004801750183\n",
      "197 0.04300413280725479\n",
      "198 0.041422776877880096\n",
      "199 0.039904359728097916\n",
      "200 0.03844446316361427\n",
      "201 0.037039123475551605\n",
      "202 0.035687800496816635\n",
      "203 0.034387268126010895\n",
      "204 0.03313664346933365\n",
      "205 0.03193473443388939\n",
      "206 0.030776677653193474\n",
      "207 0.0296636875718832\n",
      "208 0.028595387935638428\n",
      "209 0.02756541408598423\n",
      "210 0.02657417580485344\n",
      "211 0.025621352717280388\n",
      "212 0.02470230497419834\n",
      "213 0.023818939924240112\n",
      "214 0.022968582808971405\n",
      "215 0.022149691358208656\n",
      "216 0.02136208489537239\n",
      "217 0.020602988079190254\n",
      "218 0.019872352480888367\n",
      "219 0.019169814884662628\n",
      "220 0.018492620438337326\n",
      "221 0.01784038543701172\n",
      "222 0.017212888225913048\n",
      "223 0.016607262194156647\n",
      "224 0.01602410525083542\n",
      "225 0.015462823212146759\n",
      "226 0.014922860078513622\n",
      "227 0.014402123168110847\n",
      "228 0.013899792917072773\n",
      "229 0.013416233472526073\n",
      "230 0.01294991746544838\n",
      "231 0.012500328943133354\n",
      "232 0.012067056261003017\n",
      "233 0.011650631204247475\n",
      "234 0.011248186230659485\n",
      "235 0.010860479436814785\n",
      "236 0.01048672292381525\n",
      "237 0.01012648269534111\n",
      "238 0.009779100306332111\n",
      "239 0.009443790651857853\n",
      "240 0.009121098555624485\n",
      "241 0.008809727616608143\n",
      "242 0.008509468287229538\n",
      "243 0.008219784125685692\n",
      "244 0.007940319366753101\n",
      "245 0.0076711601577699184\n",
      "246 0.0074112191796302795\n",
      "247 0.007160408888012171\n",
      "248 0.006918575614690781\n",
      "249 0.00668515358120203\n",
      "250 0.006459972821176052\n",
      "251 0.006242586765438318\n",
      "252 0.006032807752490044\n",
      "253 0.005830374546349049\n",
      "254 0.005635090637952089\n",
      "255 0.005446628667414188\n",
      "256 0.005264722276479006\n",
      "257 0.00508921779692173\n",
      "258 0.004919691476970911\n",
      "259 0.004756055772304535\n",
      "260 0.004598152358084917\n",
      "261 0.004445541650056839\n",
      "262 0.004298262298107147\n",
      "263 0.004156146664172411\n",
      "264 0.004018891137093306\n",
      "265 0.0038864649832248688\n",
      "266 0.0037587452679872513\n",
      "267 0.0036353380419313908\n",
      "268 0.0035159573890268803\n",
      "269 0.003400611924007535\n",
      "270 0.0032892602030187845\n",
      "271 0.0031817283015698195\n",
      "272 0.003077856730669737\n",
      "273 0.0029774969443678856\n",
      "274 0.0028805327601730824\n",
      "275 0.002786915050819516\n",
      "276 0.0026964382268488407\n",
      "277 0.002609007991850376\n",
      "278 0.002524537965655327\n",
      "279 0.002442855853587389\n",
      "280 0.002364066196605563\n",
      "281 0.0022877580486238003\n",
      "282 0.002214014995843172\n",
      "283 0.002142759505659342\n",
      "284 0.00207400880753994\n",
      "285 0.002007350791245699\n",
      "286 0.0019429608946666121\n",
      "287 0.0018807640299201012\n",
      "288 0.0018205902306362987\n",
      "289 0.0017623922321945429\n",
      "290 0.0017061318503692746\n",
      "291 0.0016517259646207094\n",
      "292 0.001599122304469347\n",
      "293 0.001548298285342753\n",
      "294 0.0014992420328781009\n",
      "295 0.0014518493553623557\n",
      "296 0.0014058958040550351\n",
      "297 0.0013614409836009145\n",
      "298 0.0013184135314077139\n",
      "299 0.001276763970963657\n",
      "300 0.001236468437127769\n",
      "301 0.0011975147062912583\n",
      "302 0.001159821287728846\n",
      "303 0.0011233823606744409\n",
      "304 0.0010881496127694845\n",
      "305 0.0010540313087403774\n",
      "306 0.001021066913381219\n",
      "307 0.0009891401277855039\n",
      "308 0.0009582788334228098\n",
      "309 0.0009284426341764629\n",
      "310 0.0008995043463073671\n",
      "311 0.0008715150179341435\n",
      "312 0.0008444413542747498\n",
      "313 0.0008182345191016793\n",
      "314 0.0007928367704153061\n",
      "315 0.0007682761643081903\n",
      "316 0.0007444829680025578\n",
      "317 0.0007214623619802296\n",
      "318 0.0006991971167735755\n",
      "319 0.0006776549271307886\n",
      "320 0.0006568104727193713\n",
      "321 0.0006365809822455049\n",
      "322 0.0006170347332954407\n",
      "323 0.0005980796995572746\n",
      "324 0.0005797244375571609\n",
      "325 0.0005619305884465575\n",
      "326 0.0005447347648441792\n",
      "327 0.0005280812038108706\n",
      "328 0.000511951744556427\n",
      "329 0.0004963242099620402\n",
      "330 0.0004811767430510372\n",
      "331 0.0004665192391257733\n",
      "332 0.0004523552197497338\n",
      "333 0.00043860674486495554\n",
      "334 0.0004252883663866669\n",
      "335 0.0004123886756133288\n",
      "336 0.00039988794014789164\n",
      "337 0.00038777198642492294\n",
      "338 0.0003760471590794623\n",
      "339 0.00036468898179009557\n",
      "340 0.00035367210512049496\n",
      "341 0.00034300473635084927\n",
      "342 0.00033267459366470575\n",
      "343 0.00032266436028294265\n",
      "344 0.0003129557007923722\n",
      "345 0.00030353193869814277\n",
      "346 0.0002944202860817313\n",
      "347 0.00028560322243720293\n",
      "348 0.0002770467253867537\n",
      "349 0.00026874273316934705\n",
      "350 0.00026069197338074446\n",
      "351 0.00025289313634857535\n",
      "352 0.00024534674594178796\n",
      "353 0.00023802310170140117\n",
      "354 0.00023092376068234444\n",
      "355 0.00022404252376873046\n",
      "356 0.0002173754182877019\n",
      "357 0.00021090396330691874\n",
      "358 0.00020463269902393222\n",
      "359 0.00019855635764542967\n",
      "360 0.00019266628078185022\n",
      "361 0.0001869607367552817\n",
      "362 0.00018142536282539368\n",
      "363 0.0001760561572154984\n",
      "364 0.00017085076251532882\n",
      "365 0.0001657979446463287\n",
      "366 0.00016090415010694414\n",
      "367 0.000156156558659859\n",
      "368 0.00015155557775869966\n",
      "369 0.00014708326489198953\n",
      "370 0.00014275757712312043\n",
      "371 0.0001385633077006787\n",
      "372 0.00013449383550323546\n",
      "373 0.00013054026931058615\n",
      "374 0.00012671037984546274\n",
      "375 0.00012299465015530586\n",
      "376 0.00011939249816350639\n",
      "377 0.00011589840141823515\n",
      "378 0.00011250753595959395\n",
      "379 0.00010922711953753605\n",
      "380 0.00010603709961287677\n",
      "381 0.00010294561798218638\n",
      "382 9.994197898777202e-05\n",
      "383 9.702728857519105e-05\n",
      "384 9.420060086995363e-05\n",
      "385 9.145276999333873e-05\n",
      "386 8.880051609594375e-05\n",
      "387 8.622238237876445e-05\n",
      "388 8.371517469640821e-05\n",
      "389 8.128704939736053e-05\n",
      "390 7.892577559687197e-05\n",
      "391 7.663694850634784e-05\n",
      "392 7.442022615578026e-05\n",
      "393 7.22702025086619e-05\n",
      "394 7.018020551186055e-05\n",
      "395 6.8154840846546e-05\n",
      "396 6.618248880840838e-05\n",
      "397 6.427319021895528e-05\n",
      "398 6.241852679522708e-05\n",
      "399 6.061849489924498e-05\n",
      "400 5.887183942832053e-05\n",
      "401 5.717705425922759e-05\n",
      "402 5.553277878789231e-05\n",
      "403 5.393789615482092e-05\n",
      "404 5.239138408796862e-05\n",
      "405 5.0885177188320085e-05\n",
      "406 4.9428064812673256e-05\n",
      "407 4.8008907469920814e-05\n",
      "408 4.6632390876766294e-05\n",
      "409 4.529737634584308e-05\n",
      "410 4.4001033529639244e-05\n",
      "411 4.274195453035645e-05\n",
      "412 4.15216782130301e-05\n",
      "413 4.0336017264053226e-05\n",
      "414 3.9184164052130654e-05\n",
      "415 3.806568565778434e-05\n",
      "416 3.6982852179789916e-05\n",
      "417 3.5926816053688526e-05\n",
      "418 3.4903179766843095e-05\n",
      "419 3.391033169464208e-05\n",
      "420 3.294488851679489e-05\n",
      "421 3.200865467078984e-05\n",
      "422 3.110008765361272e-05\n",
      "423 3.02194275718648e-05\n",
      "424 2.936096461780835e-05\n",
      "425 2.8525697416625917e-05\n",
      "426 2.7717789635062218e-05\n",
      "427 2.6932717446470633e-05\n",
      "428 2.6169600459979847e-05\n",
      "429 2.5427503715036437e-05\n",
      "430 2.4708819182706065e-05\n",
      "431 2.4012140784179792e-05\n",
      "432 2.3333645003731363e-05\n",
      "433 2.267512354592327e-05\n",
      "434 2.203520489274524e-05\n",
      "435 2.141275763278827e-05\n",
      "436 2.0811539798160084e-05\n",
      "437 2.02264309336897e-05\n",
      "438 1.96547043742612e-05\n",
      "439 1.9103154045296833e-05\n",
      "440 1.8564120182418264e-05\n",
      "441 1.804308340069838e-05\n",
      "442 1.753539800120052e-05\n",
      "443 1.7043834304786287e-05\n",
      "444 1.6565167243243195e-05\n",
      "445 1.6099509593914263e-05\n",
      "446 1.564882586535532e-05\n",
      "447 1.5211353456834331e-05\n",
      "448 1.4785416169615928e-05\n",
      "449 1.4371953511727042e-05\n",
      "450 1.397025516780559e-05\n",
      "451 1.3577408935816493e-05\n",
      "452 1.3198869964980986e-05\n",
      "453 1.2830018022214063e-05\n",
      "454 1.2472577509470284e-05\n",
      "455 1.212396273331251e-05\n",
      "456 1.1785775313910563e-05\n",
      "457 1.1456159882072825e-05\n",
      "458 1.113676444219891e-05\n",
      "459 1.0826721336343326e-05\n",
      "460 1.052547486324329e-05\n",
      "461 1.023221375362482e-05\n",
      "462 9.947734724846669e-06\n",
      "463 9.67041796684498e-06\n",
      "464 9.401614079251885e-06\n",
      "465 9.13940129976254e-06\n",
      "466 8.886475370672997e-06\n",
      "467 8.63929744809866e-06\n",
      "468 8.40012671687873e-06\n",
      "469 8.167190571839456e-06\n",
      "470 7.94092284195358e-06\n",
      "471 7.720097528363112e-06\n",
      "472 7.506423116865335e-06\n",
      "473 7.29946987121366e-06\n",
      "474 7.097073194017867e-06\n",
      "475 6.900622338434914e-06\n",
      "476 6.709909030178096e-06\n",
      "477 6.524716809508391e-06\n",
      "478 6.344168923533289e-06\n",
      "479 6.168620075186482e-06\n",
      "480 5.998057531542145e-06\n",
      "481 5.832808710692916e-06\n",
      "482 5.6721578403085005e-06\n",
      "483 5.515819339052541e-06\n",
      "484 5.363068339647725e-06\n",
      "485 5.2155073717585765e-06\n",
      "486 5.072050953458529e-06\n",
      "487 4.931675448460737e-06\n",
      "488 4.7969638217182364e-06\n",
      "489 4.665365395339904e-06\n",
      "490 4.536813776212512e-06\n",
      "491 4.411467216414167e-06\n",
      "492 4.2903629946522415e-06\n",
      "493 4.172884473518934e-06\n",
      "494 4.057932983414503e-06\n",
      "495 3.945854587072972e-06\n",
      "496 3.838152224489022e-06\n",
      "497 3.7334300486691063e-06\n",
      "498 3.6302883472671965e-06\n",
      "499 3.531618631313904e-06\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
