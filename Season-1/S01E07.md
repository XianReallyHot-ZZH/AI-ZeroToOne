# GPU & CUDA
> https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
## GPU Architecture
![CPU & GPU Arch Diagram](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/gpu-devotes-more-transistors-to-data-processing.png)

## CUDA Core Concepts
### Host & Device
 - Host: CPU
 - Device: GPU
    - SM(Streaming Multiprocessor): CUDA Core + Shared Memory + Register
        - CUDA Core: real arithmetic unit, General Purpose Calculation
        - Tensor Core: Matrix Muplication & Deep Learning Operations
        - RT Core: Ray Tracing Calculation
### Kernels
 - Kernel: function that runs on the device(GPU)
```c++
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

// Host code
int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}
```
### Thread Hierarchy
![Grid & Block](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/grid-of-thread-blocks.png)
 - use block to define the number of threads
```c++
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```
 - use multiple blocks to define the number of threads
```c++
 // Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;
    if (i < N && j < N)
        C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```
### Thread Scheduling
 - Warp: 32 threads, the smallest unit of execution by hardware
 - Warps are scheduled to the SMs(CUDA Cores / Tensor Cores / RT Cores)
 - One CUDA Core can execute one thread at a time(1 or 2 arithmetic operations per cycle)
 - Example: RTX 4060, 24 SMs, 3072 CUDA Cores(can handle 96 warps at a time)
    - 128 CUDA Cores per SM
    - 4 Tensor Cores per SM
    - 2 RT Core per SM
### Memory Hierarchy
![Memory Hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/memory-hierarchy.png)
### Data Transfer
 - Memory Copy
    - cudaMemcpy(dst, src, size, direction)
 - Access Directly
    - cudaHostAlloc(&h_data, size, cudaHostAllocMapped)
    - cudaHostGetDevicePointer(&d_data, h_data, 0)
 - Unified Memory
    - cudaMallocManaged(&data, size)
