{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coding: Machine Translation with Transformer\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 1m high-quality pairs of data\n",
    " - Model: Seq2seq with Transformer (Attention is all you need)\n",
    " - GPU: 1660TI"
   ],
   "id": "7430319f164ba198"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 可以先测试网络连接\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\")\n",
    "    print(\"网络连接正常\")\n",
    "except:\n",
    "    print(\"网络连接可能存在问题\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\", cache_dir=r\"D:\\Developer\\LLM\\FuggingFace-cache-model\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    import re  # 添加这一行\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "# 选出最多500万条数据\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ],
   "id": "5849dc8dec6a8f6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 继承Dataset构建英中文翻译数据集\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        # 对英文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        en_tokens = self.tokenizer_en(en_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 对中文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        zh_tokens = self.tokenizer_zh(zh_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 返回数据结构\n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),     # 去除张量的多余维度，输出以为数字数组，文本对应的tokenizer后的数组\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,         # 原始文本\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "# 构建训练和验证数据集\n",
    "def create_dataloaders(dataset, batch_size=128, num_workers=0, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "\n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                train_size=train_size,\n",
    "                                                random_state=42)\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets， 得到tokenizer后的训练数据集和验证数据集\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create DataLoaders 创建了pytorch数据加载器，封装了数据集，并定义了batch_size，shuffle，num_workers，方便训练时设置数据加载相关的超参数\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset, batch_size=64)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ],
   "id": "382950eb10c7c1e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the Transformer model for Machine Translation\n",
    "# Following \"Attention is All You Need\" architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 # 模型维度必须能被注意力头数整除，模型维度就相当于是每个token在embedding编码后的维度\n",
    "\n",
    "        self.d_model = d_model  # 模型维度 (int)\n",
    "        self.num_heads = num_heads  # 注意力头数 (int)\n",
    "        self.d_k = d_model // num_heads # 每个注意力头维度 (int)\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 查询权重矩阵 (d_model × d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # 键权重矩阵 (d_model × d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # 值权重矩阵 (d_model × d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 输出权重矩阵 (d_model × d_model)\n",
    "\n",
    "        # dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # 实现缩放点积注意力机制。\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate attention scores 计算注意力分值\n",
    "            Q: 查询张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            K: 键张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            K.transpose(-2, -1): 转置后的键张量，shape: (batch_size, num_heads, d_k, seq_len)\n",
    "            torch.matmul(Q, K.transpose(-2, -1)): 矩阵乘法，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "            scores: 注意力分数，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # (batch_size, num_heads, seq_len, seq_len)，相当于计算查询和键之间的相似度，当查询和键是同一个矩阵的时候，就是计算两个向量的自我相似度。\n",
    "\n",
    "        \"\"\"\n",
    "        Apply mask if provided\n",
    "            mask: 遮蔽张量，shape: (batch_size, 1, seq_len, seq_len) 或 (batch_size, num_heads, seq_len, seq_len)\n",
    "            scores: 遮蔽后的注意力分数，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # 注意力权重，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        \"\"\"\n",
    "        Apply attention to values\n",
    "            V: 值张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            output: 注意力输出，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights # 返回注意力输出output:(batch_size, num_heads, seq_len, d_k) 和注意力权重attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    query: 查询输入，shape: (batch_size, seq_len, d_model)\n",
    "    key: 键输入，shape: (batch_size, seq_len, d_model)\n",
    "    value: 值输入，shape: (batch_size, seq_len, d_model)\n",
    "    batch_size: 批次大小 (int)\n",
    "    \"\"\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Linear transformations and reshape\n",
    "            self.W_q(query): 线性变换，shape: (batch_size, seq_len, d_model)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k): 重塑，shape: (batch_size, seq_len, num_heads, d_k)\n",
    "            Q, K, V: 转置后，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Apply attention\n",
    "            attention_output: 注意力输出，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            attention_weights: 注意力权重，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        \"\"\"\n",
    "        Concatenate heads\n",
    "            attention_output.transpose(1, 2): 转置，shape: (batch_size, seq_len, num_heads, d_k)\n",
    "            .contiguous().view(batch_size, -1, self.d_model): 连续化并重塑，shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear transformation\n",
    "        output = self.W_o(attention_output)\n",
    "\n",
    "        return output   # output: 最终输出，shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "# 定义位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    max_len: 序列最大长度 (int，默认 100)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # pe: 位置编码矩阵，shape: (max_len, d_model), 初始化为零矩阵，用于存储位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        '''\n",
    "        torch.arange(0, max_len, dtype=torch.float): 生成 0 到 max_len-1 的序列，shape: (max_len,)\n",
    "        .unsqueeze(1): 增加一个维度，shape: (max_len, 1)\n",
    "        position: 位置索引矩阵，每一行代表一个位置索引\n",
    "        '''\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape: (max_len, 1)\n",
    "        '''\n",
    "        torch.arange(0, d_model, 2): 生成 0, 2, 4, ..., d_model-2 的序列（偶数索引）\n",
    "        .float(): 转换为浮点数\n",
    "        (-math.log(10000.0) / d_model): 缩放因子\n",
    "        div_term: 频率分母项，shape: (d_model/2,)\n",
    "        '''\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        '''\n",
    "        position * div_term: 广播乘法，shape: (max_len, d_model/2)\n",
    "        torch.sin(position * div_term): 正弦编码，shape: (max_len, d_model/2)\n",
    "        pe[:, 0::2]: 选择偶数列（0, 2, 4, ...），shape: (max_len, d_model/2)\n",
    "        torch.cos(position * div_term): 余弦编码，shape: (max_len, d_model/2)\n",
    "        pe[:, 1::2]: 选择奇数列（1, 3, 5, ...），shape: (max_len, d_model/2)\n",
    "        '''\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        '''\n",
    "        .unsqueeze(0): 增加批次维度，shape: (1, max_len, d_model)\n",
    "        .transpose(0, 1): 交换第0和第1维度，shape: (max_len, 1, d_model)\n",
    "        '''\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        # 将 pe 注册为缓冲区，不会被当作模型参数更新，但在 GPU 上会跟随模型移动\n",
    "        self.register_buffer('pe', pe) # shape: (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 输入张量，shape: (seq_len, batch_size, d_model)\n",
    "        self.pe[:x.size(0), :]: 选择与输入序列长度相匹配的位置编码，shape: (seq_len, 1, d_model)\n",
    "        x + self.pe[:x.size(0), :]: 广播加法，将位置编码添加到输入中，shape: (seq_len, batch_size, d_model)\n",
    "        返回添加了位置编码的张量\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size(0), :]   # shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "# Transformer 编码器层\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    num_heads: 注意力头数 (int)\n",
    "    d_ff: 前馈网络隐藏层维度 (int)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)   # 多头自注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        # 前馈神经网络模块\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),   # 第一层线性变换: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),  # ReLU 激活函数: shape 保持不变\n",
    "            nn.Dropout(dropout), # Dropout: shape 保持不变\n",
    "            nn.Linear(d_ff, d_model)    # 第二层线性变换: (batch_size, seq_len, d_ff) → (batch_size, seq_len d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 第一个层归一化，shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # 第二个层归一化，shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    x: 输入张量，shape: (batch_size, seq_len, d_model)\n",
    "    mask: 注意力掩码（可选），shape: (batch_size, 1, seq_len, seq_len) 或 (batch_size, num_heads, seq_len, seq_len)\n",
    "    '''\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attention(x, x, x, mask)    # attn_output: 自注意力输出，shape: (batch_size, seq_len, d_model)\n",
    "        x = self.norm1(x + self.dropout(attn_output))   # 自注意层残差结果 shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)    # 前馈网络输出，shape: (batch_size, seq_len, d_model)\n",
    "        x = self.norm2(x + self.dropout(ff_output)) # FFN层残差结果 shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return x    # shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "'''\n",
    "Transformer 解码器层\n",
    "    这个类实现了 Transformer 的解码器层，包含三个主要组件：\n",
    "        自注意力机制：允许解码器关注目标序列中已生成的部分（使用掩码防止未来信息泄露）\n",
    "        交叉注意力机制：允许解码器关注编码器的输出\n",
    "        前馈神经网络：对每个位置独立应用的全连接网络\n",
    "    每个组件后面都跟有：\n",
    "        Dropout 正则化\n",
    "        残差连接（跳跃连接）\n",
    "        层归一化\n",
    "'''\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    num_heads: 注意力头数 (int)\n",
    "    d_ff: 前馈网络隐藏层维度 (int)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)   # 多头自注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout) # 多头交叉注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        # 前馈神经网络模块\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),   # 第一层线性变换: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),  # ReLU 激活函数: shape 保持不变\n",
    "            nn.Dropout(dropout), # Dropout: shape 保持不变\n",
    "            nn.Linear(d_ff, d_model)    # 第二层线性变换: (batch_size, seq_len, d_ff) → (batch_size, seq_len d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 第1层归一化(用于自注意力后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # 第2层归一化(用于交叉注意力后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model))\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # 第3层归一化(用于前馈网络后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout\n",
    "\n",
    "    \"\"\"\n",
    "    x: 解码器输入（目标序列），shape: (batch_size, tgt_seq_len, d_model)\n",
    "    encoder_output: 编码器输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "    self_attn_mask: 自注意力掩码（用于防止未来信息泄露），shape: (batch_size, 1, tgt_seq_len, tgt_seq_len) 或 (batch_size, num_heads, tgt_seq_len, tgt_seq_len)\n",
    "    cross_attn_mask: 交叉注意力掩码，shape: (batch_size, 1, tgt_seq_len, src_seq_len) 或 (batch_size, num_heads, tgt_seq_len, src_seq_len)\n",
    "    \"\"\"\n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        self_attn_output = self.self_attention(x, x, x, self_attn_mask) # 自注意力输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))  # 自注意层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Cross-attention with residual connection\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, cross_attn_mask)    # 交叉注意力输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output)) # 交叉注意层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)    # 前馈网络输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm3(x + self.dropout(ff_output))     # FFN层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        return x    # shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "'''\n",
    "完整的 Transformer 模型，包含以下主要组件：\n",
    "    嵌入层：将输入 token 转换为向量表示\n",
    "    位置编码：添加位置信息\n",
    "    编码器：由多个编码器层组成\n",
    "    解码器：由多个解码器层组成\n",
    "    输出投影：将解码器输出映射到词汇表大小\n",
    "输入输出 shape:\n",
    "    输入:\n",
    "        src: (batch_size, src_seq_len)\n",
    "        tgt: (batch_size, tgt_seq_len)\n",
    "    输出:\n",
    "        (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "'''\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder_vocab_size: 编码器词汇表大小 (int)\n",
    "    decoder_vocab_size: 解码器词汇表大小 (int)\n",
    "    d_model: 模型维度 (int，默认 512)\n",
    "    num_heads: 注意力头数 (int，默认 8)\n",
    "    num_encoder_layers: 编码器层数 (int，默认 6)\n",
    "    num_decoder_layers: 解码器层数 (int，默认 6)\n",
    "    d_ff: 前馈网络隐藏层维度 (int，默认 2048)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    max_len: 序列最大长度 (int，默认 100)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, d_model=512, num_heads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1, max_len=100):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(encoder_vocab_size, d_model, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, d_model, padding_idx=0)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Encoder layers 编码器层列表，包含 num_encoder_layers 个编码器层\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers    解码器层列表，包含 num_decoder_layers 个解码器层\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, decoder_vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)  # 使用 Xavier 均匀分布初始化参数。\n",
    "\n",
    "    # 创建填充掩码,\n",
    "    # x: 输入张量，shape: (batch_size, seq_len)\n",
    "    def create_padding_mask(self, x, pad_idx=0):\n",
    "        return (x != pad_idx).unsqueeze(1).unsqueeze(2) # 返回: 填充掩码，shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    '''\n",
    "    创建前瞻掩码（防止未来信息泄露）。\n",
    "        size: 序列长度 (int)\n",
    "        返回: 前瞻掩码，shape: (size, size)\n",
    "    '''\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 0\n",
    "\n",
    "    '''\n",
    "    src: 源序列，shape: (batch_size, src_seq_len)\n",
    "    src_mask: 源掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "    '''\n",
    "    def encode(self, src, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        \"\"\"\n",
    "        self.encoder_embedding(src): 嵌入，shape: (batch_size, src_seq_len) → (batch_size, src_seq_len, d_model)\n",
    "        src_emb: 嵌入后乘以 sqrt(d_model)，shape: (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        src_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        \"\"\"\n",
    "        src_emb.transpose(0, 1): 转置，shape: (src_seq_len, batch_size, d_model)\n",
    "        self.positional_encoding(...): 添加位置编码，shape: (src_seq_len, batch_size, d_model)\n",
    "        .transpose(0, 1): 转置回来，shape: (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        src_emb = self.positional_encoding(src_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        src_emb = self.dropout(src_emb) # 应用 dropout，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        encoder_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)    # 经过所有编码器层，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    \"\"\"\n",
    "    tgt: 目标序列，shape: (batch_size, tgt_seq_len)\n",
    "    encoder_output: 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "    tgt_mask: 目标掩码，shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "    src_mask: 源掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "    \"\"\"\n",
    "    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        \"\"\"\n",
    "        self.decoder_embedding(tgt): 嵌入，shape: (batch_size, tgt_seq_len) → (batch_size, tgt_seq_len, d_model)\n",
    "        tgt_emb: 嵌入后乘以 sqrt(d_model)，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        tgt_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        '''\n",
    "        tgt_emb.transpose(0, 1): 转置，shape: (tgt_seq_len, batch_size, d_model)\n",
    "        self.positional_encoding(...): 添加位置编码，shape: (tgt_seq_len, batch_size, d_model)\n",
    "        .transpose(0, 1): 转置回来，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        '''\n",
    "        tgt_emb = self.positional_encoding(tgt_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        tgt_emb = self.dropout(tgt_emb) # 应用 dropout，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        decoder_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, tgt_mask, src_mask)  # 经过所有解码器层，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "    \"\"\"\n",
    "    前向传播方法。\n",
    "        src: 源序列，shape: (batch_size, src_seq_len)\n",
    "        tgt: 目标序列，shape: (batch_size, tgt_seq_len)\n",
    "    \"\"\"\n",
    "    def forward(self, src, tgt):\n",
    "        # Create masks\n",
    "        src_mask = self.create_padding_mask(src)    # 源填充掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "        \"\"\"\n",
    "        self.create_padding_mask(tgt): 目标填充掩码，shape: (batch_size, 1, 1, tgt_seq_len)\n",
    "        self.create_look_ahead_mask(tgt.size(1)): 前瞻掩码，shape: (tgt_seq_len, tgt_seq_len)\n",
    "        tgt_mask: 组合掩码，shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        tgt_mask = self.create_padding_mask(tgt) & self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        # Encode\n",
    "        encoder_output = self.encode(src, src_mask) # 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, encoder_output, tgt_mask, src_mask)   # 解码器输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output_projection(decoder_output)\n",
    "\n",
    "        return output   # 最终输出，shape: (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "\n",
    "    \"\"\"\n",
    "    生成翻译结果（贪婪解码）。\n",
    "        src: 源序列，shape: (batch_size, src_seq_len)\n",
    "        max_length: 最大生成长度 (int，默认 100)\n",
    "        bos_token: 开始标记 (int，默认 101)\n",
    "        eos_token: 结束标记 (int，默认 102)\n",
    "        pad_token: 填充标记 (int，默认 0)\n",
    "    \"\"\"\n",
    "    def generate(self, src, max_length=100, bos_token=101, eos_token=102, pad_token=0):\n",
    "        \"\"\"Generate translation using greedy decoding\"\"\"\n",
    "        self.eval()\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "\n",
    "        # Encode source\n",
    "        src_mask = self.create_padding_mask(src)    # 源填充掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "        encoder_output = self.encode(src, src_mask) # 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Initialize decoder input with BOS token, 初始化解码器输入（只有开始标记），shape: (batch_size, 1)\n",
    "        decoder_input = torch.full((batch_size, 1), bos_token, device=device)\n",
    "\n",
    "        generated_sequences = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Create target mask\n",
    "            tgt_mask = self.create_padding_mask(decoder_input) & \\\n",
    "                      self.create_look_ahead_mask(decoder_input.size(1)).to(device)\n",
    "            # 目标掩码，shape: (batch_size, 1, decoder_input.size(1), decoder_input.size(1))\n",
    "\n",
    "            # Decode\n",
    "            decoder_output = self.decode(decoder_input, encoder_output, tgt_mask, src_mask) # 解码器输出，shape: (batch_size, decoder_input.size(1), d_model)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            next_token_logits = self.output_projection(decoder_output[:, -1, :])    # 取最后一个时间步的输出，shape: (batch_size, d_model)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # 下一个标记的 logits，shape: (batch_size, decoder_vocab_size)\n",
    "\n",
    "            # Append to decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)   # 更新解码器输入，shape: (batch_size, decoder_input.size(1) + 1)\n",
    "\n",
    "            # Check if all sequences have generated EOS token, 检查是否所有序列都生成了结束标记\n",
    "            if torch.all(next_token.squeeze() == eos_token):\n",
    "                break\n",
    "\n",
    "        # Remove BOS token from generated sequences\n",
    "        generated_sequences = decoder_input[:, 1:]  # 移除开始标记后的生成序列，shape: (batch_size, decoder_input.size(1) - 1)\n",
    "\n",
    "        return generated_sequences  # (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "\n",
    "# Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'd_model': 256,     # 从512减少到256，减少参数量\n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'd_ff': 2048,       # 从2048减少到1024，减少前馈网络参数\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 100\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Transformer Model Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Model Dimension: {model_config['d_model']}\")\n",
    "print(f\"Number of Heads: {model_config['num_heads']}\")\n",
    "print(f\"Encoder Layers: {model_config['num_encoder_layers']}\")\n",
    "print(f\"Decoder Layers: {model_config['num_decoder_layers']}\")\n",
    "print(f\"Feed Forward Dimension: {model_config['d_ff']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "decoder_input = target_ids[:, :-1]  # Remove last token for teacher forcing\n",
    "outputs = model(source_ids, decoder_input) # Forward pass，shape: (batch_size, target_len - 1， decoder_vocab_size)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len-1, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=50)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0].tolist()[:20]}...\")  # Show first 20 tokens\n",
    "\n"
   ],
   "id": "5c8135cba413a52c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model with comprehensive training loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for Seq2Seq machine translation model\n",
    "\n",
    "    model: 要训练的模型，shape: 任意\n",
    "    train_dataloader: 训练数据加载器\n",
    "    val_dataloader: 验证数据加载器\n",
    "    device: 训练设备（CPU 或 GPU）\n",
    "    learning_rate: 学习率 (float，默认 1e-3)\n",
    "    weight_decay: 权重衰减 (float，默认 1e-5)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, device,\n",
    "                 learning_rate=1e-3, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "\n",
    "        # Loss function - ignore padding tokens (index 0)\n",
    "        '''\n",
    "        self.criterion: 交叉熵损失函数，忽略填充标记（索引为 0）\n",
    "        输入:\n",
    "            预测值: (N, C) 其中 N 是样本数，C 是类别数\n",
    "            真实值: (N,)\n",
    "        输出: 标量损失值\n",
    "        '''\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(model.parameters(),\n",
    "                                  lr=learning_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2\n",
    "        )\n",
    "\n",
    "        # Training history\n",
    "        self.history = defaultdict(list)\n",
    "\n",
    "    '''\n",
    "    计算批次损失\n",
    "    outputs: 模型输出，shape: (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "    targets: 目标序列，shape: (batch_size, tgt_seq_len)\n",
    "    target_lengths: 目标序列长度（可选）\n",
    "    '''\n",
    "    def calculate_loss(self, outputs, targets, target_lengths=None):\n",
    "        \"\"\"Calculate loss for the batch\"\"\"\n",
    "        # Reshape for loss calculation (use reshape instead of view for non-contiguous tensors)\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))    # 展平的输出，shape: (batch_size * tgt_seq_len, decoder_vocab_size)\n",
    "        targets_flat = targets.reshape(-1)  # 展平的目标，shape: (batch_size * tgt_seq_len,)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(outputs_flat, targets_flat)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            source_ids = batch['source_ids'].to(self.device)\n",
    "            target_ids = batch['target_ids'].to(self.device)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with teacher forcing\n",
    "            decoder_input = target_ids[:, :-1]  # Remove last token\n",
    "            decoder_targets = target_ids[:, 1:]  # Remove first token (BOS)\n",
    "\n",
    "            outputs = self.model(source_ids, decoder_input)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(outputs, decoder_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "            })\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 100 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(self.val_dataloader, desc=\"Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                source_ids = batch['source_ids'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                decoder_input = target_ids[:, :-1]\n",
    "                decoder_targets = target_ids[:, 1:]\n",
    "\n",
    "                outputs = self.model(source_ids, decoder_input)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.calculate_loss(outputs, decoder_targets)\n",
    "\n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Val Loss': f'{loss.item():.4f}',\n",
    "                    'Avg Val Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "                })\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def train(self, num_epochs=10, save_path=None):\n",
    "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
    "        print(f\"=== Starting Training for {num_epochs} Epochs ===\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Train batches: {len(self.train_dataloader)}\")\n",
    "        print(f\"Validation batches: {len(self.val_dataloader)}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "\n",
    "            # Validate\n",
    "            val_loss = self.validate_epoch()\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Calculate epoch time\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'model_config': model_config\n",
    "                    }, save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Training summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training completed in {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Final train loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final val loss: {self.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot loss\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "\n",
    "        # Plot learning rate\n",
    "        axes[0, 1].plot(self.history['learning_rate'], color='green')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "        # Plot epoch time\n",
    "        axes[1, 0].plot(self.history['epoch_time'], color='orange')\n",
    "        axes[1, 0].set_title('Epoch Training Time')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # Plot loss comparison\n",
    "        x = range(len(self.history['train_loss']))\n",
    "        axes[1, 1].fill_between(x, self.history['train_loss'], alpha=0.3, color='blue', label='Train Loss')\n",
    "        axes[1, 1].fill_between(x, self.history['val_loss'], alpha=0.3, color='red', label='Val Loss')\n",
    "        axes[1, 1].plot(self.history['train_loss'], color='blue', linewidth=2)\n",
    "        axes[1, 1].plot(self.history['val_loss'], color='red', linewidth=2)\n",
    "        axes[1, 1].set_title('Loss Comparison')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Start training for 10 epochs\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(num_epochs=10, save_path=\"best_seq2seq_transformer_model.pth\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history()\n"
   ],
   "id": "f8482590923101c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fixed Comprehensive Machine Translation Testing Code for Transformer Model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "class TransformerTranslationTester:\n",
    "    \"\"\"\n",
    "    Comprehensive tester for the Transformer machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device, max_length=100):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load tokenizers\n",
    "        self.tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "        # Special tokens\n",
    "        self.bos_token = 101  # [CLS] token used as BOS\n",
    "        self.eos_token = 102  # [SEP] token used as EOS\n",
    "        self.pad_token = 0    # [PAD] token\n",
    "\n",
    "        print(\"=== Transformer Translation Tester Initialized ===\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"English tokenizer vocab size: {self.tokenizer_en.vocab_size}\")\n",
    "        print(f\"Chinese tokenizer vocab size: {self.tokenizer_zh.vocab_size}\")\n",
    "        print(f\"Max sequence length: {max_length}\")\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess input text\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "        return text\n",
    "\n",
    "    def encode_english(self, text):\n",
    "        \"\"\"Encode English text to token IDs\"\"\"\n",
    "        text = self.preprocess_text(text)\n",
    "        tokens = self.tokenizer_en(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].to(self.device)\n",
    "\n",
    "    def decode_chinese(self, token_ids):\n",
    "        \"\"\"Decode Chinese token IDs to text\"\"\"\n",
    "        # Remove padding tokens and special tokens\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.cpu().tolist()\n",
    "\n",
    "        # Remove padding (0), BOS (101), and EOS (102) tokens\n",
    "        cleaned_tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in [0, 101, 102]:\n",
    "                cleaned_tokens.append(token_id)\n",
    "            elif token_id == 102:  # Stop at EOS token\n",
    "                break\n",
    "\n",
    "        if not cleaned_tokens:\n",
    "            return \"[Empty translation]\"\n",
    "\n",
    "        try:\n",
    "            text = self.tokenizer_zh.decode(cleaned_tokens, skip_special_tokens=True)\n",
    "            # Clean up extra spaces for Chinese\n",
    "            text = re.sub(r'\\s+', '', text)  # Remove all spaces for Chinese\n",
    "            return text if text else \"[Empty translation]\"\n",
    "        except Exception as e:\n",
    "            return f\"[Decoding error: {str(e)}]\"\n",
    "\n",
    "    def translate_single(self, english_text, use_beam_search=False, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Translate a single English sentence to Chinese\n",
    "\n",
    "        Args:\n",
    "            english_text: Input English text\n",
    "            use_beam_search: Whether to use beam search (simplified implementation)\n",
    "            beam_size: Beam search size (only used if use_beam_search=True)\n",
    "            temperature: Sampling temperature for generation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode input\n",
    "            source_ids = self.encode_english(english_text)\n",
    "\n",
    "            if not use_beam_search:\n",
    "                # Use the model's built-in greedy generation\n",
    "                generated = self.model.generate(\n",
    "                    source_ids,\n",
    "                    max_length=self.max_length,\n",
    "                    bos_token=self.bos_token,\n",
    "                    eos_token=self.eos_token,\n",
    "                    pad_token=self.pad_token\n",
    "                )\n",
    "                translation = self.decode_chinese(generated[0])\n",
    "            else:\n",
    "                # Use simplified beam search\n",
    "                translation = self._beam_search_translate(source_ids, beam_size, temperature)\n",
    "\n",
    "        return translation\n",
    "\n",
    "    def _beam_search_translate(self, source_ids, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Simplified beam search implementation for Transformer model\n",
    "        \"\"\"\n",
    "        batch_size = source_ids.size(0)\n",
    "        device = source_ids.device\n",
    "\n",
    "        # Encode source\n",
    "        src_mask = self.model.create_padding_mask(source_ids)\n",
    "        encoder_output = self.model.encode(source_ids, src_mask)\n",
    "\n",
    "        # Initialize beams: (sequence, score)\n",
    "        beams = [(torch.tensor([[self.bos_token]], device=device), 0.0)]\n",
    "\n",
    "        for step in range(self.max_length):\n",
    "            new_beams = []\n",
    "\n",
    "            for seq, score in beams:\n",
    "                # If sequence already ended, keep it\n",
    "                if seq[0, -1].item() == self.eos_token:\n",
    "                    new_beams.append((seq, score))\n",
    "                    continue\n",
    "\n",
    "                # Create masks for current sequence\n",
    "                tgt_mask = self.model.create_padding_mask(seq) & \\\n",
    "                          self.model.create_look_ahead_mask(seq.size(1)).to(device)\n",
    "\n",
    "                # Get decoder output\n",
    "                decoder_output = self.model.decode(seq, encoder_output, tgt_mask, src_mask)\n",
    "\n",
    "                # Get next token probabilities\n",
    "                logits = self.model.output_projection(decoder_output[:, -1, :])\n",
    "\n",
    "                # Apply temperature\n",
    "                if temperature != 1.0:\n",
    "                    logits = logits / temperature\n",
    "\n",
    "                # Get probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Get top k candidates\n",
    "                top_probs, top_indices = torch.topk(probs, min(beam_size, probs.size(-1)))\n",
    "\n",
    "                for i in range(top_probs.size(1)):\n",
    "                    token_id = top_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                    token_prob = top_probs[0, i].item()\n",
    "                    new_seq = torch.cat([seq, token_id], dim=1)\n",
    "                    new_score = score + math.log(token_prob + 1e-10)\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "\n",
    "            # Keep only top beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "            # Check if all beams ended\n",
    "            if all(seq[0, -1].item() == self.eos_token for seq, _ in beams):\n",
    "                break\n",
    "\n",
    "        # Return best beam\n",
    "        best_seq = beams[0][0]\n",
    "        return self.decode_chinese(best_seq[0])\n",
    "\n",
    "    def calculate_bleu_score(self, reference, candidate):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score (simplified version)\n",
    "        \"\"\"\n",
    "        def get_ngrams(tokens, n):\n",
    "            return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "        # Tokenize (character-level for Chinese)\n",
    "        ref_tokens = list(reference.replace(' ', ''))\n",
    "        cand_tokens = list(candidate.replace(' ', ''))\n",
    "\n",
    "        if len(cand_tokens) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate precision for n-grams (1 to 4)\n",
    "        precisions = []\n",
    "        for n in range(1, 5):\n",
    "            ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "            cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "\n",
    "            if len(cand_ngrams) == 0:\n",
    "                precisions.append(0.0)\n",
    "                continue\n",
    "\n",
    "            matches = sum((ref_ngrams & cand_ngrams).values())\n",
    "            precision = matches / len(get_ngrams(cand_tokens, n))\n",
    "            precisions.append(precision)\n",
    "\n",
    "        # Calculate brevity penalty\n",
    "        ref_len = len(ref_tokens)\n",
    "        cand_len = len(cand_tokens)\n",
    "\n",
    "        if cand_len > ref_len:\n",
    "            bp = 1.0\n",
    "        else:\n",
    "            bp = math.exp(1 - ref_len / (cand_len + 1e-10))\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        if min(precisions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        bleu = bp * math.exp(sum(math.log(p + 1e-10) for p in precisions) / 4)\n",
    "        return bleu\n",
    "\n",
    "    def test_examples(self, test_cases, use_beam_search=False, beam_size=3, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Test the model on multiple examples\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MACHINE TRANSLATION TESTING RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Model: Transformer (Attention is All You Need)\")\n",
    "        print(f\"Decoding: {'Beam Search' if use_beam_search else 'Greedy'}\")\n",
    "        if use_beam_search:\n",
    "            print(f\"Beam size: {beam_size}\")\n",
    "            print(f\"Temperature: {temperature}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        results = []\n",
    "        total_time = 0\n",
    "\n",
    "        for i, (english, expected_chinese) in enumerate(test_cases, 1):\n",
    "            print(f\"\\nTest {i}/{len(test_cases)}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"English:  {english}\")\n",
    "\n",
    "            # Translate\n",
    "            start_time = time.time()\n",
    "            translation = self.translate_single(english, use_beam_search, beam_size, temperature)\n",
    "            translation_time = time.time() - start_time\n",
    "            total_time += translation_time\n",
    "\n",
    "            print(f\"Expected: {expected_chinese}\")\n",
    "            print(f\"Generated: {translation}\")\n",
    "            print(f\"Time: {translation_time:.3f}s\")\n",
    "\n",
    "            # Calculate BLEU score if reference is provided\n",
    "            if expected_chinese and expected_chinese != \"\":\n",
    "                bleu_score = self.calculate_bleu_score(expected_chinese, translation)\n",
    "                print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "            else:\n",
    "                bleu_score = None\n",
    "                print(\"BLEU Score: N/A (no reference)\")\n",
    "\n",
    "            results.append({\n",
    "                'english': english,\n",
    "                'expected': expected_chinese,\n",
    "                'translation': translation,\n",
    "                'time': translation_time,\n",
    "                'bleu': bleu_score\n",
    "            })\n",
    "\n",
    "        # Summary statistics\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total examples: {len(test_cases)}\")\n",
    "        print(f\"Total time: {total_time:.3f}s\")\n",
    "        print(f\"Average time per translation: {total_time/len(test_cases):.3f}s\")\n",
    "\n",
    "        bleu_scores = [r['bleu'] for r in results if r['bleu'] is not None]\n",
    "        if bleu_scores:\n",
    "            print(f\"Average BLEU score: {np.mean(bleu_scores):.4f}\")\n",
    "            print(f\"Max BLEU score: {max(bleu_scores):.4f}\")\n",
    "            print(f\"Min BLEU score: {min(bleu_scores):.4f}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def interactive_translation(self):\n",
    "        \"\"\"Interactive translation mode\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INTERACTIVE TRANSLATION MODE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Enter English text to translate (type 'quit' to exit)\")\n",
    "        print(\"Type 'beam' to toggle beam search mode\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        use_beam_search = False\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(f\"\\nEnglish ({'Beam' if use_beam_search else 'Greedy'}): \").strip()\n",
    "\n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "\n",
    "                if user_input.lower() == 'beam':\n",
    "                    use_beam_search = not use_beam_search\n",
    "                    print(f\"Switched to {'Beam Search' if use_beam_search else 'Greedy'} mode\")\n",
    "                    continue\n",
    "\n",
    "                if not user_input:\n",
    "                    print(\"Please enter some text.\")\n",
    "                    continue\n",
    "\n",
    "                # Translate\n",
    "                start_time = time.time()\n",
    "                translation = self.translate_single(user_input, use_beam_search=use_beam_search)\n",
    "                translation_time = time.time() - start_time\n",
    "\n",
    "                print(f\"Chinese:  {translation}\")\n",
    "                print(f\"Time: {translation_time:.3f}s\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test cases for English to Chinese translation\n",
    "TRANSFORMER_TEST_CASES = [\n",
    "    # Basic greetings and common phrases\n",
    "    (\"Hello\", \"你好\"),\n",
    "    (\"Good morning\", \"早上好\"),\n",
    "    (\"Thank you\", \"谢谢\"),\n",
    "    (\"Nice to meet you\", \"很高兴见到你\"),\n",
    "    (\"Goodbye\", \"再见\"),\n",
    "\n",
    "    # Simple sentences\n",
    "    (\"I love you\", \"我爱你\"),\n",
    "    (\"What is your name?\", \"你叫什么名字？\"),\n",
    "    (\"Where are you from?\", \"你来自哪里？\"),\n",
    "    (\"How are you?\", \"你好吗？\"),\n",
    "    (\"Can you help me?\", \"你能帮助我吗？\"),\n",
    "\n",
    "    # Daily life\n",
    "    (\"I am hungry\", \"我饿了\"),\n",
    "    (\"The weather is nice\", \"天气很好\"),\n",
    "    (\"I need to go to work\", \"我需要去工作\"),\n",
    "    (\"Let's have dinner\", \"我们吃晚饭吧\"),\n",
    "    (\"I am learning Chinese\", \"我在学中文\"),\n",
    "\n",
    "    # Without reference translations (for testing only)\n",
    "    (\"Machine learning is important\", \"\"),\n",
    "    (\"I like to read books\", \"\"),\n",
    "    (\"The cat is sleeping\", \"\"),\n",
    "    (\"Tomorrow is Monday\", \"\"),\n",
    "    (\"Coffee tastes good\", \"\"),\n",
    "]\n",
    "\n",
    "# Initialize the corrected tester\n",
    "print(\"Initializing Transformer translation tester...\")\n",
    "transformer_tester = TransformerTranslationTester(model, device, max_length=100)\n",
    "\n",
    "# Check if model was trained\n",
    "has_trained_model = 'history' in globals() and history is not None\n",
    "if not has_trained_model:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WARNING: Model appears to be untrained!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"The model has not been trained yet, so translations will be random.\")\n",
    "    print(\"To get meaningful results, you should train the model first.\")\n",
    "    print(\"You can still test the translation pipeline, but expect poor quality.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Test with simple examples first (Greedy decoding)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING SIMPLE EXAMPLES (Greedy Decoding)\")\n",
    "print(\"=\"*80)\n",
    "simple_results = transformer_tester.test_examples(TRANSFORMER_TEST_CASES[:3], use_beam_search=False)\n",
    "\n",
    "# Test one example with beam search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING WITH BEAM SEARCH\")\n",
    "print(\"=\"*80)\n",
    "beam_results = transformer_tester.test_examples(TRANSFORMER_TEST_CASES[3:4], use_beam_search=True, beam_size=3, temperature=0.8)\n",
    "\n",
    "# Quick performance test\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK PERFORMANCE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello\",\n",
    "    \"I am happy\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "print(\"Testing translation speed:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    start_time = time.time()\n",
    "    translation = transformer_tester.translate_single(sentence)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Test {i}: {end_time - start_time:.3f}s\")\n",
    "    print(f\"  Input:  {sentence}\")\n",
    "    print(f\"  Output: {translation}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFORMER TESTING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"You can now use:\")\n",
    "print(\"- transformer_tester.translate_single('your text here')\")\n",
    "print(\"- transformer_tester.interactive_translation()\")\n",
    "print(\"- transformer_tester.test_examples(your_test_cases)\")\n",
    "print(\"=\"*80)"
   ],
   "id": "ef1276ba63cd639a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
