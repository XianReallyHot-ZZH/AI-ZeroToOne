{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coding: Machine Translation with Transformer\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 1m high-quality pairs of data\n",
    " - Model: Seq2seq with Transformer (Attention is all you need)\n",
    " - GPU: 1660TI"
   ],
   "id": "7430319f164ba198"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-23T14:16:49.401751Z",
     "start_time": "2025-08-23T14:16:49.067249Z"
    }
   },
   "source": [
    "# 可以先测试网络连接\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\")\n",
    "    print(\"网络连接正常\")\n",
    "except:\n",
    "    print(\"网络连接可能存在问题\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络连接正常\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T14:18:11.556521Z",
     "start_time": "2025-08-23T14:17:53.816612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\", cache_dir=r\"D:\\Developer\\LLM\\FuggingFace-cache-model\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    import re  # 添加这一行\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "# 选出最多500万条数据\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ],
   "id": "5849dc8dec6a8f6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T14:21:57.815241Z",
     "start_time": "2025-08-23T14:21:33.246175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 继承Dataset构建英中文翻译数据集\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        # 对英文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        en_tokens = self.tokenizer_en(en_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 对中文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        zh_tokens = self.tokenizer_zh(zh_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 返回数据结构\n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),     # 去除张量的多余维度，输出以为数字数组，文本对应的tokenizer后的数组\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,         # 原始文本\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "# 构建训练和验证数据集\n",
    "def create_dataloaders(dataset, batch_size=128, num_workers=0, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "\n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                train_size=train_size,\n",
    "                                                random_state=42)\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets， 得到tokenizer后的训练数据集和验证数据集\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create DataLoaders 创建了pytorch数据加载器，封装了数据集，并定义了batch_size，shuffle，num_workers，方便训练时设置数据加载相关的超参数\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset, batch_size=64)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ],
   "id": "382950eb10c7c1e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for en: 30522\n",
      "Vocab size for zh: 21128\n",
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 16950 batches\n",
      "Validation DataLoader: 893 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 64\n",
      "Source example: Iraq\n",
      "Source tokens: tensor([ 101, 5712,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Target example: 伊拉克\n",
      "Target tokens: tensor([ 101,  823, 2861, 1046,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 64\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens: tensor([  101,  6356,  1998,  1037,  1013,  3438,  1013, 19714,  1013,  5587,\n",
      "         1012,  1015,  1010, 11498,  1012,  1020,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens: tensor([  101,   113,   100,   120,  8183,   120, 11410,   120,   100,   119,\n",
      "          122,  8024,  5018,   127,  3667,   114,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T08:03:21.160636Z",
     "start_time": "2025-08-24T08:03:19.774953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Transformer model for Machine Translation\n",
    "# Following \"Attention is All You Need\" architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 # 模型维度必须能被注意力头数整除，模型维度就相当于是每个token在embedding编码后的维度\n",
    "\n",
    "        self.d_model = d_model  # 模型维度 (int)\n",
    "        self.num_heads = num_heads  # 注意力头数 (int)\n",
    "        self.d_k = d_model // num_heads # 每个注意力头维度 (int)\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # 查询权重矩阵 (d_model × d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # 键权重矩阵 (d_model × d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # 值权重矩阵 (d_model × d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # 输出权重矩阵 (d_model × d_model)\n",
    "\n",
    "        # dropout 层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # 实现缩放点积注意力机制。\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate attention scores 计算注意力分值\n",
    "            Q: 查询张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            K: 键张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            K.transpose(-2, -1): 转置后的键张量，shape: (batch_size, num_heads, d_k, seq_len)\n",
    "            torch.matmul(Q, K.transpose(-2, -1)): 矩阵乘法，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "            scores: 注意力分数，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # (batch_size, num_heads, seq_len, seq_len)，相当于计算查询和键之间的相似度，当查询和键是同一个矩阵的时候，就是计算两个向量的自我相似度。\n",
    "\n",
    "        \"\"\"\n",
    "        Apply mask if provided\n",
    "            mask: 遮蔽张量，shape: (batch_size, 1, seq_len, seq_len) 或 (batch_size, num_heads, seq_len, seq_len)\n",
    "            scores: 遮蔽后的注意力分数，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # 注意力权重，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        \"\"\"\n",
    "        Apply attention to values\n",
    "            V: 值张量，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            output: 注意力输出，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights # 返回注意力输出output:(batch_size, num_heads, seq_len, d_k) 和注意力权重attention_weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "    \"\"\"\n",
    "    query: 查询输入，shape: (batch_size, seq_len, d_model)\n",
    "    key: 键输入，shape: (batch_size, seq_len, d_model)\n",
    "    value: 值输入，shape: (batch_size, seq_len, d_model)\n",
    "    batch_size: 批次大小 (int)\n",
    "    \"\"\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        \"\"\"\n",
    "        Linear transformations and reshape\n",
    "            self.W_q(query): 线性变换，shape: (batch_size, seq_len, d_model)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k): 重塑，shape: (batch_size, seq_len, num_heads, d_k)\n",
    "            Q, K, V: 转置后，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        \"\"\"\n",
    "        Apply attention\n",
    "            attention_output: 注意力输出，shape: (batch_size, num_heads, seq_len, d_k)\n",
    "            attention_weights: 注意力权重，shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        \"\"\"\n",
    "        Concatenate heads\n",
    "            attention_output.transpose(1, 2): 转置，shape: (batch_size, seq_len, num_heads, d_k)\n",
    "            .contiguous().view(batch_size, -1, self.d_model): 连续化并重塑，shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear transformation\n",
    "        output = self.W_o(attention_output)\n",
    "\n",
    "        return output   # output: 最终输出，shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "# 定义位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    max_len: 序列最大长度 (int，默认 100)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # pe: 位置编码矩阵，shape: (max_len, d_model), 初始化为零矩阵，用于存储位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        '''\n",
    "        torch.arange(0, max_len, dtype=torch.float): 生成 0 到 max_len-1 的序列，shape: (max_len,)\n",
    "        .unsqueeze(1): 增加一个维度，shape: (max_len, 1)\n",
    "        position: 位置索引矩阵，每一行代表一个位置索引\n",
    "        '''\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # shape: (max_len, 1)\n",
    "        '''\n",
    "        torch.arange(0, d_model, 2): 生成 0, 2, 4, ..., d_model-2 的序列（偶数索引）\n",
    "        .float(): 转换为浮点数\n",
    "        (-math.log(10000.0) / d_model): 缩放因子\n",
    "        div_term: 频率分母项，shape: (d_model/2,)\n",
    "        '''\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        '''\n",
    "        position * div_term: 广播乘法，shape: (max_len, d_model/2)\n",
    "        torch.sin(position * div_term): 正弦编码，shape: (max_len, d_model/2)\n",
    "        pe[:, 0::2]: 选择偶数列（0, 2, 4, ...），shape: (max_len, d_model/2)\n",
    "        torch.cos(position * div_term): 余弦编码，shape: (max_len, d_model/2)\n",
    "        pe[:, 1::2]: 选择奇数列（1, 3, 5, ...），shape: (max_len, d_model/2)\n",
    "        '''\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        '''\n",
    "        .unsqueeze(0): 增加批次维度，shape: (1, max_len, d_model)\n",
    "        .transpose(0, 1): 交换第0和第1维度，shape: (max_len, 1, d_model)\n",
    "        '''\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        # 将 pe 注册为缓冲区，不会被当作模型参数更新，但在 GPU 上会跟随模型移动\n",
    "        self.register_buffer('pe', pe) # shape: (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 输入张量，shape: (seq_len, batch_size, d_model)\n",
    "        self.pe[:x.size(0), :]: 选择与输入序列长度相匹配的位置编码，shape: (seq_len, 1, d_model)\n",
    "        x + self.pe[:x.size(0), :]: 广播加法，将位置编码添加到输入中，shape: (seq_len, batch_size, d_model)\n",
    "        返回添加了位置编码的张量\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size(0), :]   # shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "# Transformer 编码器层\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    num_heads: 注意力头数 (int)\n",
    "    d_ff: 前馈网络隐藏层维度 (int)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)   # 多头自注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        # 前馈神经网络模块\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),   # 第一层线性变换: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),  # ReLU 激活函数: shape 保持不变\n",
    "            nn.Dropout(dropout), # Dropout: shape 保持不变\n",
    "            nn.Linear(d_ff, d_model)    # 第二层线性变换: (batch_size, seq_len, d_ff) → (batch_size, seq_len d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 第一个层归一化，shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # 第二个层归一化，shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    '''\n",
    "    x: 输入张量，shape: (batch_size, seq_len, d_model)\n",
    "    mask: 注意力掩码（可选），shape: (batch_size, 1, seq_len, seq_len) 或 (batch_size, num_heads, seq_len, seq_len)\n",
    "    '''\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attention(x, x, x, mask)    # attn_output: 自注意力输出，shape: (batch_size, seq_len, d_model)\n",
    "        x = self.norm1(x + self.dropout(attn_output))   # 自注意层残差结果 shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)    # 前馈网络输出，shape: (batch_size, seq_len, d_model)\n",
    "        x = self.norm2(x + self.dropout(ff_output)) # FFN层残差结果 shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return x    # shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "'''\n",
    "Transformer 解码器层\n",
    "    这个类实现了 Transformer 的解码器层，包含三个主要组件：\n",
    "        自注意力机制：允许解码器关注目标序列中已生成的部分（使用掩码防止未来信息泄露）\n",
    "        交叉注意力机制：允许解码器关注编码器的输出\n",
    "        前馈神经网络：对每个位置独立应用的全连接网络\n",
    "    每个组件后面都跟有：\n",
    "        Dropout 正则化\n",
    "        残差连接（跳跃连接）\n",
    "        层归一化\n",
    "'''\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: 模型维度 (int)\n",
    "    num_heads: 注意力头数 (int)\n",
    "    d_ff: 前馈网络隐藏层维度 (int)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)   # 多头自注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout) # 多头交叉注意力模块,输入输出 shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        # 前馈神经网络模块\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),   # 第一层线性变换: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_ff)\n",
    "            nn.ReLU(),  # ReLU 激活函数: shape 保持不变\n",
    "            nn.Dropout(dropout), # Dropout: shape 保持不变\n",
    "            nn.Linear(d_ff, d_model)    # 第二层线性变换: (batch_size, seq_len, d_ff) → (batch_size, seq_len d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 第1层归一化(用于自注意力后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # 第2层归一化(用于交叉注意力后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model))\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # 第3层归一化(用于前馈网络后）,shape: (batch_size, seq_len, d_model) → (batch_size, seq_len, d_model))\n",
    "        self.dropout = nn.Dropout(dropout)  # dropout\n",
    "\n",
    "    \"\"\"\n",
    "    x: 解码器输入（目标序列），shape: (batch_size, tgt_seq_len, d_model)\n",
    "    encoder_output: 编码器输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "    self_attn_mask: 自注意力掩码（用于防止未来信息泄露），shape: (batch_size, 1, tgt_seq_len, tgt_seq_len) 或 (batch_size, num_heads, tgt_seq_len, tgt_seq_len)\n",
    "    cross_attn_mask: 交叉注意力掩码，shape: (batch_size, 1, tgt_seq_len, src_seq_len) 或 (batch_size, num_heads, tgt_seq_len, src_seq_len)\n",
    "    \"\"\"\n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        self_attn_output = self.self_attention(x, x, x, self_attn_mask) # 自注意力输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))  # 自注意层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Cross-attention with residual connection\n",
    "        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output, cross_attn_mask)    # 交叉注意力输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output)) # 交叉注意层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)    # 前馈网络输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        x = self.norm3(x + self.dropout(ff_output))     # FFN层残差结果 shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        return x    # shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "'''\n",
    "完整的 Transformer 模型，包含以下主要组件：\n",
    "    嵌入层：将输入 token 转换为向量表示\n",
    "    位置编码：添加位置信息\n",
    "    编码器：由多个编码器层组成\n",
    "    解码器：由多个解码器层组成\n",
    "    输出投影：将解码器输出映射到词汇表大小\n",
    "输入输出 shape:\n",
    "    输入:\n",
    "        src: (batch_size, src_seq_len)\n",
    "        tgt: (batch_size, tgt_seq_len)\n",
    "    输出:\n",
    "        (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "'''\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder_vocab_size: 编码器词汇表大小 (int)\n",
    "    decoder_vocab_size: 解码器词汇表大小 (int)\n",
    "    d_model: 模型维度 (int，默认 512)\n",
    "    num_heads: 注意力头数 (int，默认 8)\n",
    "    num_encoder_layers: 编码器层数 (int，默认 6)\n",
    "    num_decoder_layers: 解码器层数 (int，默认 6)\n",
    "    d_ff: 前馈网络隐藏层维度 (int，默认 2048)\n",
    "    dropout: dropout 概率 (float，默认 0.1)\n",
    "    max_len: 序列最大长度 (int，默认 100)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, d_model=512, num_heads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1, max_len=100):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(encoder_vocab_size, d_model, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(decoder_vocab_size, d_model, padding_idx=0)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Encoder layers 编码器层列表，包含 num_encoder_layers 个编码器层\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers    解码器层列表，包含 num_decoder_layers 个解码器层\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, decoder_vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)  # 使用 Xavier 均匀分布初始化参数。\n",
    "\n",
    "    # 创建填充掩码,\n",
    "    # x: 输入张量，shape: (batch_size, seq_len)\n",
    "    def create_padding_mask(self, x, pad_idx=0):\n",
    "        return (x != pad_idx).unsqueeze(1).unsqueeze(2) # 返回: 填充掩码，shape: (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    '''\n",
    "    创建前瞻掩码（防止未来信息泄露）。\n",
    "        size: 序列长度 (int)\n",
    "        返回: 前瞻掩码，shape: (size, size)\n",
    "    '''\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 0\n",
    "\n",
    "    '''\n",
    "    src: 源序列，shape: (batch_size, src_seq_len)\n",
    "    src_mask: 源掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "    '''\n",
    "    def encode(self, src, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        \"\"\"\n",
    "        self.encoder_embedding(src): 嵌入，shape: (batch_size, src_seq_len) → (batch_size, src_seq_len, d_model)\n",
    "        src_emb: 嵌入后乘以 sqrt(d_model)，shape: (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        src_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        \"\"\"\n",
    "        src_emb.transpose(0, 1): 转置，shape: (src_seq_len, batch_size, d_model)\n",
    "        self.positional_encoding(...): 添加位置编码，shape: (src_seq_len, batch_size, d_model)\n",
    "        .transpose(0, 1): 转置回来，shape: (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        src_emb = self.positional_encoding(src_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        src_emb = self.dropout(src_emb) # 应用 dropout，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        encoder_output = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)    # 经过所有编码器层，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        return encoder_output\n",
    "\n",
    "    \"\"\"\n",
    "    tgt: 目标序列，shape: (batch_size, tgt_seq_len)\n",
    "    encoder_output: 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "    tgt_mask: 目标掩码，shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "    src_mask: 源掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "    \"\"\"\n",
    "    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        # Embedding and positional encoding\n",
    "        \"\"\"\n",
    "        self.decoder_embedding(tgt): 嵌入，shape: (batch_size, tgt_seq_len) → (batch_size, tgt_seq_len, d_model)\n",
    "        tgt_emb: 嵌入后乘以 sqrt(d_model)，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        tgt_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        '''\n",
    "        tgt_emb.transpose(0, 1): 转置，shape: (tgt_seq_len, batch_size, d_model)\n",
    "        self.positional_encoding(...): 添加位置编码，shape: (tgt_seq_len, batch_size, d_model)\n",
    "        .transpose(0, 1): 转置回来，shape: (batch_size, tgt_seq_len, d_model)\n",
    "        '''\n",
    "        tgt_emb = self.positional_encoding(tgt_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        tgt_emb = self.dropout(tgt_emb) # 应用 dropout，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        decoder_output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, tgt_mask, src_mask)  # 经过所有解码器层，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "    \"\"\"\n",
    "    前向传播方法。\n",
    "        src: 源序列，shape: (batch_size, src_seq_len)\n",
    "        tgt: 目标序列，shape: (batch_size, tgt_seq_len)\n",
    "    \"\"\"\n",
    "    def forward(self, src, tgt):\n",
    "        # Create masks\n",
    "        src_mask = self.create_padding_mask(src)    # 源填充掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "        \"\"\"\n",
    "        self.create_padding_mask(tgt): 目标填充掩码，shape: (batch_size, 1, 1, tgt_seq_len)\n",
    "        self.create_look_ahead_mask(tgt.size(1)): 前瞻掩码，shape: (tgt_seq_len, tgt_seq_len)\n",
    "        tgt_mask: 组合掩码，shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        tgt_mask = self.create_padding_mask(tgt) & self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        # Encode\n",
    "        encoder_output = self.encode(src, src_mask) # 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, encoder_output, tgt_mask, src_mask)   # 解码器输出，shape: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.output_projection(decoder_output)\n",
    "\n",
    "        return output   # 最终输出，shape: (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "\n",
    "    \"\"\"\n",
    "    生成翻译结果（贪婪解码）。\n",
    "        src: 源序列，shape: (batch_size, src_seq_len)\n",
    "        max_length: 最大生成长度 (int，默认 100)\n",
    "        bos_token: 开始标记 (int，默认 101)\n",
    "        eos_token: 结束标记 (int，默认 102)\n",
    "        pad_token: 填充标记 (int，默认 0)\n",
    "    \"\"\"\n",
    "    def generate(self, src, max_length=100, bos_token=101, eos_token=102, pad_token=0):\n",
    "        \"\"\"Generate translation using greedy decoding\"\"\"\n",
    "        self.eval()\n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "\n",
    "        # Encode source\n",
    "        src_mask = self.create_padding_mask(src)    # 源填充掩码，shape: (batch_size, 1, 1, src_seq_len)\n",
    "        encoder_output = self.encode(src, src_mask) # 编码器输出，shape: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # Initialize decoder input with BOS token, 初始化解码器输入（只有开始标记），shape: (batch_size, 1)\n",
    "        decoder_input = torch.full((batch_size, 1), bos_token, device=device)\n",
    "\n",
    "        generated_sequences = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Create target mask\n",
    "            tgt_mask = self.create_padding_mask(decoder_input) & \\\n",
    "                      self.create_look_ahead_mask(decoder_input.size(1)).to(device)\n",
    "            # 目标掩码，shape: (batch_size, 1, decoder_input.size(1), decoder_input.size(1))\n",
    "\n",
    "            # Decode\n",
    "            decoder_output = self.decode(decoder_input, encoder_output, tgt_mask, src_mask) # 解码器输出，shape: (batch_size, decoder_input.size(1), d_model)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            next_token_logits = self.output_projection(decoder_output[:, -1, :])    # 取最后一个时间步的输出，shape: (batch_size, d_model)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # 下一个标记的 logits，shape: (batch_size, decoder_vocab_size)\n",
    "\n",
    "            # Append to decoder input\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)   # 更新解码器输入，shape: (batch_size, decoder_input.size(1) + 1)\n",
    "\n",
    "            # Check if all sequences have generated EOS token, 检查是否所有序列都生成了结束标记\n",
    "            if torch.all(next_token.squeeze() == eos_token):\n",
    "                break\n",
    "\n",
    "        # Remove BOS token from generated sequences\n",
    "        generated_sequences = decoder_input[:, 1:]  # 移除开始标记后的生成序列，shape: (batch_size, decoder_input.size(1) - 1)\n",
    "\n",
    "        return generated_sequences  # (batch_size, tgt_seq_len, decoder_vocab_size)\n",
    "\n",
    "# Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'd_model': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 2,\n",
    "    'num_decoder_layers': 2,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 100\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Transformer(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Transformer Model Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Model Dimension: {model_config['d_model']}\")\n",
    "print(f\"Number of Heads: {model_config['num_heads']}\")\n",
    "print(f\"Encoder Layers: {model_config['num_encoder_layers']}\")\n",
    "print(f\"Decoder Layers: {model_config['num_decoder_layers']}\")\n",
    "print(f\"Feed Forward Dimension: {model_config['d_ff']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "decoder_input = target_ids[:, :-1]  # Remove last token for teacher forcing\n",
    "outputs = model(source_ids, decoder_input) # Forward pass，shape: (batch_size, target_len - 1， decoder_vocab_size)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len-1, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=50)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0].tolist()[:20]}...\")  # Show first 20 tokens\n",
    "\n"
   ],
   "id": "5c8135cba413a52c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transformer Model Architecture ===\n",
      "Device: cuda\n",
      "Encoder Vocabulary Size: 30,522\n",
      "Decoder Vocabulary Size: 21,128\n",
      "Model Dimension: 512\n",
      "Number of Heads: 8\n",
      "Encoder Layers: 2\n",
      "Decoder Layers: 2\n",
      "Feed Forward Dimension: 2048\n",
      "Dropout Rate: 0.1\n",
      "\n",
      "Total Parameters: 51,996,296\n",
      "Trainable Parameters: 51,996,296\n",
      "Model Size: 198.35 MB\n",
      "\n",
      "=== Model Architecture ===\n",
      "Transformer(\n",
      "  (encoder_embedding): Embedding(30522, 512, padding_idx=0)\n",
      "  (decoder_embedding): Embedding(21128, 512, padding_idx=0)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerEncoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-1): 2 x TransformerDecoderLayer(\n",
      "      (self_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (cross_attention): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_projection): Linear(in_features=512, out_features=21128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "=== Testing Model Forward Pass ===\n",
      "Source shape: torch.Size([64, 100])\n",
      "Target shape: torch.Size([64, 100])\n",
      "Output shape: torch.Size([64, 99, 21128])\n",
      "Expected shape: [batch_size, target_len-1, decoder_vocab_size]\n",
      "Actual shape: [64, 99, 21128]\n",
      "\n",
      "=== Testing Model Generation ===\n",
      "Generated sequence shape: torch.Size([2, 50])\n",
      "Generated tokens (first sample): [15754, 19910, 19910, 19910, 5026, 18550, 11912, 11912, 11912, 11912, 11912, 11912, 11912, 11912, 11912, 11912, 18550, 18550, 11912, 5026]...\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
