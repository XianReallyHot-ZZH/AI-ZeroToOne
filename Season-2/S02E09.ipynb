{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coding: Machine Translation (RNN + Attention)\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 1m high-quality pairs of data\n",
    " - Model: Seq2seq with RNN + Attention\n",
    " - GPU: 1660TI"
   ],
   "id": "9daaa7facc2fa102"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:37:35.218233Z",
     "start_time": "2025-08-17T08:37:34.880965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以先测试网络连接\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\")\n",
    "    print(\"网络连接正常\")\n",
    "except:\n",
    "    print(\"网络连接可能存在问题\")"
   ],
   "id": "41f13ee766790829",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络连接正常\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:37:48.939253Z",
     "start_time": "2025-08-17T08:37:38.321975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\", cache_dir=r\"D:\\Developer\\LLM\\FuggingFace-cache-model\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    import re  # 添加这一行\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "\n",
    "# 选出最多500万条数据\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)"
   ],
   "id": "483aca00fb9b0e26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T09:01:15.928079Z",
     "start_time": "2025-08-17T09:00:51.456355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 继承Dataset构建英中文翻译数据集\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        # 对英文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        en_tokens = self.tokenizer_en(en_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 对中文文本进行tokenizer，截取max_length，不够会进行填充，返回pytorch张量\n",
    "        zh_tokens = self.tokenizer_zh(zh_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        # 返回数据结构\n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),     # 去除张量的多余维度，输出以为数字数组，文本对应的tokenizer后的数组\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,         # 原始文本\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "# 构建训练和验证数据集\n",
    "def create_dataloaders(dataset, batch_size=128, num_workers=0, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "\n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                train_size=train_size,\n",
    "                                                random_state=42)\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets， 得到tokenizer后的训练数据集和验证数据集\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create DataLoaders 创建了pytorch数据加载器，封装了数据集，并定义了batch_size，shuffle，num_workers，方便训练时设置数据加载相关的超参数\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "        break\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ],
   "id": "b9723f2d59d69810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for en: 30522\n",
      "Vocab size for zh: 21128\n",
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 8475 batches\n",
      "Validation DataLoader: 447 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 128\n",
      "Source example: 9 November 2005\n",
      "Source tokens: tensor([ 101, 1023, 2281, 2384,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Target example: 2005年11月9日\n",
      "Target tokens: tensor([ 101, 8232, 2399, 8111, 3299,  130, 3189,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 128\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens: tensor([  101,  6356,  1998,  1037,  1013,  3438,  1013, 19714,  1013,  5587,\n",
      "         1012,  1015,  1010, 11498,  1012,  1020,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens: tensor([  101,   113,   100,   120,  8183,   120, 11410,   120,   100,   119,\n",
      "          122,  8024,  5018,   127,  3667,   114,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T12:56:18.759986Z",
     "start_time": "2025-08-17T12:56:12.794227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Seq2Seq model with GRU + Attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Attention网络结构\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    QKV Attention mechanism for Seq2Seq models\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # QKV transformations，bias=False 在这里是为了简化注意力机制中的线性变换，减少不必要的参数，同时不会显著影响模型性能\n",
    "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)  # Query projection\n",
    "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)  # Key projection\n",
    "        self.W_v = nn.Linear(hidden_size, hidden_size, bias=False)  # Value projection\n",
    "\n",
    "    # Encoder-Decoder attention\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Compute attention weights and context vector\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs: [batch_size, seq_len, hidden_size]     编码器所有时间步的隐藏状态\n",
    "            decoder_hidden: [batch_size, hidden_size]   解码器当前时间步的隐藏状态\n",
    "\n",
    "        Returns:\n",
    "            context: [batch_size, hidden_size]  上下文向量 context 包含了编码器输出中与当前解码器状态最相关的信息（融合信息）\n",
    "            attention_weights: [batch_size, seq_len]    代表着当前时间步的解码器状态与编码器所有时间步的隐藏状态的相似度（概率值），每个权重表示对应位置的重要性\n",
    "        \"\"\"\n",
    "        # Q: Transform decoder hidden state 解码器当前时间步的隐藏状态的转化\n",
    "        Q = self.W_q(decoder_hidden.unsqueeze(1))  # [batch_size, hidden_size] 加维度 [batch_size, 1, hidden_size] 线性变化 [batch_size, 1, hidden_size]\n",
    "\n",
    "        # K, V: Transform encoder outputs 编码器所有时间步的隐藏状态的转化\n",
    "        K = self.W_k(encoder_outputs)  # [batch_size, seq_len, hidden_size] 线性变化 [batch_size, seq_len, hidden_size]\n",
    "        V = self.W_v(encoder_outputs)  # [batch_size, seq_len, hidden_size] 线性变化 [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        '''\n",
    "        Scaled dot-product attention（缩放点积注意力机制）\n",
    "            1、计算查询（Q）和键（K）之间的相似度得分\n",
    "            2、使用 softmax 将得分转换为权重\n",
    "            3、使用这些权重对值（V）进行加权求和\n",
    "        '''\n",
    "        # K.transpose(1, 2) [batch_size, hidden_size, seq_len]\n",
    "        # Q.bmm(K.transpose(1, 2)) # [batch_size, 1, seq_len] 注意力分数矩阵，代表着当前时间步的输入与编码器所有时间步的隐藏状态的相似度（分数值）\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.hidden_size)\n",
    "        # attention_weights: [batch_size, seq_len]  将注意力分数转换为概率分布（注意力权重），代表着当前时间步的输入与编码器所有时间步的隐藏状态的相似度（概率值），每个权重表示对应位置的重要性\n",
    "        attention_weights = F.softmax(scores.squeeze(1), dim=1)\n",
    "        # attention_weights.unsqueeze(1) 在第1维添加一个维度，变为 [batch_size, 1, seq_len]\n",
    "        # V  [batch_size, seq_len, hidden_size]\n",
    "        # torch.bmm(attention_weights.unsqueeze(1), V) 这一步实现了对编码器各个时间步的加权求和 [batch_size, 1, hidden_size]\n",
    "        # Context vector: 移除第1维(大小为1的维度)得到[batch_size, hidden_size]，生成的上下文向量 context 包含了编码器输出中与当前解码器状态最相关的信息\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), V).squeeze(1)\n",
    "\n",
    "        return context, attention_weights\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder component of the Seq2Seq model using GRU\n",
    "    Processes the input sequence and generates context vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # 编码层网络超参数\n",
    "        self.vocab_size = vocab_size        # 这个是固定的，字典的大小，和具体的语言和具体的tokenizer实现有关\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer to convert token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # GRU layer for processing sequences\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Args:\n",
    "            input_seq: Input token sequences [batch_size, seq_len]\n",
    "            input_lengths: Actual lengths of sequences (for packed sequences)\n",
    "\n",
    "        Returns:\n",
    "            outputs: All hidden states [batch_size, seq_len, hidden_size]\n",
    "            hidden: Final hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert token IDs to embeddings\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n",
    "\n",
    "        # Pass through GRU\n",
    "        # outputs: All hidden states [batch_size, seq_len, hidden_size]\n",
    "        # hidden: Final hidden state [num_layers, batch_size, hidden_size]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        # outputs: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "        return outputs, hidden\n",
    "\n",
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder component of the Seq2Seq model using GRU with Attention\n",
    "    Generates output sequence one token at a time\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # 解码器网络超参数\n",
    "        self.vocab_size = vocab_size        # 这个是固定的，字典的大小，和具体的语言和具体的tokenizer实现有关\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer for target tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "        # GRU layer for generating sequences (input: embedding + context)\n",
    "        # 解码器中的每一层rnn的输入是embedding和context的拼接\n",
    "        self.rnn = nn.GRU(embed_size + hidden_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "        # Output projection layer to vocabulary (input: GRU output)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    # 解码器中的前向计算中的每一步\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder with attention (single step)\n",
    "        TODO: the logic of this function is inconsistent with the forward_seq function, need to fix it\n",
    "\n",
    "        Args:\n",
    "            input_token: Current input token [batch_size, 1]  单词\n",
    "            hidden: Hidden state from encoder/previous step [num_layers, batch_size, hidden_size]\n",
    "            encoder_outputs: All encoder outputs [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "            output: Vocabulary predictions [batch_size, vocab_size]\n",
    "            hidden: Updated hidden state [num_layers, batch_size, hidden_size]\n",
    "            attention_weights: Attention weights [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # input_token: [batch_size, 1]\n",
    "        embedded = self.embedding(input_token)  # [batch_size, 1, embed_size]\n",
    "\n",
    "        # Use the top layer hidden state for attention computation，获取输入的hidden中最后一层的隐藏状态\n",
    "        decoder_hidden_for_attention = hidden[-1]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Compute attention context\n",
    "        context, attention_weights = self.attention(encoder_outputs, decoder_hidden_for_attention)\n",
    "        # context: [batch_size, hidden_size]\n",
    "        # attention_weights: [batch_size, seq_len]\n",
    "\n",
    "        # Concatenate embedding with context vector\n",
    "        rnn_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
    "        # rnn_input: [batch_size, 1, embed_size + hidden_size]\n",
    "\n",
    "        # Pass through GRU\n",
    "        gru_out, hidden = self.rnn(rnn_input, hidden)\n",
    "        # gru_out: [batch_size, 1, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Concatenate GRU output with context for final projection\n",
    "        output_input = gru_out.squeeze(1)\n",
    "        # output_input: [batch_size, hidden_size]\n",
    "\n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(output_input)  # [batch_size, vocab_size]\n",
    "\n",
    "        # output：[batch_size, vocab_size] 预测的下一个单词的概率\n",
    "        # hidden：[num_layers, batch_size, hidden_size] 隐藏状态，下一个时间步的隐藏状态输入\n",
    "        # attention_weights：[batch_size, seq_len]，代表着当前时间步的解码器状态与编码器所有时间步的隐藏状态的相似度（概率值），每个权重表示对应位置的重要性\n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    # 这个是针对训练优化的前向计算流程，目的是为了加速训练，原本一个单词一个单词预测的方式太慢了，将串行改为并行处理，加速训练\n",
    "    def forward_seq(self, input_seq, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        ⚡ FAST Sequence forward for training with attention (teacher forcing)\n",
    "        Vectorized implementation without loops for parallel processing\n",
    "\n",
    "        Args:\n",
    "            input_seq: Target sequence [batch_size, seq_len]\n",
    "            hidden: Initial hidden state [num_layers, batch_size, hidden_size]\n",
    "            encoder_outputs: All encoder hidden states [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "            outputs: Vocabulary predictions [batch_size, seq_len, vocab_size]\n",
    "            final_hidden: Final hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_seq.shape\n",
    "\n",
    "        # Embed the entire sequence\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n",
    "\n",
    "        # Step 1: Get all decoder hidden states by running GRU with encoder context\n",
    "        encoder_hidden_context = hidden[-1].unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, hidden_size]\n",
    "        temp_input = torch.cat([embedded, encoder_hidden_context], dim=2)  # [batch_size, seq_len, embed_size + hidden_size]\n",
    "        temp_gru_out, temp_hidden = self.rnn(temp_input, hidden)\n",
    "        # temp_gru_out: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Step 2: Extract decoder hidden states for attention computation\n",
    "        # For multi-layer GRU, we use the top layer outputs for attention\n",
    "        decoder_hiddens = temp_gru_out  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Step 3: Compute attention contexts for all timesteps in parallel\n",
    "        # Reshape to process all timesteps at once\n",
    "        decoder_hiddens_reshaped = decoder_hiddens.reshape(-1, self.hidden_size)  # [batch_size * seq_len, hidden_size]\n",
    "        encoder_outputs_expanded = encoder_outputs.unsqueeze(1).expand(-1, seq_len, -1, -1).contiguous()\n",
    "        encoder_outputs_reshaped = encoder_outputs_expanded.reshape(-1, encoder_outputs.size(1), self.hidden_size)\n",
    "        # encoder_outputs_reshaped: [batch_size * seq_len, source_len, hidden_size]\n",
    "\n",
    "        # Compute attention for all decoder steps at once\n",
    "        contexts, _ = self.attention(encoder_outputs_reshaped, decoder_hiddens_reshaped)\n",
    "        # contexts: [batch_size * seq_len, hidden_size]\n",
    "\n",
    "        # Reshape contexts back\n",
    "        contexts = contexts.reshape(batch_size, seq_len, self.hidden_size)  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Step 4: Concatenate embeddings with contexts and recompute GRU\n",
    "        enhanced_input = torch.cat([embedded, contexts], dim=2)  # [batch_size, seq_len, embed_size + hidden_size]\n",
    "\n",
    "        # Final GRU pass with attention-enhanced input\n",
    "        gru_out, final_hidden = self.rnn(enhanced_input, hidden)\n",
    "        # gru_out: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Step 5: Project to vocabulary for all timesteps\n",
    "        outputs = self.output_projection(gru_out)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # outputs：[batch_size, seq_len, vocab_size]，预测字段序列结果\n",
    "        # final_hidden：[num_layers, batch_size, hidden_size] ，最终的隐藏状态\n",
    "        return outputs, final_hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Sequence-to-Sequence model using GRU\n",
    "    Combines Encoder and Decoder for translation\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, embed_size=512,\n",
    "                 hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = Encoder(encoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = Decoder(decoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "\n",
    "    # 模型训练前向计算实现\n",
    "    def forward(self, source_seq, target_seq):\n",
    "        \"\"\"\n",
    "        ⚡ Training forward with attention mechanism\n",
    "\n",
    "        Args:\n",
    "            source_seq: Source sequence [batch_size, seq_len]\n",
    "            target_seq: Target sequence [batch_size, target_len]\n",
    "\n",
    "        Returns:\n",
    "            outputs: Vocabulary predictions [batch_size, target_len, decoder_vocab_size]\n",
    "        \"\"\"\n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden = self.encoder(source_seq)\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Decode with attention\n",
    "        outputs, _ = self.decoder.forward_seq(target_seq, hidden, encoder_outputs)\n",
    "        # outputs: [batch_size, seq_len, output_size]，目标序列的预测结果\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # 预测生成（根据翻译的输入字段序列生成翻译结果字段序列）\n",
    "    def generate(self, source_seq, max_length=100, start_token=101, end_token=102):\n",
    "        \"\"\"\n",
    "        Generate translation for given source sequence with attention (inference mode)\n",
    "\n",
    "        Args:\n",
    "            source_seq: Source sequence [batch_size, source_len]\n",
    "            max_length: Maximum length of generated sequence\n",
    "            start_token: BOS token ID (101 for BERT)\n",
    "            end_token: EOS token ID (102 for BERT)\n",
    "\n",
    "        Returns:\n",
    "            generated_seq: Generated sequence [batch_size, generated_len]\n",
    "            attention_weights: Attention weights for each generated token [batch_size, generated_len, source_len]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        batch_size = source_seq.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode source sequence\n",
    "            encoder_outputs, hidden = self.encoder(source_seq)\n",
    "            # encoder_outputs: [batch_size, source_len, hidden_size]\n",
    "\n",
    "            # Initialize with start token: [batch_size, 1] 生成预测的第一个输入单词，即BOS token ID (101 for BERT)\n",
    "            decoder_input = torch.full((batch_size, 1), start_token, dtype=torch.long).to(source_seq.device)\n",
    "\n",
    "            # Store generated tokens and attention weights\n",
    "            generated_tokens = []\n",
    "            attention_weights_list = []\n",
    "\n",
    "            # 一个单词一个单词往下预测\n",
    "            for _ in range(max_length):\n",
    "                # Get next token prediction with attention\n",
    "                output, hidden, attention_weights = self.decoder(decoder_input, hidden, encoder_outputs)\n",
    "                # output: [batch_size, vocab_size]\n",
    "                # hidden: [num_layers, batch_size, hidden_size]\n",
    "                # attention_weights: [batch_size, seq_len]\n",
    "\n",
    "                # Store attention weights\n",
    "                attention_weights_list.append(attention_weights.unsqueeze(1))  # [batch_size, 1, source_len]\n",
    "\n",
    "                # Get the token with highest probability\n",
    "                '''\n",
    "                argmax 在数学上对 logits 和概率分布结果相同\n",
    "                    如果 z1 > z2 > z3\n",
    "                   那么 softmax(z1) > softmax(z2) > softmax(z3)\n",
    "                   所以 argmax([z1, z2, z3]) = argmax([softmax(z1), softmax(z2), softmax(z3)])\n",
    "                '''\n",
    "                next_token = output.argmax(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
    "                generated_tokens.append(next_token)\n",
    "\n",
    "                # Use predicted token as next input\n",
    "                decoder_input = next_token\n",
    "\n",
    "                # Stop if all sequences generated EOS token\n",
    "                '''\n",
    "                1.next_token.squeeze():\n",
    "                    next_token 的形状是 [batch_size, 1]\n",
    "                    squeeze() 移除大小为1的维度，变成 [batch_size]\n",
    "                2.next_token.squeeze() == end_token:\n",
    "                    将每个批次中生成的token与结束符(end_token)进行比较\n",
    "                    返回一个布尔张量，形状为 [batch_size]\n",
    "                    如果某个样本生成了结束符，对应位置为 True，否则为 False\n",
    "                3.torch.all(...):\n",
    "                    检查所有元素是否都为 True\n",
    "                    只有当批次中所有样本都生成了结束符时，才返回 True\n",
    "                4.break:\n",
    "                    如果条件满足，跳出循环，停止生成\n",
    "                '''\n",
    "                if torch.all(next_token.squeeze() == end_token):\n",
    "                    break\n",
    "\n",
    "            # Concatenate all generated tokens and attention weights\n",
    "            generated_seq = torch.cat(generated_tokens, dim=1)   # [batch_size, seq_len]\n",
    "            all_attention_weights = torch.cat(attention_weights_list, dim=1)  # [batch_size, generated_len, source_len]\n",
    "\n",
    "        return generated_seq, all_attention_weights\n",
    "\n",
    "# Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'embed_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Seq2Seq(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Seq2Seq Model with GRU Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Embedding Size: {model_config['embed_size']}\")\n",
    "print(f\"Hidden Size: {model_config['hidden_size']}\")\n",
    "print(f\"Number of Layers: {model_config['num_layers']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "print(f\"RNN Type: GRU (Gated Recurrent Unit)\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "outputs = model(source_ids, target_ids)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=100)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated[0].shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0][0].tolist()}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2cc07047b429b19d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Seq2Seq Model with GRU Architecture ===\n",
      "Device: cuda\n",
      "Encoder Vocabulary Size: 30,522\n",
      "Decoder Vocabulary Size: 21,128\n",
      "Embedding Size: 128\n",
      "Hidden Size: 256\n",
      "Number of Layers: 2\n",
      "Dropout Rate: 0.1\n",
      "RNN Type: GRU (Gated Recurrent Unit)\n",
      "\n",
      "Total Parameters: 13,816,712\n",
      "Trainable Parameters: 13,816,712\n",
      "Model Size: 52.71 MB\n",
      "\n",
      "=== Model Architecture ===\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(30522, 128, padding_idx=0)\n",
      "    (rnn): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(21128, 128, padding_idx=0)\n",
      "    (attention): Attention(\n",
      "      (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (W_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "    )\n",
      "    (rnn): GRU(384, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (output_projection): Linear(in_features=256, out_features=21128, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Testing Model Forward Pass ===\n",
      "Source shape: torch.Size([128, 100])\n",
      "Target shape: torch.Size([128, 100])\n",
      "Output shape: torch.Size([128, 100, 21128])\n",
      "Expected shape: [batch_size, target_len, decoder_vocab_size]\n",
      "Actual shape: [128, 100, 21128]\n",
      "\n",
      "=== Testing Model Generation ===\n",
      "Generated sequence shape: torch.Size([2, 100])\n",
      "Generated tokens (first sample): [18400, 7099, 16656, 16656, 17460, 20841, 17897, 19082, 19082, 8106, 5609, 15650, 17061, 4711, 18711, 18711, 11869, 11869, 15638, 12935, 12935, 8707, 1770, 4570, 4570, 15558, 1081, 3381, 571, 11157, 11157, 11215, 15774, 7882, 16486, 16486, 16486, 19728, 18488, 18488, 18488, 18488, 20786, 20786, 2369, 2369, 15064, 15064, 16546, 16546, 14552, 16775, 9227, 15944, 1312, 1312, 84, 16189, 3724, 18155, 1604, 9453, 12389, 12389, 1394, 15741, 5728, 5728, 15931, 15931, 18400, 6400, 13758, 13758, 17371, 17371, 13518, 20128, 20128, 1333, 276, 276, 276, 12080, 12080, 48, 14682, 8009, 8009, 8009, 13827, 13827, 15260, 1563, 10123, 10123, 9703, 9703, 13092, 18651]\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
