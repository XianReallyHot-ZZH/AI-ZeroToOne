{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coding: Machine Translation by RNN\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 5m high-quality pairs of data\n",
    " - Model: Seq2seq with Encoder & Decoder framework\n",
    " - GPU: 1660TI"
   ],
   "id": "c43b2903b576698e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Seq2Seq Encoder-Decoder Architecture\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Sequence-to-Sequence (Seq2Seq) model with Encoder-Decoder architecture is a neural network framework designed for tasks where both input and output are sequences of variable length, such as machine translation (English → Chinese in your case).\n",
    "\n",
    "```\n",
    "Input Sequence (English):  \"Hello world\"\n",
    "                              ↓\n",
    "                          [ENCODER]\n",
    "                              ↓\n",
    "                        Context Vector\n",
    "                              ↓\n",
    "                          [DECODER]\n",
    "                              ↓\n",
    "Output Sequence (Chinese): \"你好世界\"\n",
    "```\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "The encoder processes the input sequence and compresses the information into a fixed-size context vector (also called thought vector).\n",
    "\n",
    "```\n",
    "Input: [w1, w2, w3, ..., wn]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [RNN][RNN][RNN]...[RNN]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [h1] [h2] [h3] ... [hn] → Context Vector (hn)\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Embedding Layer**: Converts input tokens to dense vectors\n",
    "- **RNN Layers**: LSTM/GRU cells process the sequence sequentially\n",
    "- **Hidden States**: Capture information at each time step\n",
    "- **Final Context**: Last hidden state becomes the context vector\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # Context vectors\n",
    "```\n",
    "\n",
    "### 2. Decoder\n",
    "\n",
    "The decoder generates the output sequence one token at a time, using the context vector from the encoder.\n",
    "\n",
    "```\n",
    "Context Vector (C) → [RNN] → [RNN] → [RNN] → ... → [RNN]\n",
    "                      ↓       ↓       ↓             ↓\n",
    "                    [y1]    [y2]    [y3]   ...   [yn]\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Initial State**: Initialized with encoder's context vector\n",
    "- **RNN Layers**: Generate hidden states for each output position\n",
    "- **Output Projection**: Maps hidden states to vocabulary probabilities\n",
    "- **Softmax**: Converts logits to probability distribution\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, target_seq, encoder_hidden, encoder_cell):\n",
    "        embedded = self.embedding(target_seq)\n",
    "        outputs, _ = self.rnn(embedded, encoder_hidden)\n",
    "        predictions = self.output_projection(outputs)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "## Complete Architecture Flow\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "```\n",
    "1. Input Processing:\n",
    "   English: \"Hello world\" → [101, 7592, 2088, 102] (tokenized)\n",
    "   Chinese: \"[BOS] 你好世界 [EOS]\" → [101, 872, 1962, 686, 102] (tokenized)\n",
    "\n",
    "2. Encoder Forward Pass:\n",
    "   Input: [101, 7592, 2088, 102]\n",
    "   ↓\n",
    "   Embedding: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]\n",
    "   ↓\n",
    "   LSTM/GRU: h1, h2, h3, h4 → Context Vector (h4)\n",
    "\n",
    "3. Decoder Forward Pass:\n",
    "   Initial State: (h4) from encoder\n",
    "   Input: [101, 872, 1962, 686]\n",
    "   ↓\n",
    "   LSTM/GRU: generates hidden states for each position\n",
    "   ↓\n",
    "   Output Projection: [vocab_size] logits for each position\n",
    "   ↓\n",
    "   Loss Calculation: CrossEntropy with targets [872, 1962, 686, 102]\n",
    "```\n",
    "\n",
    "### Inference Phase\n",
    "\n",
    "```\n",
    "1. Encode input sequence: \"Hello world\"\n",
    "2. Initialize decoder with encoder's context vector\n",
    "3. Start with [BOS] token\n",
    "4. Generate tokens one by one:\n",
    "   - Input: [BOS] → Output: 你 (probability distribution)\n",
    "   - Input: [BOS] 你 → Output: 好\n",
    "   - Input: [BOS] 你 好 → Output: 世\n",
    "   - Input: [BOS] 你 好 世 → Output: 界\n",
    "   - Input: [BOS] 你 好 世 界 → Output: [EOS] (stop)\n",
    "```\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Encoder\n",
    "```\n",
    "h_t = LSTM/GRU(embedding(x_t), h_{t-1})\n",
    "context = h_n  # Final hidden state\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "```\n",
    "s_t = LSTM/GRU(embedding(y_{t-1}), s_{t-1})  # s_0 = context\n",
    "P(y_t | y_1...y_{t-1}, x) = softmax(W_s * s_t + b_s)\n",
    "```\n",
    "\n",
    "### Loss Function\n",
    "```\n",
    "Loss = -∑∑ log P(y_t^i | y_1^i...y_{t-1}^i, x^i)\n",
    "```\n",
    "\n",
    "## Architecture Advantages\n",
    "\n",
    "1. **Variable Length Handling**: Can process sequences of different lengths\n",
    "2. **End-to-End Learning**: Jointly optimizes encoder and decoder\n",
    "3. **Context Preservation**: Encoder captures semantic meaning in context vector\n",
    "4. **Language Agnostic**: Works for any language pair\n",
    "\n",
    "## Architecture Limitations\n",
    "\n",
    "1. **Information Bottleneck**: Fixed-size context vector may lose information\n",
    "2. **Long Sequence Problem**: Difficulty with very long input sequences\n",
    "3. **Sequential Processing**: Cannot parallelize during inference\n",
    "\n",
    "## Improvements & Variants\n",
    "\n",
    "1. **Attention Mechanism**: Allows decoder to focus on relevant encoder states\n",
    "2. **Bidirectional Encoder**: Processes sequence in both directions\n",
    "3. **Beam Search**: Better decoding strategy than greedy search\n",
    "4. **Teacher Forcing**: Training technique using ground truth as decoder input\n",
    "\n",
    "## Implementation Architecture for Your Project\n",
    "\n",
    "Based on your dataset (WMT-17 EN-ZH) and tokenizers (BERT-based), here's the recommended architecture:\n",
    "\n",
    "```\n",
    "Input: English sentence (max_length=100)\n",
    "↓\n",
    "BERT Tokenizer (vocab_size=30522) → Token IDs\n",
    "↓\n",
    "Embedding Layer (30522 → 512)\n",
    "↓\n",
    "Encoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Context Vector (1024-dim)\n",
    "↓\n",
    "Decoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Output Projection (1024 → 21128)\n",
    "↓\n",
    "Chinese Token IDs → BERT Tokenizer → Chinese sentence\n",
    "```"
   ],
   "id": "de8eaec2f3bd079c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:19:19.530357Z",
     "start_time": "2025-08-10T13:19:19.277047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 可以先测试网络连接\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\")\n",
    "    print(\"网络连接正常\")\n",
    "except:\n",
    "    print(\"网络连接可能存在问题\")"
   ],
   "id": "553a01635e46508f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络连接正常\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:19:32.199261Z",
     "start_time": "2025-08-10T13:19:21.427303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\", cache_dir=r\"D:\\Developer\\LLM\\FuggingFace-cache-model\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    import re  # 添加这一行\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)\n"
   ],
   "id": "f981b4eb6b850121",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Size:  25134743\n",
      "Filtered Dataset Size:  1141860\n",
      "Dataset Size:  1141860\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Zambia (7)\n",
      "赞比亚(7)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15:00 to 18:00 Informal consultations (closed) Conference Room 5 (NLB)\n",
      "下午3:00－6:00 非正式磋商(闭门会议) 第5会议室(北草坪会议大楼)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spain\n",
      "西班牙\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mr. Robert Morrison\n",
      "Robert Morrison先生 加拿大自然资源部\n",
      "----------------------------------------------------------------------------------------------------\n",
      "This satisfied the kids, but not the husband.\n",
      "\"孩子们得到了满意的答案, 但她的丈夫却没有。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shutaro Omura (Japan)\n",
      "Shutaro Omura（日本）\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\"Oh,\" she says, \"what am I THINKING about!\n",
      "——”    “哦，”她说，“你看我在想些什么啊！\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30 June 2005\n",
      "2005年6月30日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11 July-5 August 2005\n",
      "2005年7月11日-8月5日\n",
      "----------------------------------------------------------------------------------------------------\n",
      "May 2011\n",
      "2011年5月\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T13:19:46.886846Z",
     "start_time": "2025-08-10T13:19:36.990730Z"
    }
   },
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 自定义数据集（对原始数据集进行自定义处理，进行tokenizer）\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        en_tokens = self.tokenizer_en(en_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        zh_tokens = self.tokenizer_zh(zh_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),     # 去除张量的多余维度，输出以为数字数组\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "# 制作训练数据集Loader和验证数据集Loader\n",
    "def create_dataloaders(dataset, batch_size=64, num_workers=0, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "\n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                train_size=train_size,\n",
    "                                                random_state=42)\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        # pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # for batch in dataloader:\n",
    "    #     print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "    #     print(f\"Source example: {batch['source_text'][0]}\")\n",
    "    #     print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "    #     print(f\"Target example: {batch['target_text'][0]}\")\n",
    "    #     print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "    #     break\n",
    "\n",
    "    # 使用next和iter更直接地获取一个batch\n",
    "    try:\n",
    "        batch = next(iter(dataloader))\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens shape: {batch['source_ids'][0].shape}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens shape: {batch['target_ids'][0].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting batch: {e}\")\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for en: 30522\n",
      "Vocab size for zh: 21128\n",
      "Train dataset size: 1084767\n",
      "Validation dataset size: 57093\n",
      "Train DataLoader: 16950 batches\n",
      "Validation DataLoader: 893 batches\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 64\n",
      "Source example: Miss Graziella Dubra\n",
      "Source tokens shape: torch.Size([100])\n",
      "Target example: Graziella Dubra小姐\n",
      "Target tokens shape: torch.Size([100])\n",
      "\n",
      "==================================================\n",
      "Sample batch from DataLoader:\n",
      "==================================================\n",
      "Batch size: 64\n",
      "Source example: 74 and A/60/640/Add.1, para. 6\n",
      "Source tokens shape: torch.Size([100])\n",
      "Target example: (A/60/640/Add.1，第6段)\n",
      "Target tokens shape: torch.Size([100])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:38:47.531373Z",
     "start_time": "2025-08-10T13:38:46.755018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Seq2Seq model with GRU\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder component of the Seq2Seq model using GRU\n",
    "    Processes the input sequence and generates context vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer to convert token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # GRU layer for processing sequences\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "    # 输入的是语句token序列（固定长度，制作数据集的时候已经处理成固定长度的token序列了），输出的是所有隐藏状态和最终隐藏状态\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder\n",
    "\n",
    "        Args:\n",
    "            input_seq: Input token sequences [batch_size, seq_len]\n",
    "            input_lengths: Actual lengths of sequences (for packed sequences)\n",
    "\n",
    "        Returns:\n",
    "            outputs: All hidden states [batch_size, seq_len, hidden_size]\n",
    "            hidden: Final hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # Convert token IDs to embeddings\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n",
    "\n",
    "        # Pass through GRU\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        # outputs: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder component of the Seq2Seq model using GRU\n",
    "    Generates output sequence one token at a time\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512, hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer for target tokens\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "        # GRU layer for generating sequences\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers,\n",
    "                         batch_first=True, dropout=dropout, bidirectional=False)\n",
    "\n",
    "        # Output projection layer to vocabulary\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    # 输入一个单词，输出vocab_size大小的概率（最终去确定是哪个单词）\n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder (single step)\n",
    "\n",
    "        Args:\n",
    "            input_token: Current input token [batch_size, 1]  输入为一个单词，输出也为一个单词\n",
    "            hidden: Hidden state from encoder/previous step [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        Returns:\n",
    "            output: Vocabulary predictions [batch_size, vocab_size]\n",
    "            hidden: Updated hidden state [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # input_token: [batch_size, 1]\n",
    "        embedded = self.embedding(input_token)  # [batch_size, 1, embed_size]\n",
    "\n",
    "        # Pass through GRU\n",
    "        gru_out, hidden = self.rnn(embedded, hidden)\n",
    "        # gru_out: [batch_size, 1, hidden_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(gru_out.squeeze(1))  # [batch_size, hidden_size] -> [batch_size, vocab_size]\n",
    "\n",
    "        # output: [batch_size, vocab_size]\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "        return output, hidden\n",
    "\n",
    "    def forward_parallel(self, input_seq, hidden):\n",
    "        \"\"\"\n",
    "        ⚡ NEW - Parallel forward for training (teacher forcing)\n",
    "        Process entire sequence at once for 10-50x speedup during training\n",
    "        \"\"\"\n",
    "        # input_sequence: [batch_size, seq_len]\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embed_size]\n",
    "\n",
    "        # Process entire sequence in parallel,这里会进行优化，但是单词预测的时候还是会走单步预测，其他阶段会进行底层优化并行\n",
    "        gru_out, final_hidden = self.rnn(embedded, hidden)\n",
    "        # gru_out: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Project to vocabulary for all timesteps\n",
    "        outputs = self.output_projection(gru_out)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # outputs : [batch_size, seq_len, vocab_size]\n",
    "        # final_hidden: [num_layers, batch_size, hidden_size]\n",
    "        return outputs, final_hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Sequence-to-Sequence model using GRU\n",
    "    Combines Encoder and Decoder for translation\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_vocab_size, decoder_vocab_size, embed_size=512,\n",
    "                 hidden_size=1024, num_layers=2, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder_vocab_size = encoder_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize encoder and decoder\n",
    "        self.encoder = Encoder(encoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = Decoder(decoder_vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "\n",
    "    # 根据输入的源序列和目标序列生成每个token的预测序列\n",
    "    def forward(self, source_seq, target_seq):\n",
    "        \"\"\"\n",
    "        ⚡ OPTIMIZED - Fast training forward with parallel processing\n",
    "        Uses teacher forcing + parallel decoder processing\n",
    "        \"\"\"\n",
    "\n",
    "        # hidden: [num_layers, batch_size, hidden_size]\n",
    "        _, hidden = self.encoder(source_seq)\n",
    "\n",
    "        # output: [batch_size, seq_len, vocab_size]\n",
    "        outputs, _ = self.decoder.forward_parallel(target_seq, hidden)\n",
    "\n",
    "        # output: [batch_size, seq_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    # 根据输入的源序列生成目标序列\n",
    "    def generate(self, source_seq, max_length=100, start_token=101, end_token=102):\n",
    "        \"\"\"\n",
    "        Generate translation for given source sequence (inference mode)\n",
    "\n",
    "        Args:\n",
    "            source_seq: Source sequence [batch_size, source_len]\n",
    "            max_length: Maximum length of generated sequence\n",
    "            start_token: BOS token ID (101 for BERT)\n",
    "            end_token: EOS token ID (102 for BERT)\n",
    "\n",
    "        Returns:\n",
    "            generated_seq: Generated sequence [batch_size, generated_len]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        batch_size = source_seq.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode source sequence\n",
    "            _, hidden = self.encoder(source_seq)\n",
    "\n",
    "            # Initialize with start token\n",
    "            # decoder_input: [batch_size, 1]\n",
    "            decoder_input = torch.full((batch_size, 1), start_token, dtype=torch.long).to(source_seq.device)\n",
    "\n",
    "            # Store generated tokens\n",
    "            generated_tokens = []\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                # Get next token prediction\n",
    "                # output: [batch_size, 1, vocab_size]\n",
    "                output, hidden = self.decoder(decoder_input, hidden)\n",
    "\n",
    "                # Get the token with highest probability\n",
    "                # next_token: [batch_size, 1]\n",
    "                next_token = output.argmax(dim=1).unsqueeze(1)\n",
    "                # generated_tokens: [batch_size, i+1]\n",
    "                generated_tokens.append(next_token)\n",
    "\n",
    "                # Use predicted token as next input\n",
    "                decoder_input = next_token\n",
    "\n",
    "                # Stop if all sequences generated EOS token\n",
    "                if torch.all(next_token.squeeze() == end_token):\n",
    "                    break\n",
    "\n",
    "            # Concatenate all generated tokens\n",
    "            # generated_seq: [batch_size, 1],输出预测的序列\n",
    "            generated_seq = torch.cat(generated_tokens, dim=1)\n",
    "\n",
    "        return generated_seq\n",
    "\n",
    "    # Model configuration based on your dataset\n",
    "model_config = {\n",
    "    'encoder_vocab_size': encoder_vocab_size,  # 30522 (English BERT)\n",
    "    'decoder_vocab_size': decoder_vocab_size,  # 21128 (Chinese BERT)\n",
    "    'embed_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = Seq2Seq(**model_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Seq2Seq Model with GRU Architecture ===\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Encoder Vocabulary Size: {model_config['encoder_vocab_size']:,}\")\n",
    "print(f\"Decoder Vocabulary Size: {model_config['decoder_vocab_size']:,}\")\n",
    "print(f\"Embedding Size: {model_config['embed_size']}\")\n",
    "print(f\"Hidden Size: {model_config['hidden_size']}\")\n",
    "print(f\"Number of Layers: {model_config['num_layers']}\")\n",
    "print(f\"Dropout Rate: {model_config['dropout']}\")\n",
    "print(f\"RNN Type: GRU (Gated Recurrent Unit)\")\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"\\n=== Model Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Test the model with a sample batch\n",
    "print(f\"\\n=== Testing Model Forward Pass ===\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "source_ids = sample_batch['source_ids'].to(device)\n",
    "target_ids = sample_batch['target_ids'].to(device)\n",
    "\n",
    "print(f\"Source shape: {source_ids.shape}\")\n",
    "print(f\"Target shape: {target_ids.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.train()\n",
    "outputs = model(source_ids, target_ids)\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"Expected shape: [batch_size, target_len, decoder_vocab_size]\")\n",
    "print(f\"Actual shape: [{outputs.shape[0]}, {outputs.shape[1]}, {outputs.shape[2]}]\")        # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "# Test generation\n",
    "print(f\"\\n=== Testing Model Generation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(source_ids[:2], max_length=100)  # Generate for first 2 samples\n",
    "    print(f\"Generated sequence shape: {generated.shape}\")\n",
    "    print(f\"Generated tokens (first sample): {generated[0].tolist()}\")"
   ],
   "id": "89647506c8cf2b56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Seq2Seq Model with GRU Architecture ===\n",
      "Device: cuda\n",
      "Encoder Vocabulary Size: 30,522\n",
      "Decoder Vocabulary Size: 21,128\n",
      "Embedding Size: 128\n",
      "Hidden Size: 256\n",
      "Number of Layers: 2\n",
      "Dropout Rate: 0.1\n",
      "RNN Type: GRU (Gated Recurrent Unit)\n",
      "\n",
      "Total Parameters: 13,423,496\n",
      "Trainable Parameters: 13,423,496\n",
      "Model Size: 51.21 MB\n",
      "\n",
      "=== Model Architecture ===\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(30522, 128, padding_idx=0)\n",
      "    (rnn): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(21128, 128, padding_idx=0)\n",
      "    (rnn): GRU(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (output_projection): Linear(in_features=256, out_features=21128, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Testing Model Forward Pass ===\n",
      "Source shape: torch.Size([64, 100])\n",
      "Target shape: torch.Size([64, 100])\n",
      "Output shape: torch.Size([64, 100, 21128])\n",
      "Expected shape: [batch_size, target_len, decoder_vocab_size]\n",
      "Actual shape: [64, 100, 21128]\n",
      "\n",
      "=== Testing Model Generation ===\n",
      "Generated sequence shape: torch.Size([2, 100])\n",
      "Generated tokens (first sample): [3812, 3812, 15099, 9529, 9529, 12098, 12098, 14260, 12796, 12796, 3812, 13439, 17966, 14221, 20471, 5199, 8978, 4870, 2139, 5848, 12702, 12702, 15482, 4168, 4168, 5458, 12558, 12597, 19966, 1366, 1366, 1366, 8486, 1366, 1366, 7106, 6670, 13272, 4550, 12474, 15240, 6627, 4550, 15359, 16677, 10616, 10616, 12712, 16745, 11213, 695, 2062, 11096, 6171, 15821, 15821, 943, 2213, 4195, 10119, 19431, 17492, 19431, 17492, 4, 1656, 19525, 21115, 1087, 2337, 5173, 5173, 5792, 5792, 4870, 10598, 10598, 13977, 8978, 15627, 15627, 3080, 3080, 11350, 18747, 3884, 15424, 15424, 12098, 12098, 12796, 12796, 14513, 14513, 17700, 3763, 3763, 10035, 10035, 278]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model with comprehensive training loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for Seq2Seq machine translation model\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, device,\n",
    "                 learning_rate=1e-3, weight_decay=1e-5):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "\n",
    "        # Loss function - ignore padding tokens (index 0)，忽略填充值（索引为0）的损失计算，对所有元素取平均\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "\n",
    "        # Optimizer\n",
    "        \"\"\"\n",
    "        # 标准梯度下降更新公式：\n",
    "        # w_new = w_old - learning_rate * gradient\n",
    "\n",
    "        # 使用 weight_decay (weight_decay=1e-5)\n",
    "        # 带L2正则化的梯度下降更新公式：\n",
    "        w_new = w_old - learning_rate * (gradient + weight_decay * w_old)\n",
    "\n",
    "        w大的会减少的更多\n",
    "        weight_decay 通过在每次更新时添加一个与权重大小成正比的衰减项，以及一个乘法衰减因子，有效地防止了权重变得过大，从而约束权重在合理范围内。\n",
    "        \"\"\"\n",
    "        self.optimizer = optim.Adam(model.parameters(),\n",
    "                                  lr=learning_rate,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "        # Learning rate scheduler，当监测指标停止改善时降低学习率\n",
    "        # mode='min' 的工作逻辑：\n",
    "        # 1. 持续监测验证损失\n",
    "        # 2. 如果验证损失连续 'patience' 个epoch没有减少\n",
    "        # 3. 则将学习率乘以 'factor' (通常是减小学习率)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            # mode='min'：监测指标越小越好\n",
    "            # factor=0.5：每次学习率减少时，将学习率乘以0.5\n",
    "            # patience=2：patience为2，表示在2个epoch内没有改善时，学习率将减少\n",
    "            self.optimizer, mode='min', factor=0.5, patience=2,\n",
    "        )\n",
    "\n",
    "        # Training history，创建训练历史记录字典，用于存储训练过程中的各种指标\n",
    "        self.history = defaultdict(list)\n",
    "\n",
    "    def calculate_loss(self, outputs, targets, target_lengths=None):\n",
    "        \"\"\"Calculate loss for the batch\"\"\"\n",
    "        # Reshape for loss calculation (use reshape instead of view for non-contiguous tensors)\n",
    "        # outputs_flat：将输出从 [batch_size, seq_len, vocab_size] 变为 [batch_size*seq_len, vocab_size]\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "        # targets_flat：将目标从 [batch_size, seq_len] 变为 [batch_size*seq_len]\n",
    "        targets_flat = targets.reshape(-1)\n",
    "\n",
    "        # Calculate loss\n",
    "        # loss: [batch_size*seq_len]\n",
    "        loss = self.criterion(outputs_flat, targets_flat)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(self.train_dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "        # Loop over batches\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            source_ids = batch['source_ids'].to(self.device)\n",
    "            target_ids = batch['target_ids'].to(self.device)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with teacher forcing\n",
    "            decoder_input = target_ids[:, :-1]  # Remove last token\n",
    "            decoder_targets = target_ids[:, 1:]  # Remove first token (BOS)\n",
    "\n",
    "            # outputs: [batch_size, seq_len, vocab_size]\n",
    "            outputs = self.model(source_ids, decoder_input)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = self.calculate_loss(outputs, decoder_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "            })\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 100 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(self.val_dataloader, desc=\"Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                source_ids = batch['source_ids'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                decoder_input = target_ids[:, :-1]\n",
    "                decoder_targets = target_ids[:, 1:]\n",
    "\n",
    "                # outputs: [batch_size, seq_len, vocab_size]\n",
    "                outputs = self.model(source_ids, decoder_input)\n",
    "\n",
    "                # Calculate loss\n",
    "                # loss: [batch_size*seq_len]\n",
    "                loss = self.calculate_loss(outputs, decoder_targets)\n",
    "\n",
    "                # Update metrics\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'Val Loss': f'{loss.item():.4f}',\n",
    "                    'Avg Val Loss': f'{epoch_loss / num_batches:.4f}'\n",
    "                })\n",
    "\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def train(self, num_epochs=10, save_path=None):\n",
    "        \"\"\"Train the model for specified number of epochs\"\"\"\n",
    "        print(f\"=== Starting Training for {num_epochs} Epochs ===\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Train batches: {len(self.train_dataloader)}\")\n",
    "        print(f\"Validation batches: {len(self.val_dataloader)}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "\n",
    "            # Validate\n",
    "            val_loss = self.validate_epoch()\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "\n",
    "            # Calculate epoch time\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            self.history['learning_rate'].append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'model_config': model_config\n",
    "                    }, save_path)\n",
    "                    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Training summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training completed in {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Final train loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Final val loss: {self.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # Plot loss\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "\n",
    "        # Plot learning rate\n",
    "        axes[0, 1].plot(self.history['learning_rate'], color='green')\n",
    "        axes[0, 1].set_title('Learning Rate Schedule')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].grid(True)\n",
    "\n",
    "        # Plot epoch time\n",
    "        axes[1, 0].plot(self.history['epoch_time'], color='orange')\n",
    "        axes[1, 0].set_title('Epoch Training Time')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Time (seconds)')\n",
    "        axes[1, 0].grid(True)\n",
    "\n",
    "        # Plot loss comparison\n",
    "        x = range(len(self.history['train_loss']))\n",
    "        axes[1, 1].fill_between(x, self.history['train_loss'], alpha=0.3, color='blue', label='Train Loss')\n",
    "        axes[1, 1].fill_between(x, self.history['val_loss'], alpha=0.3, color='red', label='Val Loss')\n",
    "        axes[1, 1].plot(self.history['train_loss'], color='blue', linewidth=2)\n",
    "        axes[1, 1].plot(self.history['val_loss'], color='red', linewidth=2)\n",
    "        axes[1, 1].set_title('Loss Comparison')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Loss')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Start training for 10 epochs\n",
    "print(\"Starting training...\")\n",
    "history = trainer.train(num_epochs=6, save_path=\"best_seq2seq_model.pth\")\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\nPlotting training history...\")\n",
    "trainer.plot_training_history()"
   ],
   "id": "4ba7a193f07a7896"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
