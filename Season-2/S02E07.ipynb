{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Coding: Machine Translation by RNN\n",
    "\n",
    " - Dataset: wmt-17, en-zh, select 5m high-quality pairs of data\n",
    " - Model: Seq2seq with Encoder & Decoder framework\n",
    " - GPU: 1660TI"
   ],
   "id": "c43b2903b576698e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Seq2Seq Encoder-Decoder Architecture\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Sequence-to-Sequence (Seq2Seq) model with Encoder-Decoder architecture is a neural network framework designed for tasks where both input and output are sequences of variable length, such as machine translation (English → Chinese in your case).\n",
    "\n",
    "```\n",
    "Input Sequence (English):  \"Hello world\"\n",
    "                              ↓\n",
    "                          [ENCODER]\n",
    "                              ↓\n",
    "                        Context Vector\n",
    "                              ↓\n",
    "                          [DECODER]\n",
    "                              ↓\n",
    "Output Sequence (Chinese): \"你好世界\"\n",
    "```\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### 1. Encoder\n",
    "\n",
    "The encoder processes the input sequence and compresses the information into a fixed-size context vector (also called thought vector).\n",
    "\n",
    "```\n",
    "Input: [w1, w2, w3, ..., wn]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [RNN][RNN][RNN]...[RNN]\n",
    "       ↓    ↓    ↓       ↓\n",
    "    [h1] [h2] [h3] ... [hn] → Context Vector (hn)\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Embedding Layer**: Converts input tokens to dense vectors\n",
    "- **RNN Layers**: LSTM/GRU cells process the sequence sequentially\n",
    "- **Hidden States**: Capture information at each time step\n",
    "- **Final Context**: Last hidden state becomes the context vector\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden  # Context vectors\n",
    "```\n",
    "\n",
    "### 2. Decoder\n",
    "\n",
    "The decoder generates the output sequence one token at a time, using the context vector from the encoder.\n",
    "\n",
    "```\n",
    "Context Vector (C) → [RNN] → [RNN] → [RNN] → ... → [RNN]\n",
    "                      ↓       ↓       ↓             ↓\n",
    "                    [y1]    [y2]    [y3]   ...   [yn]\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "- **Initial State**: Initialized with encoder's context vector\n",
    "- **RNN Layers**: Generate hidden states for each output position\n",
    "- **Output Projection**: Maps hidden states to vocabulary probabilities\n",
    "- **Softmax**: Converts logits to probability distribution\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, target_seq, encoder_hidden, encoder_cell):\n",
    "        embedded = self.embedding(target_seq)\n",
    "        outputs, _ = self.rnn(embedded, encoder_hidden)\n",
    "        predictions = self.output_projection(outputs)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "## Complete Architecture Flow\n",
    "\n",
    "### Training Phase\n",
    "\n",
    "```\n",
    "1. Input Processing:\n",
    "   English: \"Hello world\" → [101, 7592, 2088, 102] (tokenized)\n",
    "   Chinese: \"[BOS] 你好世界 [EOS]\" → [101, 872, 1962, 686, 102] (tokenized)\n",
    "\n",
    "2. Encoder Forward Pass:\n",
    "   Input: [101, 7592, 2088, 102]\n",
    "   ↓\n",
    "   Embedding: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]\n",
    "   ↓\n",
    "   LSTM/GRU: h1, h2, h3, h4 → Context Vector (h4)\n",
    "\n",
    "3. Decoder Forward Pass:\n",
    "   Initial State: (h4) from encoder\n",
    "   Input: [101, 872, 1962, 686]\n",
    "   ↓\n",
    "   LSTM/GRU: generates hidden states for each position\n",
    "   ↓\n",
    "   Output Projection: [vocab_size] logits for each position\n",
    "   ↓\n",
    "   Loss Calculation: CrossEntropy with targets [872, 1962, 686, 102]\n",
    "```\n",
    "\n",
    "### Inference Phase\n",
    "\n",
    "```\n",
    "1. Encode input sequence: \"Hello world\"\n",
    "2. Initialize decoder with encoder's context vector\n",
    "3. Start with [BOS] token\n",
    "4. Generate tokens one by one:\n",
    "   - Input: [BOS] → Output: 你 (probability distribution)\n",
    "   - Input: [BOS] 你 → Output: 好\n",
    "   - Input: [BOS] 你 好 → Output: 世\n",
    "   - Input: [BOS] 你 好 世 → Output: 界\n",
    "   - Input: [BOS] 你 好 世 界 → Output: [EOS] (stop)\n",
    "```\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Encoder\n",
    "```\n",
    "h_t = LSTM/GRU(embedding(x_t), h_{t-1})\n",
    "context = h_n  # Final hidden state\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "```\n",
    "s_t = LSTM/GRU(embedding(y_{t-1}), s_{t-1})  # s_0 = context\n",
    "P(y_t | y_1...y_{t-1}, x) = softmax(W_s * s_t + b_s)\n",
    "```\n",
    "\n",
    "### Loss Function\n",
    "```\n",
    "Loss = -∑∑ log P(y_t^i | y_1^i...y_{t-1}^i, x^i)\n",
    "```\n",
    "\n",
    "## Architecture Advantages\n",
    "\n",
    "1. **Variable Length Handling**: Can process sequences of different lengths\n",
    "2. **End-to-End Learning**: Jointly optimizes encoder and decoder\n",
    "3. **Context Preservation**: Encoder captures semantic meaning in context vector\n",
    "4. **Language Agnostic**: Works for any language pair\n",
    "\n",
    "## Architecture Limitations\n",
    "\n",
    "1. **Information Bottleneck**: Fixed-size context vector may lose information\n",
    "2. **Long Sequence Problem**: Difficulty with very long input sequences\n",
    "3. **Sequential Processing**: Cannot parallelize during inference\n",
    "\n",
    "## Improvements & Variants\n",
    "\n",
    "1. **Attention Mechanism**: Allows decoder to focus on relevant encoder states\n",
    "2. **Bidirectional Encoder**: Processes sequence in both directions\n",
    "3. **Beam Search**: Better decoding strategy than greedy search\n",
    "4. **Teacher Forcing**: Training technique using ground truth as decoder input\n",
    "\n",
    "## Implementation Architecture for Your Project\n",
    "\n",
    "Based on your dataset (WMT-17 EN-ZH) and tokenizers (BERT-based), here's the recommended architecture:\n",
    "\n",
    "```\n",
    "Input: English sentence (max_length=100)\n",
    "↓\n",
    "BERT Tokenizer (vocab_size=30522) → Token IDs\n",
    "↓\n",
    "Embedding Layer (30522 → 512)\n",
    "↓\n",
    "Encoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Context Vector (1024-dim)\n",
    "↓\n",
    "Decoder LSTM/GRU (512 → 1024, num_layers=2)\n",
    "↓\n",
    "Output Projection (1024 → 21128)\n",
    "↓\n",
    "Chinese Token IDs → BERT Tokenizer → Chinese sentence\n",
    "```"
   ],
   "id": "de8eaec2f3bd079c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 可以先测试网络连接\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"https://huggingface.co\")\n",
    "    print(\"网络连接正常\")\n",
    "except:\n",
    "    print(\"网络连接可能存在问题\")"
   ],
   "id": "553a01635e46508f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download the data & select 5m high-quality pairs\n",
    "\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# load full wmt-17 en-zh dataset\n",
    "full_dataset = load_dataset(\"wmt/wmt17\", \"zh-en\", split=\"train\", cache_dir=r\"D:\\Developer\\LLM\\FuggingFace-cache-model\")\n",
    "\n",
    "# Length & Ratio filter\n",
    "def is_high_quality(x):\n",
    "    import re  # 添加这一行\n",
    "    en = x[\"translation\"][\"en\"]\n",
    "    zh = x[\"translation\"][\"zh\"]\n",
    "    if not en or not zh:\n",
    "        return False\n",
    "    if len(en) < 3 or len(zh) < 3:\n",
    "        return False\n",
    "    if len(en) > 100 or len(zh) > 100:\n",
    "        return False\n",
    "    ratio = len(en) / len(zh)\n",
    "    if ratio < 0.5 or ratio > 2:\n",
    "        return False\n",
    "    if not re.search(r'[\\u4e00-\\u9fff]', zh):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_dataset = full_dataset.filter(is_high_quality, num_proc=10)\n",
    "dataset = filtered_dataset.select(range(min(5_000_000, len(filtered_dataset))))\n",
    "\n",
    "print(\"Full Dataset Size: \", len(full_dataset))\n",
    "print(\"Filtered Dataset Size: \", len(filtered_dataset))\n",
    "print(\"Dataset Size: \", len(dataset))\n",
    "\n",
    "# print 10 samples\n",
    "sample = dataset.shuffle(seed=42).select(range(10))\n",
    "print(\"-\"*100)\n",
    "for i in sample:\n",
    "    print(i[\"translation\"][\"en\"])\n",
    "    print(i[\"translation\"][\"zh\"])\n",
    "    print(\"-\"*100)\n"
   ],
   "id": "f981b4eb6b850121",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create PyTorch Dataset and DataLoader for training\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer_en, tokenizer_zh, max_length=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset wrapper for HuggingFace translation dataset\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: HuggingFace dataset with translation pairs\n",
    "            tokenizer_en: English tokenizer (optional, can be added later)\n",
    "            tokenizer_zh: Chinese tokenizer (optional, can be added later)\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_zh = tokenizer_zh\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        zh_text = item[\"translation\"][\"zh\"]\n",
    "\n",
    "        en_tokens = self.tokenizer_en(en_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        zh_tokens = self.tokenizer_zh(zh_text,\n",
    "                                        max_length=self.max_length,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "                'source_ids': en_tokens['input_ids'].squeeze(),     # 去除张量的多余维度，输出以为数字数组\n",
    "                'target_ids': zh_tokens['input_ids'].squeeze(),\n",
    "                'source_text': en_text,\n",
    "                'target_text': zh_text\n",
    "        }\n",
    "\n",
    "def create_dataloaders(dataset, batch_size=64, num_workers=12, train_split=0.95):\n",
    "    \"\"\"\n",
    "    Create train and validation DataLoaders from HuggingFace dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with translation pairs\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        train_split: Fraction of data to use for training\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader, val_dataloader, train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    train_size = int(train_split * len(dataset))\n",
    "\n",
    "    # Create indices for splitting\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices,\n",
    "                                                train_size=train_size,\n",
    "                                                random_state=42)\n",
    "\n",
    "    # Create train and validation datasets\n",
    "    train_dataset_hf = dataset.select(train_indices)\n",
    "    val_dataset_hf = dataset.select(val_indices)\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # get vocab sizes\n",
    "    vocab_size_en = tokenizer_en.vocab_size\n",
    "    vocab_size_zh = tokenizer_zh.vocab_size\n",
    "\n",
    "    print(f\"Vocab size for en: {vocab_size_en}\")\n",
    "    print(f\"Vocab size for zh: {vocab_size_zh}\")\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    train_dataset = TranslationDataset(train_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "    val_dataset = TranslationDataset(val_dataset_hf, tokenizer_en, tokenizer_zh)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train DataLoader: {len(train_dataloader)} batches\")\n",
    "    print(f\"Validation DataLoader: {len(val_dataloader)} batches\")\n",
    "\n",
    "    return train_dataloader, val_dataloader, vocab_size_en, vocab_size_zh\n",
    "\n",
    "def test_dataloader(dataloader):\n",
    "    \"\"\"Test the DataLoader by printing a sample batch\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Sample batch from DataLoader:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # for batch in dataloader:\n",
    "    #     print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "    #     print(f\"Source example: {batch['source_text'][0]}\")\n",
    "    #     print(f\"Source tokens: {batch['source_ids'][0]}\")\n",
    "    #     print(f\"Target example: {batch['target_text'][0]}\")\n",
    "    #     print(f\"Target tokens: {batch['target_ids'][0]}\")\n",
    "    #     break\n",
    "\n",
    "    # 使用next和iter更直接地获取一个batch\n",
    "    try:\n",
    "        batch = next(iter(dataloader))\n",
    "        print(f\"Batch size: {len(batch['source_text'])}\")\n",
    "        print(f\"Source example: {batch['source_text'][0]}\")\n",
    "        print(f\"Source tokens shape: {batch['source_ids'][0].shape}\")\n",
    "        print(f\"Target example: {batch['target_text'][0]}\")\n",
    "        print(f\"Target tokens shape: {batch['target_ids'][0].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting batch: {e}\")\n",
    "\n",
    "train_dataloader, val_dataloader, encoder_vocab_size, decoder_vocab_size = create_dataloaders(dataset)\n",
    "test_dataloader(train_dataloader)\n",
    "test_dataloader(val_dataloader)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89647506c8cf2b56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
